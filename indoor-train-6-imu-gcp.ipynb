{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import io\n",
    "from skimage.color import rgba2rgb, rgb2xyz\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from math import floor, ceil\n",
    "import random\n",
    "\n",
    "# Train data generation\n",
    "import collections\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import time\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings and altering components for GCP\n",
    "\n",
    "# path settings\n",
    "# root_path = \"../input/indoor-location-navigation/\"\n",
    "root_path = \"../jupyter/input/\"\n",
    "\n",
    "# load paths\n",
    "train_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\n",
    "test_paths = glob.glob(root_path + \"test\" + \"/*\")\n",
    "metafiles = glob.glob(root_path + \"metadata\" + \"/*\")\n",
    "\n",
    "# function imports using github repo in kaggle kernels\n",
    "# https://www.kaggle.com/getting-started/71642\n",
    "# !cp -r ../input/indoorlocationcompetition20master/indoor-location-competition-20-master/* ./\n",
    "# from io_f import read_data_file\n",
    "# from compute_f import compute_step_positions, compute_steps, \\\n",
    "# compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq\n",
    "\n",
    "# import for gcp settings\n",
    "import compute_f\n",
    "import io_f\n",
    "import visualize_f\n",
    "import main\n",
    "from io_f import read_data_file\n",
    "from compute_f import compute_step_positions, compute_steps, \\\n",
    "compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for saving files\n",
    "# !mkdir train\n",
    "# !mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter milisecond setting \n",
    "IMU_CUT = 250\n",
    "WPS_CUT = 5000\n",
    "\n",
    "# train number setting\n",
    "# TRAIN_NUM = len(train_paths)\n",
    "# TRAIN_NUM = round(len(train_paths) / 2)\n",
    "TRAIN_NUM = 10\n",
    "\n",
    "# floor translation\n",
    "FLOOR_MAP = {\"B3\":-3,\"B2\":-2,\"B1\":-1,\"F1\":0,\"1F\":0,\"F2\":1,\"2F\":1,\"F3\":2,\"3F\":2,\"F4\":3,\"4F\":3,\n",
    "             \"F5\":4,\"5F\":4,\"F6\":5,\"6F\":5,\"F7\":6,\"7F\":6,\"F8\":7,\"8F\": 7,\"F9\":8,\"9F\":8,\"F10\":9,\n",
    "             \"B\":0,\"BF\":1,\"BM\":2, \"G\":0, \"M\":0, \"P1\":0,\"P2\":1, \"LG2\":-2,\"LG1\":-1,\"LG\":0,\"LM\":0,\n",
    "             \"L1\":1,\"L2\":2,\"L3\":3,\"L4\":4,\"L5\":5,\"L6\":6,\"L7\":7,\"L8\":8,\"L9\":9,\"L10\":10,\"L11\":11}\n",
    "\n",
    "# Columns to shift to the beginning of df\n",
    "SHIFT_COLS = [\"rel_y\", \"rel_x\", \"rel_diff\", \\\n",
    "              \"magn_u_z_avg\", \"magn_u_y_avg\", \"magn_u_x_avg\", \\\n",
    "              \"gyro_z_avg\", \"gyro_y_avg\", \"gyro_x_avg\", \\\n",
    "              \"ahrs_z_avg\", \"ahrs_y_avg\", \"ahrs_x_avg\",  \\\n",
    "              \"magn_st\", \"magn_z_avg\", \"magn_y_avg\", \"magn_x_avg\", \\\n",
    "              \"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n",
    "              \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n",
    "              \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n",
    "\n",
    "SHIFT_COLS_TEST = [\"rel_y\", \"rel_x\", \"rel_diff\", \\\n",
    "                   \"magn_u_z_avg\", \"magn_u_y_avg\", \"magn_u_x_avg\", \\\n",
    "                   \"gyro_z_avg\", \"gyro_y_avg\", \"gyro_x_avg\", \\\n",
    "                   \"ahrs_z_avg\", \"ahrs_y_avg\", \"ahrs_x_avg\",  \\\n",
    "                   \"magn_st\", \"magn_z_avg\", \"magn_y_avg\", \"magn_x_avg\", \\\n",
    "                   \"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n",
    "                   \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n",
    "                   \"y\", \"x\", \"wps_diff\", \"wifi_ts\", \"site_path_timestamp\"]\n",
    "\n",
    "INT_COLS = [\"wifi_ts\"]\n",
    "CAT_COLS = [\"file_id\", \"site_id\", \"floor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Files in Train: 26,925 \n",
      "No. Files in Test: 626 \n",
      "No. of metadata files: 204\n",
      "example path:  ../jupyter/input/train/5cd56bdbe2acfd2d33b663c0/L3/5dfc80e8fa5c6000066ebdd1.txt\n",
      "site:  5cd56bdbe2acfd2d33b663c0\n",
      "floorNo:  L3\n",
      "floor_plan_filename:  ../jupyter/input/metadata/5cd56bdbe2acfd2d33b663c0/L3/floor_image.png\n",
      "json_plan_filename:  ../jupyter/input/metadata/5cd56bdbe2acfd2d33b663c0/L3/floor_info.json\n",
      "width: 97.36374120483586, height: 221.63702991982515 \n",
      "No. Lines in 1 example: 5,930\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "print(\"No. Files in Train: {:,}\".format(len(train_paths)), \"\\n\" +\n",
    "      \"No. Files in Test: {:,}\".format(len(test_paths)), \"\\n\" +\n",
    "      \"No. of metadata files: {:,}\".format(len(metafiles)))\n",
    "\n",
    "# Reading in 1 file\n",
    "def pick_example(max_range, paths):\n",
    "    ex = random.randint(0, max_range)\n",
    "    example_path = paths[ex]\n",
    "    path = f\"{example_path}\"\n",
    "    paths = path.split(\"/\")\n",
    "    site = paths[4]\n",
    "    floorNo = paths[5]\n",
    "    floor_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_image.png\"\n",
    "    json_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_info.json\"\n",
    "    with open(json_plan_filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    width_meter = json_data[\"map_info\"][\"width\"]\n",
    "    height_meter = json_data[\"map_info\"][\"height\"]\n",
    "    return path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter\n",
    "\n",
    "path, site, floorNo, floor_plan_filename, \\\n",
    "json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n",
    "print(\"example path: \", path)\n",
    "print(\"site: \", site)\n",
    "print(\"floorNo: \", floorNo)\n",
    "print(\"floor_plan_filename: \", floor_plan_filename)\n",
    "print(\"json_plan_filename: \", json_plan_filename)\n",
    "print(\"width: {}, height: {} \".format(width_meter, height_meter))\n",
    "\n",
    "with open(path) as p:\n",
    "    lines = p.readlines()\n",
    "print(\"No. Lines in 1 example: {:,}\". format(len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the data extraction class\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReadData:\n",
    "    acce: np.ndarray\n",
    "    acce_uncali: np.ndarray\n",
    "    gyro: np.ndarray\n",
    "    gyro_uncali: np.ndarray\n",
    "    magn: np.ndarray\n",
    "    magn_uncali: np.ndarray\n",
    "    ahrs: np.ndarray\n",
    "    wifi: np.ndarray\n",
    "    ibeacon: np.ndarray\n",
    "    waypoint: np.ndarray\n",
    "\n",
    "\n",
    "def read_data_file_ed(data_filename):\n",
    "    acce = []\n",
    "    acce_uncali = []\n",
    "    gyro = []\n",
    "    gyro_uncali = []\n",
    "    magn = []\n",
    "    magn_uncali = []\n",
    "    ahrs = []\n",
    "    wifi = []\n",
    "    ibeacon = []\n",
    "    waypoint = []\n",
    "\n",
    "    with open(data_filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line_data in lines:\n",
    "        line_data = line_data.strip()\n",
    "        if not line_data or line_data[0] == '#':\n",
    "            continue\n",
    "\n",
    "        line_data = line_data.split('\\t')\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER':\n",
    "            acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n",
    "            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE':\n",
    "            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n",
    "            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n",
    "            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n",
    "            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ROTATION_VECTOR':\n",
    "            ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WIFI':\n",
    "            sys_ts = line_data[0]\n",
    "            ssid = line_data[2]\n",
    "            bssid = line_data[3]\n",
    "            rssi = line_data[4]\n",
    "            lastseen_ts = line_data[6]\n",
    "            wifi_data = [sys_ts, ssid, bssid, '_'.join([ssid, bssid]), rssi, lastseen_ts]\n",
    "            wifi.append(wifi_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_BEACON':\n",
    "            ts = line_data[0]\n",
    "            uuid = line_data[2]\n",
    "            major = line_data[3]\n",
    "            minor = line_data[4]\n",
    "            txpower = line_data[5]\n",
    "            rssi = line_data[6]\n",
    "            distance = line_data[7]\n",
    "            mac_address = line_data[-2]\n",
    "            beacon_ts = line_data[-1]\n",
    "            ibeacon_data = [ts, '_'.join([uuid, major, minor]), txpower, rssi, distance, mac_address, beacon_ts]\n",
    "            ibeacon.append(ibeacon_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WAYPOINT':\n",
    "            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n",
    "\n",
    "    acce = np.array(acce)\n",
    "    acce_uncali = np.array(acce_uncali)\n",
    "    gyro = np.array(gyro)\n",
    "    gyro_uncali = np.array(gyro_uncali)\n",
    "    magn = np.array(magn)\n",
    "    magn_uncali = np.array(magn_uncali)\n",
    "    ahrs = np.array(ahrs)\n",
    "    wifi = np.array(wifi)\n",
    "    ibeacon = np.array(ibeacon)\n",
    "    waypoint = np.array(waypoint)\n",
    "\n",
    "    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../jupyter/input/train/5cd56c11e2acfd2d33b6b413/F6/\n",
      "no. of files of that floor:  10\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Find out how many wps datapoints and wifi datapoints one floor has\n",
    "train_path_floor = glob.glob(root_path + \"train\" + \"/*/*/\")\n",
    "# train_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\n",
    "ex = random.randint(0, 6)\n",
    "print(train_path_floor[ex])\n",
    "print(\"no. of files of that floor: \", len(os.listdir(train_path_floor[ex])))\n",
    "count = 0\n",
    "for f in os.listdir(train_path_floor[ex]):\n",
    "    file_path = train_path_floor[ex] + f\n",
    "    data = read_data_file_ed(file_path)\n",
    "    count += len(data.waypoint)\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n",
    "# show_site_png(root_path, site=site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature candidate\n",
    "# You can't get the waypoint in test, so use acce and ahrs data to calculate relative positions\n",
    "def calc_rel_positions(acce_datas, ahrs_datas):\n",
    "    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n",
    "    headings = compute_headings(ahrs_datas)\n",
    "    stride_lengths = compute_stride_length(step_acce_max_mins)\n",
    "    step_headings = compute_step_heading(step_timestamps, headings)\n",
    "    rel_positions = compute_rel_positions(stride_lengths, step_headings)\n",
    "    # only use del if we don't need timestamps\n",
    "    # rel_positions_del = np.delete(rel_positions, 0, 1)\n",
    "    return rel_positions\n",
    "\n",
    "# Feature candidate\n",
    "# Modify extract_magnetic_strength from github for one magnetic data point\n",
    "def extract_one_magn_strength(magn_datas):\n",
    "    d = np.array(magn_datas[2:5])\n",
    "    return np.mean(np.sqrt(np.sum(d ** 2, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path, site, floorNo, floor_plan_filename, \\\n",
    "# json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common methods\n",
    "def extract_imu_rep(imu_data, wifi_ts):\n",
    "    imu_ts = imu_data[:, 0].astype(int)\n",
    "    diff_list = []\n",
    "    for ts in imu_ts:\n",
    "        diff = abs(int(wifi_ts) - ts)\n",
    "        diff_list.append(diff)\n",
    "    # diff_idx = np.argmin(diff_list)\n",
    "    # acce_diff_range = [(i,a) for i, a in enumerate(diff_list) if a < cut_line] # uncomment if we need to check acce_diff\n",
    "    imu_diff_range = [i for i, a in enumerate(diff_list) if a < IMU_CUT]\n",
    "    imu_filtered = imu_data[imu_diff_range]\n",
    "    if imu_filtered.shape[0] == 0:\n",
    "        imu_avg_x = np.nan\n",
    "        imu_avg_y = np.nan\n",
    "        imu_avg_z = np.nan\n",
    "    else:\n",
    "        imu_avg_x = imu_filtered[:, 1].mean()\n",
    "        imu_avg_y = imu_filtered[:, 2].mean()\n",
    "        imu_avg_z = imu_filtered[:, 3].mean()\n",
    "        #print(imu_avg_x, imu_avg_y, imu_avg_z)\n",
    "    return imu_avg_x, imu_avg_y, imu_avg_z\n",
    "\n",
    "def shift_columns(cols, df):\n",
    "    for col in cols:\n",
    "        df_cols = list(df.columns)\n",
    "        df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n",
    "        df = df[df_cols]\n",
    "    return df\n",
    "\n",
    "# convert data types of certain columns\n",
    "def convert_dtypes(df, col_list, dtype):\n",
    "    for col in col_list:\n",
    "        df[col] = df[col].astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train specific methods\n",
    "def extract_nearest_wps(wps_data, wifi_ts):\n",
    "    wps_ts = wps_data[:, 0].astype(int)\n",
    "    diff_list = []\n",
    "    for ts in wps_ts:\n",
    "        diff = abs(int(wifi_ts) - ts)\n",
    "        diff_list.append(diff)\n",
    "    diff_idx = np.argmin(diff_list)\n",
    "    return diff_list[diff_idx], wps_data[diff_idx]\n",
    "\n",
    "def extract_train_path(path):\n",
    "    ex_path = f\"{path}\"\n",
    "    ex_paths = ex_path.split(\"/\")\n",
    "    site_id = ex_paths[4]\n",
    "    floor = ex_paths[5]\n",
    "    f = FLOOR_MAP[floor]\n",
    "    file_id = ex_paths[6].split(\".\")[0]\n",
    "    return site_id, file_id, f, floor\n",
    "\n",
    "def make_wifi_df_train(path):\n",
    "    try:\n",
    "        # First path\n",
    "        datas = read_data_file_ed(path)\n",
    "        acce_datas = datas.acce\n",
    "        magn_datas = datas.magn\n",
    "        ahrs_datas = datas.ahrs\n",
    "        gyro_datas = datas.gyro\n",
    "        # acce_uncali = datas.acce_uncali\n",
    "        magn_uncali = datas.magn_uncali # Only use magn for uncalibrated data, as it seems more important in initial modeling result\n",
    "        # gyro_uncali = datas.gyro_uncali\n",
    "        wifi_datas = datas.wifi\n",
    "        ibeacon_datas = datas.ibeacon\n",
    "        wps = datas.waypoint\n",
    "        rel_positions = calc_rel_positions(acce_datas, ahrs_datas)\n",
    "\n",
    "        # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n",
    "\n",
    "        # Make wifi df with wifi_ts\n",
    "        if wifi_datas.shape[0] == 0:\n",
    "            print(\"no wifi data at: \", path)\n",
    "            return []\n",
    "\n",
    "        dfs = []\n",
    "        df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n",
    "        for wifi_ts, g in df.groupby(0):\n",
    "            g = g.drop_duplicates(subset=1)\n",
    "            tmp = g.iloc[:,1:]\n",
    "            feat = tmp.set_index(1).T\n",
    "            feat[\"wifi_ts\"] = wifi_ts\n",
    "\n",
    "            # get closest wps\n",
    "            closest_wps = extract_nearest_wps(wps, wifi_ts)\n",
    "            feat[\"wps_diff\"] = closest_wps[0]\n",
    "            feat[\"x\"] = closest_wps[1][1]\n",
    "            feat[\"y\"] = closest_wps[1][2]\n",
    "\n",
    "            # get average of acce within 250ms\n",
    "            acce_avgs = extract_imu_rep(acce_datas, wifi_ts)\n",
    "            feat[\"acce_x_avg\"] = acce_avgs[0]\n",
    "            feat[\"acce_y_avg\"] = acce_avgs[1]\n",
    "            feat[\"acce_z_avg\"] = acce_avgs[2]\n",
    "\n",
    "            # get average of magn within 250ms\n",
    "            magn_avgs = extract_imu_rep(magn_datas, wifi_ts)\n",
    "            feat[\"magn_x_avg\"] = magn_avgs[0]\n",
    "            feat[\"magn_y_avg\"] = magn_avgs[1]\n",
    "            feat[\"magn_z_avg\"] = magn_avgs[2]\n",
    "            # get magnetic strength of the 250ms average magn_avg\n",
    "            feat[\"magn_st\"] = extract_one_magn_strength(magn_avgs)\n",
    "\n",
    "            # get average of ahrs within 250ms\n",
    "            ahrs_avgs = extract_imu_rep(ahrs_datas, wifi_ts)\n",
    "            feat[\"ahrs_x_avg\"] = ahrs_avgs[0]\n",
    "            feat[\"ahrs_y_avg\"] = ahrs_avgs[1]\n",
    "            feat[\"ahrs_z_avg\"] = ahrs_avgs[2]\n",
    "\n",
    "            # get average of gyro within 250ms\n",
    "            gyro_avgs = extract_imu_rep(gyro_datas, wifi_ts)\n",
    "            feat[\"gyro_x_avg\"] = gyro_avgs[0]\n",
    "            feat[\"gyro_y_avg\"] = gyro_avgs[1]\n",
    "            feat[\"gyro_z_avg\"] = gyro_avgs[2]\n",
    "\n",
    "            # get average of magn_uncali within 250ms\n",
    "            magn_uncali_avgs = extract_imu_rep(magn_uncali, wifi_ts)\n",
    "            feat[\"magn_u_x_avg\"] = magn_uncali_avgs[0]\n",
    "            feat[\"magn_u_y_avg\"] = magn_uncali_avgs[1]\n",
    "            feat[\"magn_u_z_avg\"] = magn_uncali_avgs[2]\n",
    "\n",
    "            # get closest relative positions that was worked out with acce and ahrs data\n",
    "            rel_pos = extract_nearest_wps(rel_positions, wifi_ts)\n",
    "            feat[\"rel_diff\"] = rel_pos[0]\n",
    "            feat[\"rel_x\"] = rel_pos[1][1]\n",
    "            feat[\"rel_y\"] = rel_pos[1][2]\n",
    "\n",
    "            # get floor and other path data\n",
    "            site_id, file_id, f, floor = extract_train_path(path)\n",
    "            feat[\"site_id\"] = site_id\n",
    "            feat[\"file_id\"] = file_id\n",
    "            feat[\"floor_int\"] = f\n",
    "            feat[\"floor\"] = floor\n",
    "\n",
    "            dfs.append(feat)\n",
    "        return dfs\n",
    "    except:\n",
    "        print(\"make_wifi_df_train error: \", path)\n",
    "\n",
    "\n",
    "def make_train_df(paths_df, site_list):\n",
    "    for site in site_list:\n",
    "        try:\n",
    "            print(\"site: \", site)\n",
    "            df = paths_df[paths_df[\"site_id\"] == site]\n",
    "            paths = df[\"path\"].unique()\n",
    "            dfs_all = pool.map(make_wifi_df_train, paths)\n",
    "            dfs_unpack = [row for df in dfs_all for row in df if dfs_all is not None]\n",
    "            wifi_df = pd.concat(dfs_unpack)\n",
    "            wifi_df = shift_columns(SHIFT_COLS, wifi_df)\n",
    "            wifi_df = wifi_df.fillna(-999)\n",
    "            convert_dtypes(wifi_df, INT_COLS, int)\n",
    "            convert_dtypes(wifi_df, CAT_COLS, \"category\")\n",
    "            wifi_df.to_csv(f\"./train_3/{site}_train.csv\", index=False)\n",
    "            del wifi_df\n",
    "        except:\n",
    "            print(\"make_train_df at: \", site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "10877\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# train_path filtering\n",
    "\n",
    "def extract_path_for_grouplist(path):\n",
    "    ex_path = f\"{path}\"\n",
    "    ex_paths = ex_path.split(\"/\")\n",
    "    site_id = ex_paths[4]\n",
    "    file_id = ex_paths[6].split(\".\")[0]\n",
    "    return [path, site_id, file_id]\n",
    "\n",
    "# create pathlist to be used by 2 types of paths list\n",
    "path_list = [extract_path_for_grouplist(item) for item in train_paths]\n",
    "df_paths = pd.DataFrame(path_list, columns=[\"path\", \"site_id\", \"file_id\"])\n",
    "site_id_path_list = df_paths[\"site_id\"].unique()\n",
    "\n",
    "# grouped_paths_list -> It takes 3 records from every site_id\n",
    "grouped_paths_df = df_paths.groupby(\"site_id\").sample(n=3)\n",
    "grouped_paths_list = list(grouped_paths_df[\"path\"].unique())\n",
    "# display(grouped_paths_df.head())\n",
    "\n",
    "# Get 24 sites from submission filw\n",
    "sub_df = pd.read_csv(\"../jupyter/input/sample_submission.csv\")\n",
    "sub_df[[\"site_id\", \"file_id\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
    "sub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n",
    "sub_df_site_list = sub_df[\"site_id\"].unique()\n",
    "\n",
    "df_paths_24 = df_paths[df_paths[\"site_id\"].isin(sub_df_site_list)]\n",
    "print(df_paths_24[\"site_id\"].nunique())\n",
    "print(len(df_paths_24))\n",
    "print(sub_df[\"site_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To deal with the stoppage\n",
    "# train_sites_list = df_paths_24[\"site_id\"].unique()\n",
    "# print(train_sites_list)\n",
    "# print(len(train_sites_list))\n",
    "\n",
    "# done_list = [\n",
    "#     \"5c3c44b80379370013e0fd2b\",\n",
    "#     \"5d27097f03f801723c320d97\",\n",
    "#     \"5d2709bb03f801723c32852c\",\n",
    "#     \"5d2709c303f801723c3299ee\",\n",
    "#     \"5da138754db8ce0c98bca82f\",\n",
    "#     \"5da138764db8ce0c98bcaa46\",\n",
    "#     \"5da1389e4db8ce0c98bd0547\",\n",
    "#     \"5da958dd46f8266d0737457b\"\n",
    "# ]\n",
    "\n",
    "# rest_list = list(set(train_sites_list) - set(done_list))\n",
    "# print(rest_list)\n",
    "# print(len(rest_list))\n",
    "# df_paths_24 = df_paths[df_paths[\"site_id\"].isin(rest_list)]\n",
    "# print(df_paths_24[\"site_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2435\n",
      "2720\n",
      "2650\n",
      "3072\n",
      "10877\n",
      "\n",
      "\n",
      "24\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "['5d27097f03f801723c320d97' '5d2709bb03f801723c32852c'\n",
      " '5d2709c303f801723c3299ee' '5da138754db8ce0c98bca82f'\n",
      " '5c3c44b80379370013e0fd2b' '5da138764db8ce0c98bcaa46']\n",
      "['5da1389e4db8ce0c98bd0547' '5da958dd46f8266d0737457b'\n",
      " '5a0546857ecc773753327266' '5da138314db8ce0c98bbf3a0'\n",
      " '5d27096c03f801723c31e5e0' '5d2709d403f801723c32bd39']\n",
      "['5da138364db8ce0c98bc00f1' '5d27099f03f801723c32511d'\n",
      " '5d27075f03f801723c2e360f' '5da138274db8ce0c98bbd3d2'\n",
      " '5da1382d4db8ce0c98bbe92e' '5dc8cea7659e181adb076a3f']\n",
      "['5d2709e003f801723c32d896' '5da1383b4db8ce0c98bc11ab'\n",
      " '5dbc1d84c1eb61796cf7c010' '5d2709b303f801723c327472'\n",
      " '5da138b74db8ce0c98bd4774' '5d2709a003f801723c3251bf']\n"
     ]
    }
   ],
   "source": [
    "train_sites_list = df_paths_24[\"site_id\"].unique()\n",
    "train_sites_list_1 = train_sites_list[:6]\n",
    "train_sites_list_2 = train_sites_list[6:12]\n",
    "train_sites_list_3 = train_sites_list[12:18]\n",
    "train_sites_list_4 = train_sites_list[18:24]\n",
    "\n",
    "df_paths_24_1 = df_paths_24[df_paths_24[\"site_id\"].isin(train_sites_list_1)]\n",
    "df_paths_24_2 = df_paths_24[df_paths_24[\"site_id\"].isin(train_sites_list_2)]\n",
    "df_paths_24_3 = df_paths_24[df_paths_24[\"site_id\"].isin(train_sites_list_3)]\n",
    "df_paths_24_4 = df_paths_24[df_paths_24[\"site_id\"].isin(train_sites_list_4)]\n",
    "# print(len(df_paths_24_1))\n",
    "# print(len(df_paths_24_2))\n",
    "# print(len(df_paths_24_3))\n",
    "# print(len(df_paths_24_4))\n",
    "# print(len(df_paths_24))\n",
    "# print(\"\\n\")\n",
    "# print(len(train_sites_list))\n",
    "# print(len(train_sites_list_1))\n",
    "# print(len(train_sites_list_2))\n",
    "# print(len(train_sites_list_3))\n",
    "# print(len(train_sites_list_4))\n",
    "# print(train_sites_list_1)\n",
    "# print(train_sites_list_2)\n",
    "# print(train_sites_list_3)\n",
    "# print(train_sites_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d27097f03f801723c320d97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [18:34<1:32:51, 1114.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d2709bb03f801723c32852c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [56:43<2:00:20, 1805.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d2709c303f801723c3299ee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [1:39:38<1:47:50, 2156.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da138754db8ce0c98bca82f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [1:48:37<50:35, 1518.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5c3c44b80379370013e0fd2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [2:14:16<25:25, 1525.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da138764db8ce0c98bcaa46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [2:27:16<00:00, 1472.76s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to extract data:  8837.263760328293\n",
      "site:  5da1389e4db8ce0c98bd0547\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/B2/5dc54bd921dceb0006114b4b.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/B2/5dc6530e1cda37000603043b.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/B2/5dc54bd81cda37000602fca4.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/B2/5dc653101cda37000603043f.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F4/5dc6987017ffdd0006f11974.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F4/5dc698691cda370006030a77.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F4/5dc6986f1cda370006030a7b.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F3/5dc682f41cda3700060308e2.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F3/5dc647af17ffdd0006f112bb.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F3/5dc63f2817ffdd0006f111d7.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F3/5dc6479d1cda370006030330.txt\n",
      "no wifi data at:  ../jupyter/input/train/5da1389e4db8ce0c98bd0547/F3/5dc682f91cda3700060308e8.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [05:14<26:11, 314.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da958dd46f8266d0737457b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [49:09<1:51:58, 1679.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5a0546857ecc773753327266\n",
      "no wifi data at:  ../jupyter/input/train/5a0546857ecc773753327266/F3/5d8f0954b6e29d0006fb8c0d.txt\n",
      "no wifi data at:  ../jupyter/input/train/5a0546857ecc773753327266/F3/5d8f0955b6e29d0006fb8c0f.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [1:15:25<1:21:37, 1632.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da138314db8ce0c98bbf3a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [1:27:04<42:07, 1263.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d27096c03f801723c31e5e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [1:45:26<20:05, 1205.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d2709d403f801723c32bd39\n",
      "no wifi data at:  ../jupyter/input/train/5d2709d403f801723c32bd39/2F/5dc77d571cda370006030ef7.txt\n",
      "no wifi data at:  ../jupyter/input/train/5d2709d403f801723c32bd39/2F/5dcd15b723759900063d5524.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [2:03:48<00:00, 1238.13s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to extract data:  16266.632457017899\n",
      "site:  5da138364db8ce0c98bc00f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [02:28<12:23, 148.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d27099f03f801723c32511d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [07:46<16:31, 247.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d27075f03f801723c2e360f\n",
      "no wifi data at:  ../jupyter/input/train/5d27075f03f801723c2e360f/F2/5de0cd4fbbb32e0006603cdb.txt\n",
      "no wifi data at:  ../jupyter/input/train/5d27075f03f801723c2e360f/F2/5de0a58bbbb32e0006603c93.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [2:13:42<2:59:15, 3585.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da138274db8ce0c98bbd3d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [2:15:02<1:13:23, 2201.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da1382d4db8ce0c98bbe92e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [2:39:33<32:17, 1937.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5dc8cea7659e181adb076a3f\n",
      "no wifi data at:  ../jupyter/input/train/5dc8cea7659e181adb076a3f/F2/5dcfb01594e490000612593c.txt\n",
      "no wifi data at:  ../jupyter/input/train/5dc8cea7659e181adb076a3f/F1/5dcf7852878f3300066c6d5e.txt\n",
      "no wifi data at:  ../jupyter/input/train/5dc8cea7659e181adb076a3f/F2/5dcfb02594e4900006125943.txt\n",
      "no wifi data at:  ../jupyter/input/train/5dc8cea7659e181adb076a3f/F2/5dcfafc494e4900006125908.txt\n",
      "no wifi data at:  ../jupyter/input/train/5dc8cea7659e181adb076a3f/F3/5dccee18c04f060006e6e2e6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [3:32:44<00:00, 2127.40s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to extract data:  29031.880504131317\n",
      "site:  5d2709e003f801723c32d896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [13:29<1:07:29, 809.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da1383b4db8ce0c98bc11ab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [33:53<1:10:13, 1053.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5dbc1d84c1eb61796cf7c010\n",
      "no wifi data at:  ../jupyter/input/train/5dbc1d84c1eb61796cf7c010/F2/5dd37f0444333f00067aa24d.txt\n",
      "no wifi data at:  ../jupyter/input/train/5dbc1d84c1eb61796cf7c010/F3/5dd3902827889b0006b76ae2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [1:32:46<1:49:16, 2185.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d2709b303f801723c327472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [1:57:56<1:03:57, 1918.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5da138b74db8ce0c98bd4774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [2:46:21<37:54, 2274.65s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:  5d2709a003f801723c3251bf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [2:50:26<00:00, 1704.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to extract data:  39258.86398792267\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "pool = Pool(num_cores)\n",
    "\n",
    "# # Checking purposes\n",
    "# # Kaggle nb -> 100 records:  288.3 sec\n",
    "# # gcp -> 100 records: 216.1 sec\n",
    "# # gcp -> full: 22415.6 sec (6.2 hours)\n",
    "# # error at 5dbc1d84c1eb61796cf7c010, 5dc8cea7659e181adb076a3f, \n",
    "# # 5d27075f03f801723c2e360f, 5d2709d403f801723c32bd39, 5a0546857ecc773753327266\n",
    "# # 5da1389e4db8ce0c98bd0547 -> Check this one out\n",
    "# # RuntimeWarning: invalid value encountered in double_scalars\n",
    "# # RuntimeWarning: Mean of empty slice.\n",
    "\n",
    "\n",
    "# grouped_paths_df = grouped_paths_df.iloc[:100,:]\n",
    "# print(len(grouped_paths_df))\n",
    "# grouped_paths_df = grouped_paths_df.sample(n=100)\n",
    "# train_sites_list = grouped_paths_df[\"site_id\"].unique()\n",
    "# make_train_df(grouped_paths_df, train_sites_list)\n",
    "\n",
    "# REAL training\n",
    "# df_paths_24 = df_paths_24.iloc[:25, :]\n",
    "# df_paths_24 = df_paths_24.sample(n=100)\n",
    "# df_split = np.array_split(df_paths_24, 20)\n",
    "# for df in tqdm(df_split):\n",
    "#     train_sites_list = df[\"site_id\"].unique()\n",
    "#     make_train_df(df, train_sites_list)\n",
    "\n",
    "# time of run: 25473.1 sec (train 5, with 16 sites)\n",
    "# train_sites_list = df_paths_24[\"site_id\"].unique()\n",
    "# make_train_df(df_paths_24, tqdm(train_sites_list))\n",
    "\n",
    "# time of run: 39258.8 sec (train 6, with 24 sites)\n",
    "\n",
    "make_train_df(df_paths_24_1, tqdm(train_sites_list_1))\n",
    "print(\"time to extract data: \", time.time() - start)\n",
    "\n",
    "make_train_df(df_paths_24_2, tqdm(train_sites_list_2))\n",
    "print(\"time to extract data: \", time.time() - start)\n",
    "\n",
    "make_train_df(df_paths_24_3, tqdm(train_sites_list_3))\n",
    "print(\"time to extract data: \", time.time() - start)\n",
    "\n",
    "make_train_df(df_paths_24_4, tqdm(train_sites_list_4))\n",
    "print(\"time to extract data: \", time.time() - start)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific methods\n",
    "def extract_nearest_wifi(wifi_datas, timestamp):\n",
    "    diff_list = []\n",
    "    wifi_ts = wifi_datas[:, 0]\n",
    "    for ts in wifi_ts:\n",
    "        diff = abs(int(timestamp) - int(ts))\n",
    "        diff_list.append(diff)\n",
    "    min_value = min(diff_list)\n",
    "    diff_indices = [i for i, x in enumerate(diff_list) if x == min_value]\n",
    "    wifi_datas = wifi_datas[diff_indices]\n",
    "    return wifi_datas\n",
    "\n",
    "def make_wifi_df_test(zipped_paths):\n",
    "    site_id, file_id, timestamp, site_path_timestamp = zipped_paths\n",
    "    file_path = \"../jupyter/input/test/\" + file_id + \".txt\"\n",
    "    datas = read_data_file_ed(file_path)\n",
    "    acce_datas = datas.acce\n",
    "    magn_datas = datas.magn\n",
    "    ahrs_datas = datas.ahrs\n",
    "    gyro_datas = datas.gyro\n",
    "    # acce_uncali = datas.acce_uncali\n",
    "    magn_uncali = datas.magn_uncali # Only use magn for uncalibrated data, as it seems more important in initial modeling result\n",
    "    # gyro_uncali = datas.gyro_uncali\n",
    "    wifi_datas = datas.wifi\n",
    "    ibeacon_datas = datas.ibeacon\n",
    "    # wps = datas.waypoint # not to be used\n",
    "    rel_positions = calc_rel_positions(acce_datas, ahrs_datas)\n",
    "\n",
    "    # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n",
    "\n",
    "    # Make wifi df with wifi_ts\n",
    "    if wifi_datas.shape[0] == 0:\n",
    "        print(\"no wifi data at: \", path)\n",
    "        return []\n",
    "\n",
    "    wifi_datas = extract_nearest_wifi(wifi_datas, timestamp)\n",
    "\n",
    "    dfs = []\n",
    "    df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n",
    "    for wifi_ts, g in df.groupby(0):\n",
    "        g = g.drop_duplicates(subset=1)\n",
    "        tmp = g.iloc[:,1:]\n",
    "        feat = tmp.set_index(1).T\n",
    "        feat[\"wifi_ts\"] = wifi_ts\n",
    "\n",
    "        # get closest wps\n",
    "        feat[\"wps_diff\"] = abs(int(wifi_ts) - int(timestamp))\n",
    "        feat[\"x\"] = np.nan\n",
    "        feat[\"y\"] = np.nan\n",
    "\n",
    "        # get average of acce within 250ms\n",
    "        acce_avgs = extract_imu_rep(acce_datas, wifi_ts)\n",
    "        feat[\"acce_x_avg\"] = acce_avgs[0]\n",
    "        feat[\"acce_y_avg\"] = acce_avgs[1]\n",
    "        feat[\"acce_z_avg\"] = acce_avgs[2]\n",
    "\n",
    "        # get average of magn within 250ms\n",
    "        magn_avgs = extract_imu_rep(magn_datas, wifi_ts)\n",
    "        feat[\"magn_x_avg\"] = magn_avgs[0]\n",
    "        feat[\"magn_y_avg\"] = magn_avgs[1]\n",
    "        feat[\"magn_z_avg\"] = magn_avgs[2]\n",
    "        # get magnetic strength of the 250ms average magn_avg\n",
    "        feat[\"magn_st\"] = extract_one_magn_strength(magn_avgs)\n",
    "\n",
    "        # get average of ahrs within 250ms\n",
    "        ahrs_avgs = extract_imu_rep(ahrs_datas, wifi_ts)\n",
    "        feat[\"ahrs_x_avg\"] = ahrs_avgs[0]\n",
    "        feat[\"ahrs_y_avg\"] = ahrs_avgs[1]\n",
    "        feat[\"ahrs_z_avg\"] = ahrs_avgs[2]\n",
    "\n",
    "        # get average of gyro within 250ms\n",
    "        gyro_avgs = extract_imu_rep(gyro_datas, wifi_ts)\n",
    "        feat[\"gyro_x_avg\"] = gyro_avgs[0]\n",
    "        feat[\"gyro_y_avg\"] = gyro_avgs[1]\n",
    "        feat[\"gyro_z_avg\"] = gyro_avgs[2]\n",
    "\n",
    "        # get average of magn_uncali within 250ms\n",
    "        magn_uncali_avgs = extract_imu_rep(magn_uncali, wifi_ts)\n",
    "        feat[\"magn_u_x_avg\"] = magn_uncali_avgs[0]\n",
    "        feat[\"magn_u_y_avg\"] = magn_uncali_avgs[1]\n",
    "        feat[\"magn_u_z_avg\"] = magn_uncali_avgs[2]\n",
    "        \n",
    "        # get closest relative positions that was worked out with acce and ahrs data\n",
    "        rel_pos = extract_nearest_wps(rel_positions, wifi_ts)\n",
    "        feat[\"rel_diff\"] = rel_pos[0]\n",
    "        feat[\"rel_x\"] = rel_pos[1][1]\n",
    "        feat[\"rel_y\"] = rel_pos[1][2]\n",
    "\n",
    "        # get floor and other path data\n",
    "        feat[\"site_path_timestamp\"] = site_path_timestamp\n",
    "        feat[\"site_id\"] = site_id\n",
    "        feat[\"file_id\"] = file_id\n",
    "        feat[\"floor_int\"] = np.nan\n",
    "        feat[\"floor\"] = np.nan\n",
    "        \n",
    "        dfs.append(feat)\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def make_test_df(zipped_path, site):\n",
    "    try:\n",
    "        dfs_all = pool.map(make_wifi_df_test, tqdm(zipped_path))\n",
    "        dfs_unpack = [row for df in dfs_all for row in df]\n",
    "        wifi_df = pd.concat(dfs_unpack)\n",
    "        wifi_df = shift_columns(SHIFT_COLS_TEST, wifi_df)\n",
    "        wifi_df = wifi_df.fillna(-999)\n",
    "        convert_dtypes(wifi_df, tqdm(INT_COLS), int)\n",
    "        convert_dtypes(wifi_df, tqdm(CAT_COLS), \"category\")\n",
    "        # display(wifi_df.head())\n",
    "        # print(wifi_df.iloc[:, :30].info())\n",
    "        wifi_df.to_csv(f\"./test_3/{site}_test.csv\", index=False)\n",
    "        del wifi_df\n",
    "    except:\n",
    "        print(\"make_test_df at: \", site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get submission file\n",
    "sub_df = pd.read_csv(\"../jupyter/input/sample_submission.csv\")\n",
    "sub_df[[\"site_id\", \"file_id\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
    "sub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n",
    "# sub_df_site_list = sub_df[\"site_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:00<00:00, 60391.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 92.35it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 447.33it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 12276.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 177.32it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 561.86it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 19180.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 147.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 544.55it/s]\n",
      "100%|██████████| 654/654 [00:23<00:00, 27.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.84it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 459.83it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 88102.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.32it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 605.73it/s]\n",
      "100%|██████████| 49/49 [00:00<00:00, 18646.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 274.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 868.81it/s]\n",
      "100%|██████████| 218/218 [00:00<00:00, 89406.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 192.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 637.14it/s]\n",
      "100%|██████████| 527/527 [00:50<00:00, 10.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 152.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 452.00it/s]\n",
      "100%|██████████| 716/716 [01:20<00:00,  8.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.48it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 465.90it/s]\n",
      "100%|██████████| 509/509 [00:00<00:00, 74195.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.93it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 328.40it/s]\n",
      "100%|██████████| 1223/1223 [04:01<00:00,  5.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.29it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 496.96it/s]\n",
      "100%|██████████| 531/531 [00:30<00:00, 17.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 154.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 514.72it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 42672.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.12it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 840.82it/s]\n",
      "100%|██████████| 311/311 [00:00<00:00, 101638.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 433.89it/s]\n",
      "100%|██████████| 171/171 [00:00<00:00, 76154.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 147.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 616.21it/s]\n",
      "100%|██████████| 139/139 [00:00<00:00, 50407.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 260.02it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 678.91it/s]\n",
      "100%|██████████| 380/380 [00:10<00:00, 35.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 154.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 520.36it/s]\n",
      "100%|██████████| 386/386 [00:00<00:00, 150143.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 150.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 452.35it/s]\n",
      "100%|██████████| 573/573 [00:22<00:00, 25.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 129.48it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 470.49it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 60625.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 280.24it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 837.24it/s]\n",
      "100%|██████████| 445/445 [00:00<00:00, 110304.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 92.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 496.47it/s]\n",
      "100%|██████████| 778/778 [02:10<00:00,  5.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 77.91it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 441.47it/s]\n",
      "100%|██████████| 923/923 [01:13<00:00, 12.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 379.98it/s]\n",
      "100%|██████████| 648/648 [00:23<00:00, 28.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 473.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to extract data:  3284.21510386467\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "pool = Pool(num_cores)\n",
    "\n",
    "# kaggle nb -> 100 records:  33.5 sec\n",
    "# gcp -> 100 records: 31.0 sec\n",
    "# gcp -> full (train 5): 1286.5 sec (~30min)\n",
    "# gcp -> full (train 6): 3284.2 sec\n",
    "\n",
    "# comment out to run all\n",
    "# sub_df = sub_df.sample(n=100)\n",
    "# sub_df = sub_df.iloc[:9, :]\n",
    "test_sites = sub_df[\"site_id\"].unique()\n",
    "\n",
    "# Run generator for each building\n",
    "for site in test_sites:\n",
    "    sub_df_filtered = sub_df[sub_df[\"site_id\"] == site]\n",
    "    site_file_zip = list(zip(sub_df_filtered[\"site_id\"], \\\n",
    "                             sub_df_filtered[\"file_id\"], \\\n",
    "                             sub_df_filtered[\"timestamp\"], \\\n",
    "                             sub_df_filtered[\"site_path_timestamp\"]))\n",
    "    make_test_df(site_file_zip, site)\n",
    "\n",
    "# display(wifi_df.head())\n",
    "\n",
    "print(\"time to extract data: \", time.time() - start)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(f\"num_cores={num_cores}\")\n",
    "# pool = Pool(num_cores)\n",
    "\n",
    "# # 10 paths:  6.070369720458984\n",
    "# # 100 paths:  87.05400061607361\n",
    "# # dfs_all = pool.map(make_wifi_df, tqdm(train_paths[:TRAIN_NUM]))\n",
    "# dfs_all = pool.map(make_wifi_df, tqdm(grouped_paths_list[:10]))\n",
    "\n",
    "# # time to process:  11.514546155929565\n",
    "# # dfs_all = []\n",
    "# # for path in train_paths[:TRAIN_NUM]:\n",
    "# #     dfs_all.append(make_wifi_df(path))\n",
    "\n",
    "# print(len(dfs_all))\n",
    "# print(\"time to extract data: \", time.time() - start)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# pool = Pool(num_cores)\n",
    "\n",
    "# # Do this for each building\n",
    "\n",
    "# # 10 paths:  8.992910146713257\n",
    "# # 100 paths:  2454.589078426361\n",
    "# dfs_unpack = [row for df in dfs_all for row in df]\n",
    "# wifi_df = pd.concat(dfs_unpack)\n",
    "\n",
    "# print(\"time for df conversion: \", time.time() - start)\n",
    "# print(len(wifi_df.columns))\n",
    "# print(len(wifi_df))\n",
    "# display(wifi_df.head())\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wifi_df.iloc[:,:50].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # move columns\n",
    "# cols = [\"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n",
    "#         \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n",
    "#         \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n",
    "\n",
    "# for col in cols:\n",
    "#     df_cols = list(wifi_df.columns)\n",
    "#     df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n",
    "#     wifi_df = wifi_df[df_cols]\n",
    "  \n",
    "# # Fillna\n",
    "# wifi_df = wifi_df.fillna(-999)\n",
    "\n",
    "# display(wifi_df.head())\n",
    "# print(len(wifi_df))\n",
    "\n",
    "# print(\"time to shift columns: \", time.time() - start)\n",
    "# print(wifi_df.iloc[:,:50].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"available RAM:\", psutil.virtual_memory())\n",
    "\n",
    "# train_file_name = \"indoor_train_5.pkl\"\n",
    "\n",
    "# with open(train_file_name, \"wb\") as file:\n",
    "#     pickle.dump(wifi_df, file)\n",
    "\n",
    "# del wifi_df\n",
    "# del dfs_unpack\n",
    "# del dfs_all\n",
    "# gc.collect()\n",
    "\n",
    "# print(\"available RAM after cleanup:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data it back in\n",
    "# train_file_name = \"indoor_train_5.pkl\"\n",
    "\n",
    "# with open(train_file_name, \"rb\") as file:\n",
    "#     df_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"df len: \", len(df_train), \"\\n\")\n",
    "# print(\"site_id nunique: \", df_train[\"site_id\"].nunique(), \"\\n\")\n",
    "# print(\"site_id value_counts: \", df_train[\"site_id\"].value_counts(), \"\\n\")\n",
    "# print(\"file_id nunique: \", df_train[\"file_id\"].nunique(), \"\\n\")\n",
    "# print(\"x value_counts: \", df_train[\"x\"].value_counts(), \"\\n\")\n",
    "# print(\"y value_counts: \", df_train[\"y\"].value_counts(), \"\\n\")\n",
    "# print(\"wifi_ts nunique: \", df_train[\"wifi_ts\"].nunique(), \"\\n\")\n",
    "# print(\"wps_diff nunique: \", df_train[\"wps_diff\"].nunique(), \"\\n\")\n",
    "# display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_pp = df_train.loc[:, [\"site_id\", \"x\", \"y\", \"acce_x_avg\", \"acce_y_avg\", \"acce_z_avg\"]]\n",
    "# display(df_train_pp.head())\n",
    "# sns.pairplot(df_train_pp, hue=\"site_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the wps_diff distribution\n",
    "# # Need to filter out those wps that are above 5000ms difference from wifi_ts\n",
    "# f, ax = plt.subplots(figsize=(8, 8))\n",
    "# f.patch.set_facecolor(\"white\")\n",
    "# sns.distplot(df_train[\"wps_diff\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_slim = df_train[df_train[\"wps_diff\"] < WPS_CUT]\n",
    "# perc = round(len(df_train_slim)/len(df_train)*100, 2)\n",
    "\n",
    "# print(\"no of records: \", len(df_train))\n",
    "# print(f\"Filter df_train with {WPS_CUT}, it retains {perc} % of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing timestamp distribution\n",
    "\n",
    "# # LabelEncode site_id, file_id, floor_converted, ssid, bssid\n",
    "# # def col_encode(df, cols):\n",
    "# #     for col in cols:\n",
    "# #         le = preprocessing.LabelEncoder()\n",
    "# #         df[\"%s_le\"%col] = le.fit_transform(df[col])\n",
    "\n",
    "# # col_enc = [\"site_id\", \"file_id\", \"wifi_ssid\", \"wifi_bssid\", \"beacon_ssid\"]\n",
    "# # col_encode(df_train, tqdm(col_enc))\n",
    "\n",
    "# # convert data types of certain columns\n",
    "# def convert_dtypes(df, col_list, dtype):\n",
    "#     for col in col_list:\n",
    "#         df[col] = df[col].astype(dtype)\n",
    "\n",
    "# convert_dtypes(df_train, tqdm([\"wifi_ts\"]), int)\n",
    "# convert_dtypes(df_train, tqdm([\"file_id\", \"site_id\", \"floor\"]), \"category\")\n",
    "\n",
    "# # Check\n",
    "# display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Methods for preprocessing train data: Timestamp handling\n",
    "# def find_diff_ts(ts, data):\n",
    "#     data_ts = data[0]\n",
    "#     diff_ts = int(data_ts) - int(ts)\n",
    "#     return diff_ts\n",
    "\n",
    "# def find_start_ts(path):\n",
    "#     with open(path, 'r', encoding='utf-8') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     for line_data in lines:\n",
    "#         line_data = line_data.strip()\n",
    "#         m = re.search(r\"(?<=startTime.)(.*)\", line_data)\n",
    "#         start_ts = m.groups(0)\n",
    "#         if m:\n",
    "#             return (start_ts[0])\n",
    "\n",
    "# def find_smallest_diff(t, data):\n",
    "#     if data.size == 0:\n",
    "#         return np.array([])\n",
    "#     else:\n",
    "#         data_ts = data[:, [0]]\n",
    "#         diff = []\n",
    "#         for ts in data_ts:\n",
    "#             diff.append(abs(int(t) - int(ts)))\n",
    "#         closest_index = np.argmin(diff) # if multiple records have the same value..?\n",
    "#         return data[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method for preprocessing train data: splitting acce/ahrs/gyro/magn\n",
    "# def split_axis(data, start_ts):\n",
    "#     if data.size == 0:\n",
    "#         # print(\"no axis data\")\n",
    "#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "#     else:\n",
    "#         data_ts = data[0]\n",
    "#         diff_ts = int(data[0]) - int(start_ts)\n",
    "#         x_axis = data[1]\n",
    "#         y_axis = data[2]\n",
    "#         z_axis = data[3]\n",
    "#         try:\n",
    "#             accuracy = data[4]\n",
    "#         except IndexError:\n",
    "#             accuracy = np.nan\n",
    "#         return [data_ts, diff_ts, x_axis, y_axis, z_axis, accuracy]\n",
    "\n",
    "# # Method for preprocessing train data: splitting wifi\n",
    "# def split_wifi(data, start_ts):\n",
    "#     if data.size == 0:\n",
    "#         # print(\"no wifi data\")\n",
    "#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "#     else:\n",
    "#         data_ts = data[0]\n",
    "#         diff_ts = int(data[0]) - int(start_ts)\n",
    "#         ssid = data[1]\n",
    "#         bssid = data[2]\n",
    "#         rssi = data[3]\n",
    "#         if len(data) > 5:\n",
    "#             freq = data[4]\n",
    "#             last_seen_ts = data[5]\n",
    "#         else:\n",
    "#             freq = np.nan\n",
    "#             last_seen_ts = data[-1]\n",
    "#         return [data_ts, diff_ts, ssid, bssid, rssi, freq, last_seen_ts]\n",
    "\n",
    "# # Method for preprocessing train data: splitting ibeacon\n",
    "# def split_beacon(data, start_ts):\n",
    "#     if data.size == 0:\n",
    "#         # print(\"no beacon data\")\n",
    "#         return [np.nan, np.nan, np.nan, np.nan]\n",
    "#     else:\n",
    "#         data_ts = data[0]\n",
    "#         diff_ts = int(data[0]) - int(start_ts)\n",
    "#         ssid = data[1]\n",
    "#         rssi = data[2]\n",
    "#         return [data_ts, diff_ts, ssid, rssi]\n",
    "\n",
    "# # Method for preprocessing train data: calc rel pos\n",
    "# def split_rel_pos(data, start_ts):\n",
    "#     if data.size == 0:\n",
    "#         # print(\"no rel_pos data\")\n",
    "#         return [np.nan, np.nan, np.nan, np.nan]\n",
    "#     else:\n",
    "#         data_ts = data[0]\n",
    "#         diff_ts = int(data[0]) - int(start_ts)\n",
    "#         x_axis = data[1]\n",
    "#         y_axis = data[2]\n",
    "#         return [data_ts, diff_ts, x_axis, y_axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract path and other data\n",
    "# def extract_path(path, floor_map):\n",
    "#     # split path\n",
    "#     try:\n",
    "#         ex_path = f\"{path}\"\n",
    "#         ex_paths = ex_path.split(\"/\")\n",
    "#         site_id = ex_paths[4]\n",
    "#         floor = ex_paths[5]\n",
    "#         f = floor_map[floor]\n",
    "#         file_id = ex_paths[6].split(\".\")[0]\n",
    "#         return [site_id, file_id, f, floor]\n",
    "#     except:\n",
    "#         print(\"extract_path error\")\n",
    "\n",
    "# # Definitely needs to be refactored\n",
    "# def extract_data(path):\n",
    "#     start_ts = find_start_ts(path)\n",
    "#     path_datas = read_data_file(path)\n",
    "#     acce = path_datas.acce\n",
    "#     ahrs = path_datas.ahrs\n",
    "#     magn = path_datas.magn\n",
    "#     gyro = path_datas.gyro\n",
    "#     acce_uncali = path_datas.acce_uncali\n",
    "#     magn_uncali = path_datas.magn_uncali\n",
    "#     gyro_uncali = path_datas.gyro_uncali\n",
    "#     wifi = path_datas.wifi\n",
    "#     wps = path_datas.waypoint\n",
    "#     ibeacon = path_datas.ibeacon\n",
    "#     rel_positions = calc_rel_positions(acce, ahrs)\n",
    "\n",
    "#     # Changed from: just extracting wps time stamps -> take all acce uncalib timestamps\n",
    "#     # ts = np.unique(wps[:, [0]])\n",
    "#     if acce_uncali.any():\n",
    "#         # print(\"acce_uncali\")\n",
    "#         ts = np.unique(acce_uncali[:, [0]]) # take uncalibrated access, as sometimes access has less data\n",
    "#     elif acce.any():\n",
    "#         # print(\"acce\")\n",
    "#         ts = np.unique(acce[:, [0]])\n",
    "#     else:\n",
    "#         print(\"no acce or acce_uncali\")\n",
    "\n",
    "#     # extract data for each timestamp of waypoints\n",
    "#     res = []\n",
    "#     for t in ts:\n",
    "#         try:\n",
    "#             wp_closest = find_smallest_diff(t, wps)\n",
    "#             closest_wp_ts = wp_closest[0]\n",
    "#             diff_ts_wp_ts = abs(int(t) - int(closest_wp_ts))\n",
    "#             # time_stamp_cut = 2000, only the records within 2 sec of waypoint are kept\n",
    "#             if diff_ts_wp_ts < time_stamp_cut:\n",
    "#                 # flag to indicate how close the data point is to the wps\n",
    "#                 # print(\"diff_ts_wp_ts\", diff_ts_wp_ts)\n",
    "#                 within_100ms = True if abs(diff_ts_wp_ts) <= 100 else False\n",
    "#                 within_200ms = True if abs(diff_ts_wp_ts) <= 200 else False\n",
    "#                 x = wp_closest[1]\n",
    "#                 y = wp_closest[2]\n",
    "#                 # print(\"x, y: \", x, y)\n",
    "#                 diff_start_ts = int(t) - int(start_ts)\n",
    "#                 diff_start_wp_ts = int(closest_wp_ts) - int(start_ts)\n",
    "#                 # print(\"diff_start_ts, diff_start_wp_ts: \", diff_start_ts, diff_start_wp_ts)\n",
    "#                 acce_closest = split_axis(find_smallest_diff(t, acce), start_ts)\n",
    "#                 ahrs_closest = split_axis(find_smallest_diff(t, ahrs), start_ts)\n",
    "#                 magn_closest = split_axis(find_smallest_diff(t, magn), start_ts)\n",
    "#                 magn_closest.append(extract_one_magn_strength(magn_closest)) # append magnetic strength only for the magn data\n",
    "#                 gyro_closest = split_axis(find_smallest_diff(t, gyro), start_ts)\n",
    "#                 # print(\"acce: \", acce_closest)\n",
    "#                 # print(\"ahrs: \", ahrs_closest)\n",
    "#                 # print(\"magn: \", magn_closest)\n",
    "#                 # print(\"gyro: \", gyro_closest)\n",
    "#                 acce_u_closest = split_axis(find_smallest_diff(t, acce_uncali), start_ts)\n",
    "#                 magn_u_closest = split_axis(find_smallest_diff(t, magn_uncali), start_ts)\n",
    "#                 gyro_u_closest = split_axis(find_smallest_diff(t, gyro_uncali), start_ts)\n",
    "#                 # print(\"acce_u_closest: \", acce_u_closest)\n",
    "#                 # print(\"magn_u_closest: \", magn_u_closest)\n",
    "#                 # print(\"gyro_u_closest: \", gyro_u_closest)\n",
    "#                 wifi_closest = split_wifi(find_smallest_diff(t, wifi), start_ts)\n",
    "#                 if len(ibeacon) > 0:\n",
    "#                     beacon_closest = split_beacon(find_smallest_diff(t, ibeacon), start_ts)\n",
    "#                 else:\n",
    "#                     beacon_closest = [np.nan, np.nan, np.nan, np.nan]\n",
    "#                 rel_pos = split_rel_pos(find_smallest_diff(t, rel_positions), start_ts)\n",
    "#                 # print([t, x, y, int(closest_wp_ts), acce_closest, acce_u_closest])\n",
    "#                 res.append([int(t), start_ts, diff_start_ts, x, y, int(closest_wp_ts), diff_start_wp_ts, diff_ts_wp_ts, within_100ms, within_200ms] + \\\n",
    "#                            acce_closest + ahrs_closest + magn_closest + gyro_closest + \\\n",
    "#                            acce_u_closest + magn_u_closest + gyro_u_closest + \\\n",
    "#                            wifi_closest + beacon_closest + rel_pos\n",
    "#                           )\n",
    "#             else:\n",
    "#                 # print(\"no wp made it through timestamp cut\")\n",
    "#                 continue\n",
    "#         except Exception as exc:\n",
    "#             pass\n",
    "#             # print(\"Error message: \", exc)\n",
    "#             # print(\"extract_test_data error\")\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%timeit\n",
    "\n",
    "# # 5.55 ms ± 1.76 ms per loop\n",
    "# path, site, floorNo, floor_plan_filename, \\\n",
    "# json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n",
    "\n",
    "# def one_trace_to_rows(path, floor_map):\n",
    "#     try:\n",
    "#         path_info = extract_path(path, floor_map)\n",
    "#         data = extract_data(path)\n",
    "#         # rows = list(itertools.chain(path_info, *data))\n",
    "#         rows = []\n",
    "#         for d in data:\n",
    "#             row = path_info + d\n",
    "#             rows.append(row)\n",
    "#             # print(\"row: \", row)\n",
    "#         return rows\n",
    "#     except:\n",
    "#         print(\"one_trace_to_rows error at: \", path)\n",
    "\n",
    "# # path -> train/5cd56bdbe2acfd2d33b663c0/L3/5dfc8108241c3600064049b9.txt\n",
    "# # time w/ for loop with 1 train_path -> 11.6\n",
    "# # time w/ itertools.chain for 1 train_path -> 11.8\n",
    "# start = time.time()\n",
    "# path_info = extract_path(path, floor_map)\n",
    "# print(\"path: \", path_info)\n",
    "# rows = one_trace_to_rows(path, floor_map)\n",
    "# print(\"time to process one train_path\", time.time() - start)\n",
    "# #print(\"col count: \", len(rows[0]))\n",
    "# print(\"rows: \", rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run row making function for all training paths\n",
    "# # print(train_paths[:10])\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# all_rows = []\n",
    "# for train_path in train_paths[:10]:\n",
    "#     rows = one_trace_to_rows(train_path, floor_map)\n",
    "#     all_rows.extend(rows)\n",
    "\n",
    "# one_trace_df = pd.DataFrame(all_rows)\n",
    "# display(len(one_trace_df))\n",
    "\n",
    "# # Data below are the time it took to create the old version of training data (only waypoints)\n",
    "# # without Pool\n",
    "# # 10 -> 1.64 sec\n",
    "# # 100 -> 28.12 sec\n",
    "# # 1000 -> 286.67 sec\n",
    "# # to process training (~26,000 files) -> ~7500 sec (~2hours)\n",
    "# print(time.time() - start)\n",
    "\n",
    "# with Pool\n",
    "# no need for wrapper with pool.starmap -> https://qiita.com/okiyuki99/items/a54797cb44eb4ae571f6\n",
    "\n",
    "# Memo about Pool\n",
    "# with Pool\n",
    "# 10 -> 1.09 sec\n",
    "# 100 -> 12.35 sec\n",
    "# 1000 -> 113.87 sec\n",
    "# to process training (~26,000 files) -> ~3000 sec (~50min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if we can make df\n",
    "\n",
    "# # column names\n",
    "# col_names = [\"site_id\", \"file_id\", \"floor_converted\", \"floor\", \\\n",
    "#              \"ts\", \"start_ts\", \"diff_start_ts\", \"x\", \"y\", \\\n",
    "#              \"closest_wp_ts\", \"diff_start_wp_ts\", \"diff_ts_wp_ts\", \"within_100ms\", \"within_200ms\", \\\n",
    "#              \"acce_ts\", \"diff_acce_ts\", \"acce_x\", \"acce_y\", \"acce_z\", \"acce_acc\", \\\n",
    "#              \"ahrs_ts\", \"diff_ahrs_ts\", \"ahrs_x\", \"ahrs_y\", \"ahrs_z\", \"ahrs_acc\", \\\n",
    "#              \"magn_ts\", \"diff_magn_ts\", \"magn_x\", \"magn_y\", \"magn_z\", \"magn_acc\", \"magn_strength\",\\\n",
    "#              \"gyro_ts\", \"diff_gyro_ts\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_acc\", \\\n",
    "#              \"acce_u_ts\", \"diff_acce_u_ts\", \"acce_u_x\", \"acce_u_y\", \"acce_u_z\", \"acce_u_acc\", \\\n",
    "#              \"magn_u_ts\", \"diff_magn_u_ts\", \"magn_u_x\", \"magn_u_y\", \"magn_u_z\", \"magn_u_acc\", \\\n",
    "#              \"gyro_u_ts\", \"diff_gyro_u_ts\", \"gyro_u_x\", \"gyro_u_y\", \"gyro_u_z\", \"gyro_u_acc\", \\\n",
    "#              \"wifi_ts\", \"diff_wifi_ts\", \"wifi_ssid\", \"wifi_bssid\", \"wifi_rssi\", \"wifi_freq\", \"wifi_last_seen_ts\", \\\n",
    "#              \"beacon_ts\", \"diff_beacon_ts\", \"beacon_ssid\", \"beacon_rssi\", \\\n",
    "#              \"rel_ts\", \"diff_rel_ts\", \"rel_x\", \"rel_y\"\n",
    "#             ]\n",
    "\n",
    "# print(len(col_names))\n",
    "\n",
    "# df = pd.DataFrame(rows, columns=col_names)\n",
    "# print(\"df len: \", len(df))\n",
    "# print(\"site_id nunique: \", df[\"site_id\"].nunique())\n",
    "# print(\"file_id nunique: \", df[\"file_id\"].nunique())\n",
    "# print(\"x value_counts: \", df[\"x\"].value_counts())\n",
    "# print(\"y value_counts: \", df[\"y\"].value_counts())\n",
    "# print(\"event ts nunique: \", df[\"ts\"].nunique())\n",
    "# print(\"start ts nunique: \", df[\"start_ts\"].nunique()) # should be one\n",
    "# print(\"diff_ts_wp_ts value_counts: \", df[\"diff_ts_wp_ts\"].value_counts())\n",
    "# print(\"diff_ts_wp_ts nunique: \", df[\"diff_ts_wp_ts\"].nunique())\n",
    "# print(\"within_100ms value_counts: \", df[\"within_100ms\"].value_counts())\n",
    "# print(\"within_100ms nunique: \", df[\"within_100ms\"].nunique())\n",
    "# print(\"within_100ms count: \", df[\"within_100ms\"].count())\n",
    "# print(\"within_200ms value_counts: \", df[\"within_200ms\"].value_counts())\n",
    "# print(\"within_200ms nunique: \", df[\"within_200ms\"].nunique())\n",
    "# print(\"within_200ms count: \", df[\"within_200ms\"].count())\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set pool\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(f\"num_cores={num_cores}\")\n",
    "# # args = [(p, floor_map) for p in train_paths[:train_num]]\n",
    "# args = [(p, floor_map) for p in grouped_paths_list]\n",
    "# pool = Pool(num_cores)\n",
    "\n",
    "# start = time.time()\n",
    "# # w/ 250ms settings, 3 random samples from each site_id\n",
    "# # 2 paths -> 18.7 sec\n",
    "# # 10 paths -> 315 sec (df len is 1994)\n",
    "# # 100 paths -> 708 sec (df len is 7183)\n",
    "# # all ~ 600 paths -> \n",
    "\n",
    "# # errors\n",
    "# # grouped_paths_list -> 100 paths -> site_id: 8 errors, 27 correct\n",
    "# # grouped_paths_list -> 100 paths -> file_id: 23 errors, 77 correct\n",
    "\n",
    "# # all in one go -> xxx sec\n",
    "# # array_split -> 5891.8 sec\n",
    "\n",
    "# # all in one go\n",
    "# # res = pool.starmap(one_trace_to_rows, args)\n",
    "\n",
    "# # split the args\n",
    "# res = []\n",
    "# for arg in tqdm(np.array_split(args, 50)):\n",
    "#     res.extend(pool.starmap(one_trace_to_rows, arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## KEEP THIS CELL FOR LATER REF ##############################\n",
    "\n",
    "# Error in ~20% of the train paths -> caused by not having acces_uncali to create the event timestamps\n",
    "\n",
    "# error files\n",
    "# /5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\n",
    "# /5cd56b5ae2acfd2d33b58548/2F/5cf214bbc852a70008c01607.txt\n",
    "# /5cd56b5ae2acfd2d33b58548/2F/5cf214bda50dc300099d34cc.txt\n",
    "# /5cd56b61e2acfd2d33b58d20/F2/5d085df529994a0008202661.txt\n",
    "# /5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\n",
    "# /5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\n",
    "# /5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\n",
    "# /5cd56b5ae2acfd2d33b5854a/B1/5d073ba64a19c000086c559b.txt\n",
    "# /5cd56b5ae2acfd2d33b5854a/F1/5d07603e4cae4f000a2db525.txt\n",
    "# /5cd56b63e2acfd2d33b591c2/F2/5d0b0668912a980009fe91f2.txt\n",
    "# /5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\n",
    "# /5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\n",
    "# /5cd56b64e2acfd2d33b592b3/F2/5d0c9321c99c56000836df18.txt\n",
    "# /5cd56b64e2acfd2d33b592b3/F3/5d0c9952ea565d0008e34e8b.txt\n",
    "# /5cd56b64e2acfd2d33b592b3/F4/5d0c9d65ea565d0008e34ea2.txt\n",
    "# /5cd56b5ae2acfd2d33b58549/5F/5d0613514a19c000086c432a.txt\n",
    "# /5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\n",
    "# /5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\n",
    "# /5cd56b79e2acfd2d33b5b74e/F3/5d0b015e2f8a26000805ba44.txt\n",
    "# /5cd56b79e2acfd2d33b5b74e/F1/5d0af3452f8a26000805b830.txt\n",
    "# /5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\n",
    "# /5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\n",
    "# /5cd56b6be2acfd2d33b59d1f/F3/5d0896415125450008037c76.txt\n",
    "\n",
    "# base_path = \"../input/indoor-location-navigation/train\"\n",
    "# error_files = [\n",
    "#     \"/5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\",\n",
    "#     \"/5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\",\n",
    "#     \"/5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\",\n",
    "#     \"/5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\",\n",
    "#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\",\n",
    "#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\",\n",
    "#     \"/5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\",\n",
    "#     \"/5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\",\n",
    "#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\",\n",
    "#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\"\n",
    "# ]\n",
    "\n",
    "# working_path = \"../input/indoor-location-navigation/train/5d2709c303f801723c3299ee/1F/5dad7d6daa1d300006faa80c.txt\"\n",
    "# error_paths = [base_path + e for e in error_files]\n",
    "# rows = one_trace_to_rows(error_paths[1], floor_map)\n",
    "# print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# df_train = pd.DataFrame(res[0], columns=col_names)\n",
    "# for r in res[1:]:\n",
    "#     df = pd.DataFrame(r, columns=col_names)\n",
    "#     df_train = df_train.append(df, ignore_index=True)\n",
    "\n",
    "# print(\"time to process\", time.time() - start)\n",
    "# print(\"length of df made\", len(df_train))\n",
    "# display(df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_to_df(row_list):\n",
    "#     df_train = pd.DataFrame(row_list[0], columns=col_names)\n",
    "#     for r in row_list[1:]:\n",
    "#         df = pd.DataFrame(r, columns=col_names)\n",
    "#         df_train = df_train.append(df)\n",
    "#     return df_train\n",
    "\n",
    "# start = time.time()\n",
    "# pool = Pool(num_cores)\n",
    "\n",
    "# df_train = pool.map(list_to_df, tqdm(res))\n",
    "\n",
    "# # print(\"train_path count\", len(train_paths[:train_num]))\n",
    "# print(\"time to process\", time.time() - start)\n",
    "# print(\"length of df made\", len(df_train))\n",
    "# display(df_train.head(10))\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages\n",
    "# Differencing respect to time (as each timestep is unevenly spaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the file in parquet\n",
    "# # https://www.kaggle.com/pedrocouto39/fast-reading-w-pickle-feather-parquet-jay\n",
    "# # https://www.kaggle.com/prmohanty/python-how-to-save-and-load-ml-models\n",
    "\n",
    "# # Saving train data\n",
    "# train_file_name = \"indoor_train_4.pkl\"\n",
    "\n",
    "# with open(train_file_name, \"wb\") as file:\n",
    "#     pickle.dump(df_train, file)\n",
    "\n",
    "# # Save them to output\n",
    "# # df_train.to_csv('df_train_2.csv',index=False)\n",
    "# # df_test.to_csv('df_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data it back in\n",
    "# with open(train_file_name, \"rb\") as file:\n",
    "#     df_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"df len: \", len(df_train), \"\\n\")\n",
    "# print(\"file_id unique: \", (df_train[\"file_id\"].nunique()), \"\\n\")\n",
    "# print(\"site_id unique: \", (df_train[\"site_id\"].nunique()), \"\\n\")\n",
    "# print(\"site_id value_counts: \", (df_train[\"site_id\"].value_counts()))\n",
    "# display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get submission file\n",
    "# sub_df = pd.read_csv(\"/kaggle/input/indoor-location-navigation/sample_submission.csv\")\n",
    "# sub_df[[\"site\", \"file\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
    "# sub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n",
    "# # grouped_df = sub_df.groupby(\"file\").sample(n=2)\n",
    "# # all_file_id = grouped_df[\"file\"].unique()\n",
    "# # print(len(grouped_df))\n",
    "# # print(len(all_file_id))\n",
    "# # display(grouped_df.head())\n",
    "# display(sub_df.head())\n",
    "\n",
    "# test_site_id = sub_df[\"site\"].unique()\n",
    "# train_site_id = df_train[\"site_id\"].unique()\n",
    "# print(test_site_id, \"\\n\")\n",
    "# print(train_site_id, \"\\n\")\n",
    "# a = list(set(test_site_id) & set(train_site_id))\n",
    "# print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
