{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n\nfrom PIL import Image, ImageOps\nfrom skimage import io\nfrom skimage.color import rgba2rgb, rgb2xyz\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nfrom math import floor, ceil\nimport random\n\n# Train data generation\nimport collections\nimport csv\nfrom pathlib import Path\nfrom typing import List, Tuple, Any\n\nimport time\nimport re\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nimport multiprocessing\nfrom multiprocessing import Pool, Manager\n\nimport pickle\nimport math\nimport gc\nimport psutil\nfrom collections import Counter\n\npd.set_option(\"display.max_columns\", 100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Settings and altering components for GCP\n\n# path settings\nroot_path = \"../input/indoor-location-navigation/\"\n# root_path = \"../jupyter/input/\"\ntrain_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\ntest_paths = glob.glob(root_path + \"test\" + \"/*\")\nmetafiles = glob.glob(root_path + \"metadata\" + \"/*\")\n\n# function imports using github repo in kaggle kernels\n# https://www.kaggle.com/getting-started/71642\n!cp -r ../input/indoorlocationcompetition20master/indoor-location-competition-20-master/* ./\nfrom io_f import read_data_file\nfrom compute_f import compute_step_positions, compute_steps, \\\ncompute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq\n\n# import for gcp settings\n# import compute_f\n# import io_f\n# import visualize_f\n# import main\n# from io_f import read_data_file\n# from compute_f import compute_step_positions, compute_steps, \\\n# compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Make directory for saving files\n# !mkdir train\n# !mkdir test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls ./train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter milisecond setting \n# IMU_CUT = 250\nIMU_CUTS = [1000, 2000, 5000]\n# WPS_CUT = 5000\n\n# train number setting\n# TRAIN_NUM = len(train_paths)\n# TRAIN_NUM = round(len(train_paths) / 2)\nTRAIN_NUM = 10\n\n# floor translation\nFLOOR_MAP = {\"B3\":-3,\"B2\":-2,\"B1\":-1,\"F1\":0,\"1F\":0,\"F2\":1,\"2F\":1,\"F3\":2,\"3F\":2,\"F4\":3,\"4F\":3,\n             \"F5\":4,\"5F\":4,\"F6\":5,\"6F\":5,\"F7\":6,\"7F\":6,\"F8\":7,\"8F\": 7,\"F9\":8,\"9F\":8,\"F10\":9,\n             \"B\":0,\"BF\":1,\"BM\":2, \"G\":0, \"M\":0, \"P1\":0,\"P2\":1, \"LG2\":-2,\"LG1\":-1,\"LG\":0,\"LM\":0,\n             \"L1\":1,\"L2\":2,\"L3\":3,\"L4\":4,\"L5\":5,\"L6\":6,\"L7\":7,\"L8\":8,\"L9\":9,\"L10\":10,\"L11\":11}\n\n# Columns to shift to the beginning of df\nSHIFT_COLS = [\"rel_y\", \"rel_x\", \"rel_diff\", \\\n              \"magn_u_z_avg\", \"magn_u_y_avg\", \"magn_u_x_avg\", \\\n              \"gyro_z_avg\", \"gyro_y_avg\", \"gyro_x_avg\", \\\n              \"ahrs_z_avg\", \"ahrs_y_avg\", \"ahrs_x_avg\",  \\\n              \"magn_st\", \"magn_z_avg\", \"magn_y_avg\", \"magn_x_avg\", \\\n              \"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n              \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n              \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n\nSHIFT_COLS_TEST = [\"rel_y\", \"rel_x\", \"rel_diff\", \\\n                   \"magn_u_z_avg\", \"magn_u_y_avg\", \"magn_u_x_avg\", \\\n                   \"gyro_z_avg\", \"gyro_y_avg\", \"gyro_x_avg\", \\\n                   \"ahrs_z_avg\", \"ahrs_y_avg\", \"ahrs_x_avg\",  \\\n                   \"magn_st\", \"magn_z_avg\", \"magn_y_avg\", \"magn_x_avg\", \\\n                   \"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n                   \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n                   \"y\", \"x\", \"wps_diff\", \"wifi_ts\", \"site_path_timestamp\"]\n\nINT_COLS = [\"wifi_ts\"]\nCAT_COLS = [\"file_id\", \"site_id\", \"floor\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess\nprint(\"No. Files in Train: {:,}\".format(len(train_paths)), \"\\n\" +\n      \"No. Files in Test: {:,}\".format(len(test_paths)), \"\\n\" +\n      \"No. of metadata files: {:,}\".format(len(metafiles)))\n\n# Reading in 1 file\ndef pick_example(max_range, paths):\n    ex = random.randint(0, max_range)\n    example_path = paths[ex]\n    path = f\"{example_path}\"\n    paths = path.split(\"/\")\n    site = paths[4]\n    floorNo = paths[5]\n    floor_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_image.png\"\n    json_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_info.json\"\n    with open(json_plan_filename) as json_file:\n        json_data = json.load(json_file)\n    width_meter = json_data[\"map_info\"][\"width\"]\n    height_meter = json_data[\"map_info\"][\"height\"]\n    return path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter\n\npath, site, floorNo, floor_plan_filename, \\\njson_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\nprint(\"example path: \", path)\nprint(\"site: \", site)\nprint(\"floorNo: \", floorNo)\nprint(\"floor_plan_filename: \", floor_plan_filename)\nprint(\"json_plan_filename: \", json_plan_filename)\nprint(\"width: {}, height: {} \".format(width_meter, height_meter))\n\nwith open(path) as p:\n    lines = p.readlines()\nprint(\"No. Lines in 1 example: {:,}\". format(len(lines)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for line in lines:\n#     print(line)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Redefine the data extraction class\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass ReadData:\n    acce: np.ndarray\n    acce_uncali: np.ndarray\n    gyro: np.ndarray\n    gyro_uncali: np.ndarray\n    magn: np.ndarray\n    magn_uncali: np.ndarray\n    ahrs: np.ndarray\n    wifi: np.ndarray\n    ibeacon: np.ndarray\n    waypoint: np.ndarray\n\n\ndef read_data_file_ed(data_filename):\n    acce = []\n    acce_uncali = []\n    gyro = []\n    gyro_uncali = []\n    magn = []\n    magn_uncali = []\n    ahrs = []\n    wifi = []\n    ibeacon = []\n    waypoint = []\n\n    with open(data_filename, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    for line_data in lines:\n        line_data = line_data.strip()\n        if not line_data or line_data[0] == '#':\n            continue\n\n        line_data = line_data.split('\\t')\n\n        if line_data[1] == 'TYPE_ACCELEROMETER':\n            acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_GYROSCOPE':\n            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_ROTATION_VECTOR':\n            ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_WIFI':\n            sys_ts = line_data[0]\n            ssid = line_data[2]\n            bssid = line_data[3]\n            rssi = line_data[4]\n            lastseen_ts = line_data[6]\n            wifi_data = [sys_ts, ssid, bssid, '_'.join([ssid, bssid]), rssi, lastseen_ts]\n            wifi.append(wifi_data)\n            continue\n\n        if line_data[1] == 'TYPE_BEACON':\n            ts = line_data[0]\n            uuid = line_data[2]\n            major = line_data[3]\n            minor = line_data[4]\n            txpower = line_data[5]\n            rssi = line_data[6]\n            distance = line_data[7]\n            mac_address = line_data[-2]\n            beacon_ts = line_data[-1]\n            ibeacon_data = [ts, '_'.join([uuid, major, minor]), txpower, rssi, distance, mac_address, beacon_ts]\n            ibeacon.append(ibeacon_data)\n            continue\n\n        if line_data[1] == 'TYPE_WAYPOINT':\n            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n\n    acce = np.array(acce)\n    acce_uncali = np.array(acce_uncali)\n    gyro = np.array(gyro)\n    gyro_uncali = np.array(gyro_uncali)\n    magn = np.array(magn)\n    magn_uncali = np.array(magn_uncali)\n    ahrs = np.array(ahrs)\n    wifi = np.array(wifi)\n    ibeacon = np.array(ibeacon)\n    waypoint = np.array(waypoint)\n\n    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find out how many wps datapoints and wifi datapoints one floor has\ntrain_path_floor = glob.glob(root_path + \"train\" + \"/*/*/\")\n# train_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\nex = random.randint(0, 6)\nprint(train_path_floor[ex])\nprint(\"no. of files of that floor: \", len(os.listdir(train_path_floor[ex])))\ncount = 0\nfor f in os.listdir(train_path_floor[ex]):\n    file_path = train_path_floor[ex] + f\n    data = read_data_file_ed(file_path)\n    count += len(data.waypoint)\n    \nprint(count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n# show_site_png(root_path, site=site)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature candidate\n# You can't get the waypoint in test, so use acce and ahrs data to calculate relative positions\ndef calc_rel_positions(acce_datas, ahrs_datas):\n    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n    headings = compute_headings(ahrs_datas)\n    stride_lengths = compute_stride_length(step_acce_max_mins)\n    step_headings = compute_step_heading(step_timestamps, headings)\n    rel_positions = compute_rel_positions(stride_lengths, step_headings)\n    # only use del if we don't need timestamps\n    # rel_positions_del = np.delete(rel_positions, 0, 1)\n    return rel_positions\n\n# Feature candidate\n# Modify extract_magnetic_strength from github for one magnetic data point\ndef extract_one_magn_strength(magn_datas):\n    d = np.array(magn_datas)\n    return np.mean(np.sqrt(np.sum(d ** 2, axis=0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path, site, floorNo, floor_plan_filename, \\\n# json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Common methods\ndef extract_imu_rep(imu_data, wifi_ts, imu_cut):\n    imu_ts = imu_data[:, 0].astype(int)\n    diff_list = []\n    for ts in imu_ts:\n        diff = abs(int(wifi_ts) - ts)\n        diff_list.append(diff)\n    # diff_idx = np.argmin(diff_list)\n    # acce_diff_range = [(i,a) for i, a in enumerate(diff_list) if a < cut_line] # uncomment if we need to check acce_diff\n    imu_diff_range = [i for i, a in enumerate(diff_list) if a < imu_cut]\n    imu_filtered = imu_data[imu_diff_range]\n    if imu_filtered.shape[0] == 0:\n        print(\"no imu\")\n        imu_avg_x = np.nan\n        imu_avg_y = np.nan\n        imu_avg_z = np.nan\n    else:\n        imu_avg_x = imu_filtered[:, 1].mean()\n        imu_avg_y = imu_filtered[:, 2].mean()\n        imu_avg_z = imu_filtered[:, 3].mean()\n        #print(imu_avg_x, imu_avg_y, imu_avg_z)\n    return imu_avg_x, imu_avg_y, imu_avg_z\n\ndef shift_columns(cols, df):\n    for col in cols:\n        df_cols = list(df.columns)\n        df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n        df = df[df_cols]\n    return df\n\n# convert data types of certain columns\ndef convert_dtypes(df, col_list, dtype):\n    for col in col_list:\n        df[col] = df[col].astype(dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Train generator\n---","metadata":{}},{"cell_type":"code","source":"# Train specific methods\ndef extract_nearest_wps(wps_data, wifi_ts):\n    wps_ts = wps_data[:, 0].astype(int)\n    diff_list = []\n    for ts in wps_ts:\n        diff = abs(int(wifi_ts) - ts)\n        diff_list.append(diff)\n    diff_idx = np.argmin(diff_list)\n    return diff_list[diff_idx], wps_data[diff_idx]\n\ndef extract_train_path(path):\n    try:\n        ex_path = f\"{path}\"\n        ex_paths = ex_path.split(\"/\")\n        site_id = ex_paths[4]\n        floor = ex_paths[5]\n        f = FLOOR_MAP[floor]\n        file_id = ex_paths[6].split(\".\")[0]\n        return site_id, file_id, f, floor\n    except:\n        print(\"extract_path error\")\n\ndef make_wifi_df_train(path):\n    # First path\n    datas = read_data_file_ed(path)\n    \n    # Put IMU data in dict for later iteration\n    imu_dict = {}\n    imu_dict[\"acce\"] = datas.acce\n    imu_dict[\"magn\"] = datas.magn\n    imu_dict[\"ahrs\"] = datas.ahrs\n    imu_dict[\"gyro\"] = datas.gyro\n    # acce_uncali = datas.acce_uncali\n    imu_dict[\"magn_uncali\"] = datas.magn_uncali # Only use magn for uncalibrated data, as it seems more important in initial modeling result\n    # gyro_uncali = datas.gyro_uncali\n    \n    # Leave the non-imu data as they are\n    wifi_datas = datas.wifi\n    # ibeacon_datas = datas.ibeacon # ibeacon to be used only for test data\n    wps = datas.waypoint\n    rel_positions = calc_rel_positions(imu_dict[\"acce\"], imu_dict[\"ahrs\"])\n    \n    # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n    # Make wifi df with wifi_ts\n    if wifi_datas.shape[0] == 0:\n        print(\"no wifi data at: \", path)\n        return []\n\n    # Make wifi df with wifi_ts\n    dfs = []\n    df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n    for wifi_ts, g in df.groupby(0):\n        g = g.drop_duplicates(subset=1)\n        tmp = g.iloc[:,1:]\n        feat = tmp.set_index(1).T\n        feat[\"wifi_ts\"] = wifi_ts\n\n        # get closest wps\n        closest_wps = extract_nearest_wps(wps, wifi_ts)\n        feat[\"wps_diff\"] = closest_wps[0]\n        feat[\"x\"] = closest_wps[1][1]\n        feat[\"y\"] = closest_wps[1][2]\n\n        # get floor and other path data\n        site_id, file_id, f, floor = extract_train_path(path)\n        feat[\"floor\"] = floor\n        feat[\"floor_int\"] = f\n        feat[\"file_id\"] = file_id\n        feat[\"site_id\"] = site_id\n\n        # Loop over IMU_CUTS\n        for key, imu in imu_dict.items():\n            for imu_cut in IMU_CUTS:\n                imu_avgs = extract_imu_rep(imu, closest_wps[1][0], imu_cut)\n                feat[f\"{key}_{imu_cut}_x_avg\"] = imu_avgs[0]\n                feat[f\"{key}_{imu_cut}_y_avg\"] = imu_avgs[1]\n                feat[f\"{key}_{imu_cut}_z_avg\"] = imu_avgs[2]\n                if key == \"magn\":\n                    feat[f\"{key}_{imu_cut}_st_avg\"] = extract_one_magn_strength(imu_avgs)\n        \n        # get closest relative positions that was worked out with acce and ahrs data\n        rel_pos = extract_nearest_wps(rel_positions, wifi_ts)\n        feat[\"rel_diff\"] = rel_pos[0]\n        feat[\"rel_x\"] = rel_pos[1][1]\n        feat[\"rel_y\"] = rel_pos[1][2]\n        \n        dfs.append(feat)\n    \n    return dfs\n\n\ndef make_train_df(paths_df, site_list):\n    for site in site_list:\n        df = paths_df[paths_df[\"site_id\"] == site]\n        paths = df[\"path\"].unique()\n        # get top bssids for site\n        dfs_all = pool.map(make_wifi_df_train, tqdm(paths))\n        dfs_unpack = [row for df in dfs_all for row in df]\n        wifi_df = pd.concat(dfs_unpack)\n        all_cols = wifi_df.columns\n        cols = [col for col in all_cols if len(col) < 17]\n        cols = reversed(cols)\n        wifi_df = shift_columns(cols, wifi_df)\n        # display(wifi_df.head())\n        wifi_df = wifi_df.fillna(-999)\n        convert_dtypes(wifi_df, tqdm(INT_COLS), int)\n        convert_dtypes(wifi_df, tqdm(CAT_COLS), \"category\")\n        # display(wifi_df.head())\n        # wifi_df.to_csv(f\"./train/{site}_train.csv\", index=False)\n        del wifi_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_path filtering\n\ndef extract_path_for_grouplist(path):\n    ex_path = f\"{path}\"\n    ex_paths = ex_path.split(\"/\")\n    site_id = ex_paths[4]\n    file_id = ex_paths[6].split(\".\")[0]\n    return [path, site_id, file_id]\n\n# create pathlist to be used by 2 types of paths list\npath_list = [extract_path_for_grouplist(item) for item in train_paths]\ndf_paths = pd.DataFrame(path_list, columns=[\"path\", \"site_id\", \"file_id\"])\nsite_id_path_list = df_paths[\"site_id\"].unique()\n\n# grouped_paths_list -> It takes 3 records from every site_id\ngrouped_paths_df = df_paths.groupby(\"site_id\").sample(n=3)\ngrouped_paths_list = list(grouped_paths_df[\"path\"].unique())\ndisplay(grouped_paths_df.head())\nprint(len(df_paths))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nnum_cores = multiprocessing.cpu_count()\npool = Pool(1)\n\n# # Checking purposes\n# # 9 paths, 4 cores -> 28.5 sec\n# # 9 paths, 1 core  -> 42.0 sec\ngrouped_paths_df = grouped_paths_df.iloc[:9,:]\n# grouped_paths_df = grouped_paths_df.sample(n=100)\ntrain_sites_list = grouped_paths_df[\"site_id\"].unique()\nmake_train_df(grouped_paths_df, train_sites_list)\n\n# REAL training\n# train_sites_list = df_paths[\"site_id\"].unique()\n# make_train_df(df_paths, train_sites_list)\n\nprint(\"time to extract data: \", time.time() - start)\npool.close()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Test generator\n---","metadata":{}},{"cell_type":"code","source":"# Test specific methods\ndef extract_nearest_wifi(wifi_datas, timestamp):\n    diff_list = []\n    wifi_ts = wifi_datas[:, 0]\n    for ts in wifi_ts:\n        diff = abs(int(timestamp) - int(ts))\n        diff_list.append(diff)\n    min_value = min(diff_list)\n    diff_indices = [i for i, x in enumerate(diff_list) if x == min_value]\n    wifi_datas = wifi_datas[diff_indices]\n    return wifi_datas\n\ndef extract_correct_ts(ibeacon_datas, timestamp):\n    if ibeacon_datas.shape[0] == 0:\n        print(\"no beacon\")\n        return np.nan\n    else:\n        diff_list = []\n        ibeacon_ts = ibeacon_datas[:, 0]\n        for ts in ibeacon_ts:\n            diff = abs(int(timestamp) - int(ts))\n            diff_list.append(diff)\n        min_value = min(diff_list)\n        diff_indices = [i for i, x in enumerate(diff_list) if x == min_value]\n        ibeacon_datas = ibeacon_datas[diff_indices].flatten()\n        ibeacon_last_ts = int(ibeacon_datas[-1])\n        ibeacon_ts = int(ibeacon_datas[0])\n        correct_ts = ibeacon_last_ts - (ibeacon_ts - int(timestamp))\n        return correct_ts\n\ndef make_wifi_df_test(zipped_paths):\n    site_id, file_id, timestamp, site_path_timestamp = zipped_paths\n    file_path = \"../input/indoor-location-navigation/test/\" + file_id + \".txt\"\n    datas = read_data_file_ed(file_path)\n    \n    # Put IMU data in dict for later iteration\n    imu_dict = {}\n    imu_dict[\"acce\"] = datas.acce\n    imu_dict[\"magn\"] = datas.magn\n    imu_dict[\"ahrs\"] = datas.ahrs\n    imu_dict[\"gyro\"] = datas.gyro\n    # acce_uncali = datas.acce_uncali\n    imu_dict[\"magn_uncali\"] = datas.magn_uncali # Only use magn for uncalibrated data, as it seems more important in initial modeling result\n    # gyro_uncali = datas.gyro_uncali\n    \n    # Leave the non-imu data as they are\n    wifi_datas = datas.wifi\n    ibeacon_datas = datas.ibeacon\n    wps = datas.waypoint\n    rel_positions = calc_rel_positions(imu_dict[\"acce\"], imu_dict[\"ahrs\"])\n    \n    # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n\n    # Make wifi df with wifi_ts\n    if wifi_datas.shape[0] == 0:\n        print(\"no wifi data at: \", path)\n        return []\n\n    # Make wifi df with wifi_ts\n    wifi_datas = extract_nearest_wifi(wifi_datas, timestamp)\n    \n    dfs = []\n    df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n    for wifi_ts, g in df.groupby(0):\n        g = g.drop_duplicates(subset=1)\n        tmp = g.iloc[:,1:]\n        feat = tmp.set_index(1).T\n        feat[\"site_path_timestamp\"] = site_path_timestamp\n        correct_ts = extract_correct_ts(ibeacon_datas, timestamp) # get corrected timestamp using the last timestamp in ibeacon data\n        feat[\"correct_wps_ts\"] = correct_ts if correct_ts is not np.nan else np.nan\n        # feat[\"timestamp\"] = timestamp\n        # feat[\"wifi_ts\"] = wifi_ts\n        feat[\"wifi_ts\"] = correct_ts + (int(wifi_ts) - int(timestamp))\n\n        # get closest wps\n        feat[\"wps_diff\"] = abs(int(wifi_ts) - int(timestamp))\n        feat[\"x\"] = np.nan\n        feat[\"y\"] = np.nan\n\n        # get floor and other path data\n        feat[\"floor\"] = np.nan\n        feat[\"floor_int\"] = np.nan\n        feat[\"file_id\"] = file_id\n        feat[\"site_id\"] = site_id\n\n        # Loop over IMU_CUTS\n        for key, imu in imu_dict.items():\n            for imu_cut in IMU_CUTS:\n                imu_avgs = extract_imu_rep(imu, timestamp, imu_cut)\n                feat[f\"{key}_{imu_cut}_x_avg\"] = imu_avgs[0]\n                feat[f\"{key}_{imu_cut}_y_avg\"] = imu_avgs[1]\n                feat[f\"{key}_{imu_cut}_z_avg\"] = imu_avgs[2]\n                if key == \"magn\":\n                    feat[f\"{key}_{imu_cut}_st_avg\"] = extract_one_magn_strength(imu_avgs)\n        \n        # get closest relative positions that was worked out with acce and ahrs data\n        rel_pos = extract_nearest_wps(rel_positions, wifi_ts)\n        feat[\"rel_diff\"] = rel_pos[0]\n        feat[\"rel_x\"] = rel_pos[1][1]\n        feat[\"rel_y\"] = rel_pos[1][2]\n        \n        dfs.append(feat)\n    \n    return dfs\n\ndef make_test_df(zipped_path, site):\n    dfs_all = pool.map(make_wifi_df_test, tqdm(zipped_path))\n    dfs_unpack = [row for df in dfs_all for row in df]\n    wifi_df = pd.concat(dfs_unpack)\n    all_cols = wifi_df.columns\n    cols = [col for col in all_cols if len(col) < 20]\n    cols = reversed(cols)\n    wifi_df = shift_columns(cols, wifi_df)\n    wifi_df = wifi_df.fillna(-999)\n    convert_dtypes(wifi_df, tqdm(INT_COLS), int)\n    convert_dtypes(wifi_df, tqdm(CAT_COLS), \"category\")\n    # print(wifi_df.iloc[:, :30].info())\n    wifi_df.to_csv(f\"./test/{site}_test.csv\", index=False)\n    del wifi_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get submission file\nsub_df = pd.read_csv(\"/kaggle/input/indoor-location-navigation/sample_submission.csv\")\nsub_df[[\"site_id\", \"file_id\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\nsub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n# sub_df_site_list = sub_df[\"site_id\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nnum_cores = multiprocessing.cpu_count()\npool = Pool(num_cores)\n\n# 100 records:  33.47870922088623 sec\n# comment out to run all\n# sub_df = sub_df.sample(n=100)\nsub_df = sub_df.iloc[:9, :]\ntest_sites = sub_df[\"site_id\"].unique()\n\n# Run generator for each building\nfor site in test_sites:\n    sub_df_filtered = sub_df[sub_df[\"site_id\"] == site]\n    site_file_zip = list(zip(sub_df_filtered[\"site_id\"], \\\n                             sub_df_filtered[\"file_id\"], \\\n                             sub_df_filtered[\"timestamp\"], \\\n                             sub_df_filtered[\"site_path_timestamp\"]))\n    make_test_df(site_file_zip, site)\n\n# display(wifi_df.head())\n\nprint(\"time to extract data: \", time.time() - start)\npool.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start = time.time()\n\n# num_cores = multiprocessing.cpu_count()\n# print(f\"num_cores={num_cores}\")\n# pool = Pool(num_cores)\n\n# # 10 paths:  6.070369720458984\n# # 100 paths:  87.05400061607361\n# # dfs_all = pool.map(make_wifi_df, tqdm(train_paths[:TRAIN_NUM]))\n# dfs_all = pool.map(make_wifi_df, tqdm(grouped_paths_list[:10]))\n\n# # time to process:  11.514546155929565\n# # dfs_all = []\n# # for path in train_paths[:TRAIN_NUM]:\n# #     dfs_all.append(make_wifi_df(path))\n\n# print(len(dfs_all))\n# print(\"time to extract data: \", time.time() - start)\n# pool.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start = time.time()\n\n# num_cores = multiprocessing.cpu_count()\n# pool = Pool(num_cores)\n\n# # Do this for each building\n\n# # 10 paths:  8.992910146713257\n# # 100 paths:  2454.589078426361\n# dfs_unpack = [row for df in dfs_all for row in df]\n# wifi_df = pd.concat(dfs_unpack)\n\n# print(\"time for df conversion: \", time.time() - start)\n# print(len(wifi_df.columns))\n# print(len(wifi_df))\n# display(wifi_df.head())\n# pool.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wifi_df.iloc[:,:50].info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start = time.time()\n\n# # move columns\n# cols = [\"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n#         \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n#         \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n\n# for col in cols:\n#     df_cols = list(wifi_df.columns)\n#     df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n#     wifi_df = wifi_df[df_cols]\n  \n# # Fillna\n# wifi_df = wifi_df.fillna(-999)\n\n# display(wifi_df.head())\n# print(len(wifi_df))\n\n# print(\"time to shift columns: \", time.time() - start)\n# print(wifi_df.iloc[:,:50].info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"available RAM:\", psutil.virtual_memory())\n\n# train_file_name = \"indoor_train_5.pkl\"\n\n# with open(train_file_name, \"wb\") as file:\n#     pickle.dump(wifi_df, file)\n\n# del wifi_df\n# del dfs_unpack\n# del dfs_all\n# gc.collect()\n\n# print(\"available RAM after cleanup:\", psutil.virtual_memory())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load data it back in\n# train_file_name = \"indoor_train_5.pkl\"\n\n# with open(train_file_name, \"rb\") as file:\n#     df_train = pickle.load(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"df len: \", len(df_train), \"\\n\")\n# print(\"site_id nunique: \", df_train[\"site_id\"].nunique(), \"\\n\")\n# print(\"site_id value_counts: \", df_train[\"site_id\"].value_counts(), \"\\n\")\n# print(\"file_id nunique: \", df_train[\"file_id\"].nunique(), \"\\n\")\n# print(\"x value_counts: \", df_train[\"x\"].value_counts(), \"\\n\")\n# print(\"y value_counts: \", df_train[\"y\"].value_counts(), \"\\n\")\n# print(\"wifi_ts nunique: \", df_train[\"wifi_ts\"].nunique(), \"\\n\")\n# print(\"wps_diff nunique: \", df_train[\"wps_diff\"].nunique(), \"\\n\")\n# display(df_train.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_pp = df_train.loc[:, [\"site_id\", \"x\", \"y\", \"acce_x_avg\", \"acce_y_avg\", \"acce_z_avg\"]]\n# display(df_train_pp.head())\n# sns.pairplot(df_train_pp, hue=\"site_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Check the wps_diff distribution\n# # Need to filter out those wps that are above 5000ms difference from wifi_ts\n# f, ax = plt.subplots(figsize=(8, 8))\n# f.patch.set_facecolor(\"white\")\n# sns.distplot(df_train[\"wps_diff\"])\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_slim = df_train[df_train[\"wps_diff\"] < WPS_CUT]\n# perc = round(len(df_train_slim)/len(df_train)*100, 2)\n\n# print(\"no of records: \", len(df_train))\n# print(f\"Filter df_train with {WPS_CUT}, it retains {perc} % of data\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Visualizing timestamp distribution\n\n# # LabelEncode site_id, file_id, floor_converted, ssid, bssid\n# # def col_encode(df, cols):\n# #     for col in cols:\n# #         le = preprocessing.LabelEncoder()\n# #         df[\"%s_le\"%col] = le.fit_transform(df[col])\n\n# # col_enc = [\"site_id\", \"file_id\", \"wifi_ssid\", \"wifi_bssid\", \"beacon_ssid\"]\n# # col_encode(df_train, tqdm(col_enc))\n\n# # convert data types of certain columns\n# def convert_dtypes(df, col_list, dtype):\n#     for col in col_list:\n#         df[col] = df[col].astype(dtype)\n\n# convert_dtypes(df_train, tqdm([\"wifi_ts\"]), int)\n# convert_dtypes(df_train, tqdm([\"file_id\", \"site_id\", \"floor\"]), \"category\")\n\n# # Check\n# display(df_train.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Methods for preprocessing train data: Timestamp handling\n# def find_diff_ts(ts, data):\n#     data_ts = data[0]\n#     diff_ts = int(data_ts) - int(ts)\n#     return diff_ts\n\n# def find_start_ts(path):\n#     with open(path, 'r', encoding='utf-8') as file:\n#         lines = file.readlines()\n\n#     for line_data in lines:\n#         line_data = line_data.strip()\n#         m = re.search(r\"(?<=startTime.)(.*)\", line_data)\n#         start_ts = m.groups(0)\n#         if m:\n#             return (start_ts[0])\n\n# def find_smallest_diff(t, data):\n#     if data.size == 0:\n#         return np.array([])\n#     else:\n#         data_ts = data[:, [0]]\n#         diff = []\n#         for ts in data_ts:\n#             diff.append(abs(int(t) - int(ts)))\n#         closest_index = np.argmin(diff) # if multiple records have the same value..?\n#         return data[closest_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Method for preprocessing train data: splitting acce/ahrs/gyro/magn\n# def split_axis(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no axis data\")\n#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         x_axis = data[1]\n#         y_axis = data[2]\n#         z_axis = data[3]\n#         try:\n#             accuracy = data[4]\n#         except IndexError:\n#             accuracy = np.nan\n#         return [data_ts, diff_ts, x_axis, y_axis, z_axis, accuracy]\n\n# # Method for preprocessing train data: splitting wifi\n# def split_wifi(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no wifi data\")\n#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         ssid = data[1]\n#         bssid = data[2]\n#         rssi = data[3]\n#         if len(data) > 5:\n#             freq = data[4]\n#             last_seen_ts = data[5]\n#         else:\n#             freq = np.nan\n#             last_seen_ts = data[-1]\n#         return [data_ts, diff_ts, ssid, bssid, rssi, freq, last_seen_ts]\n\n# # Method for preprocessing train data: splitting ibeacon\n# def split_beacon(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no beacon data\")\n#         return [np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         ssid = data[1]\n#         rssi = data[2]\n#         return [data_ts, diff_ts, ssid, rssi]\n\n# # Method for preprocessing train data: calc rel pos\n# def split_rel_pos(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no rel_pos data\")\n#         return [np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         x_axis = data[1]\n#         y_axis = data[2]\n#         return [data_ts, diff_ts, x_axis, y_axis]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Extract path and other data\n# def extract_path(path, floor_map):\n#     # split path\n#     try:\n#         ex_path = f\"{path}\"\n#         ex_paths = ex_path.split(\"/\")\n#         site_id = ex_paths[4]\n#         floor = ex_paths[5]\n#         f = floor_map[floor]\n#         file_id = ex_paths[6].split(\".\")[0]\n#         return [site_id, file_id, f, floor]\n#     except:\n#         print(\"extract_path error\")\n\n# # Definitely needs to be refactored\n# def extract_data(path):\n#     start_ts = find_start_ts(path)\n#     path_datas = read_data_file(path)\n#     acce = path_datas.acce\n#     ahrs = path_datas.ahrs\n#     magn = path_datas.magn\n#     gyro = path_datas.gyro\n#     acce_uncali = path_datas.acce_uncali\n#     magn_uncali = path_datas.magn_uncali\n#     gyro_uncali = path_datas.gyro_uncali\n#     wifi = path_datas.wifi\n#     wps = path_datas.waypoint\n#     ibeacon = path_datas.ibeacon\n#     rel_positions = calc_rel_positions(acce, ahrs)\n\n#     # Changed from: just extracting wps time stamps -> take all acce uncalib timestamps\n#     # ts = np.unique(wps[:, [0]])\n#     if acce_uncali.any():\n#         # print(\"acce_uncali\")\n#         ts = np.unique(acce_uncali[:, [0]]) # take uncalibrated access, as sometimes access has less data\n#     elif acce.any():\n#         # print(\"acce\")\n#         ts = np.unique(acce[:, [0]])\n#     else:\n#         print(\"no acce or acce_uncali\")\n\n#     # extract data for each timestamp of waypoints\n#     res = []\n#     for t in ts:\n#         try:\n#             wp_closest = find_smallest_diff(t, wps)\n#             closest_wp_ts = wp_closest[0]\n#             diff_ts_wp_ts = abs(int(t) - int(closest_wp_ts))\n#             # time_stamp_cut = 2000, only the records within 2 sec of waypoint are kept\n#             if diff_ts_wp_ts < time_stamp_cut:\n#                 # flag to indicate how close the data point is to the wps\n#                 # print(\"diff_ts_wp_ts\", diff_ts_wp_ts)\n#                 within_100ms = True if abs(diff_ts_wp_ts) <= 100 else False\n#                 within_200ms = True if abs(diff_ts_wp_ts) <= 200 else False\n#                 x = wp_closest[1]\n#                 y = wp_closest[2]\n#                 # print(\"x, y: \", x, y)\n#                 diff_start_ts = int(t) - int(start_ts)\n#                 diff_start_wp_ts = int(closest_wp_ts) - int(start_ts)\n#                 # print(\"diff_start_ts, diff_start_wp_ts: \", diff_start_ts, diff_start_wp_ts)\n#                 acce_closest = split_axis(find_smallest_diff(t, acce), start_ts)\n#                 ahrs_closest = split_axis(find_smallest_diff(t, ahrs), start_ts)\n#                 magn_closest = split_axis(find_smallest_diff(t, magn), start_ts)\n#                 magn_closest.append(extract_one_magn_strength(magn_closest)) # append magnetic strength only for the magn data\n#                 gyro_closest = split_axis(find_smallest_diff(t, gyro), start_ts)\n#                 # print(\"acce: \", acce_closest)\n#                 # print(\"ahrs: \", ahrs_closest)\n#                 # print(\"magn: \", magn_closest)\n#                 # print(\"gyro: \", gyro_closest)\n#                 acce_u_closest = split_axis(find_smallest_diff(t, acce_uncali), start_ts)\n#                 magn_u_closest = split_axis(find_smallest_diff(t, magn_uncali), start_ts)\n#                 gyro_u_closest = split_axis(find_smallest_diff(t, gyro_uncali), start_ts)\n#                 # print(\"acce_u_closest: \", acce_u_closest)\n#                 # print(\"magn_u_closest: \", magn_u_closest)\n#                 # print(\"gyro_u_closest: \", gyro_u_closest)\n#                 wifi_closest = split_wifi(find_smallest_diff(t, wifi), start_ts)\n#                 if len(ibeacon) > 0:\n#                     beacon_closest = split_beacon(find_smallest_diff(t, ibeacon), start_ts)\n#                 else:\n#                     beacon_closest = [np.nan, np.nan, np.nan, np.nan]\n#                 rel_pos = split_rel_pos(find_smallest_diff(t, rel_positions), start_ts)\n#                 # print([t, x, y, int(closest_wp_ts), acce_closest, acce_u_closest])\n#                 res.append([int(t), start_ts, diff_start_ts, x, y, int(closest_wp_ts), diff_start_wp_ts, diff_ts_wp_ts, within_100ms, within_200ms] + \\\n#                            acce_closest + ahrs_closest + magn_closest + gyro_closest + \\\n#                            acce_u_closest + magn_u_closest + gyro_u_closest + \\\n#                            wifi_closest + beacon_closest + rel_pos\n#                           )\n#             else:\n#                 # print(\"no wp made it through timestamp cut\")\n#                 continue\n#         except Exception as exc:\n#             pass\n#             # print(\"Error message: \", exc)\n#             # print(\"extract_test_data error\")\n#     return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # %%timeit\n\n# # 5.55 ms Â± 1.76 ms per loop\n# path, site, floorNo, floor_plan_filename, \\\n# json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n\n# def one_trace_to_rows(path, floor_map):\n#     try:\n#         path_info = extract_path(path, floor_map)\n#         data = extract_data(path)\n#         # rows = list(itertools.chain(path_info, *data))\n#         rows = []\n#         for d in data:\n#             row = path_info + d\n#             rows.append(row)\n#             # print(\"row: \", row)\n#         return rows\n#     except:\n#         print(\"one_trace_to_rows error at: \", path)\n\n# # path -> train/5cd56bdbe2acfd2d33b663c0/L3/5dfc8108241c3600064049b9.txt\n# # time w/ for loop with 1 train_path -> 11.6\n# # time w/ itertools.chain for 1 train_path -> 11.8\n# start = time.time()\n# path_info = extract_path(path, floor_map)\n# print(\"path: \", path_info)\n# rows = one_trace_to_rows(path, floor_map)\n# print(\"time to process one train_path\", time.time() - start)\n# #print(\"col count: \", len(rows[0]))\n# print(\"rows: \", rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run row making function for all training paths\n# # print(train_paths[:10])\n# import time\n# start = time.time()\n\n# all_rows = []\n# for train_path in train_paths[:10]:\n#     rows = one_trace_to_rows(train_path, floor_map)\n#     all_rows.extend(rows)\n\n# one_trace_df = pd.DataFrame(all_rows)\n# display(len(one_trace_df))\n\n# # Data below are the time it took to create the old version of training data (only waypoints)\n# # without Pool\n# # 10 -> 1.64 sec\n# # 100 -> 28.12 sec\n# # 1000 -> 286.67 sec\n# # to process training (~26,000 files) -> ~7500 sec (~2hours)\n# print(time.time() - start)\n\n# with Pool\n# no need for wrapper with pool.starmap -> https://qiita.com/okiyuki99/items/a54797cb44eb4ae571f6\n\n# Memo about Pool\n# with Pool\n# 10 -> 1.09 sec\n# 100 -> 12.35 sec\n# 1000 -> 113.87 sec\n# to process training (~26,000 files) -> ~3000 sec (~50min)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Check if we can make df\n\n# # column names\n# col_names = [\"site_id\", \"file_id\", \"floor_converted\", \"floor\", \\\n#              \"ts\", \"start_ts\", \"diff_start_ts\", \"x\", \"y\", \\\n#              \"closest_wp_ts\", \"diff_start_wp_ts\", \"diff_ts_wp_ts\", \"within_100ms\", \"within_200ms\", \\\n#              \"acce_ts\", \"diff_acce_ts\", \"acce_x\", \"acce_y\", \"acce_z\", \"acce_acc\", \\\n#              \"ahrs_ts\", \"diff_ahrs_ts\", \"ahrs_x\", \"ahrs_y\", \"ahrs_z\", \"ahrs_acc\", \\\n#              \"magn_ts\", \"diff_magn_ts\", \"magn_x\", \"magn_y\", \"magn_z\", \"magn_acc\", \"magn_strength\",\\\n#              \"gyro_ts\", \"diff_gyro_ts\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_acc\", \\\n#              \"acce_u_ts\", \"diff_acce_u_ts\", \"acce_u_x\", \"acce_u_y\", \"acce_u_z\", \"acce_u_acc\", \\\n#              \"magn_u_ts\", \"diff_magn_u_ts\", \"magn_u_x\", \"magn_u_y\", \"magn_u_z\", \"magn_u_acc\", \\\n#              \"gyro_u_ts\", \"diff_gyro_u_ts\", \"gyro_u_x\", \"gyro_u_y\", \"gyro_u_z\", \"gyro_u_acc\", \\\n#              \"wifi_ts\", \"diff_wifi_ts\", \"wifi_ssid\", \"wifi_bssid\", \"wifi_rssi\", \"wifi_freq\", \"wifi_last_seen_ts\", \\\n#              \"beacon_ts\", \"diff_beacon_ts\", \"beacon_ssid\", \"beacon_rssi\", \\\n#              \"rel_ts\", \"diff_rel_ts\", \"rel_x\", \"rel_y\"\n#             ]\n\n# print(len(col_names))\n\n# df = pd.DataFrame(rows, columns=col_names)\n# print(\"df len: \", len(df))\n# print(\"site_id nunique: \", df[\"site_id\"].nunique())\n# print(\"file_id nunique: \", df[\"file_id\"].nunique())\n# print(\"x value_counts: \", df[\"x\"].value_counts())\n# print(\"y value_counts: \", df[\"y\"].value_counts())\n# print(\"event ts nunique: \", df[\"ts\"].nunique())\n# print(\"start ts nunique: \", df[\"start_ts\"].nunique()) # should be one\n# print(\"diff_ts_wp_ts value_counts: \", df[\"diff_ts_wp_ts\"].value_counts())\n# print(\"diff_ts_wp_ts nunique: \", df[\"diff_ts_wp_ts\"].nunique())\n# print(\"within_100ms value_counts: \", df[\"within_100ms\"].value_counts())\n# print(\"within_100ms nunique: \", df[\"within_100ms\"].nunique())\n# print(\"within_100ms count: \", df[\"within_100ms\"].count())\n# print(\"within_200ms value_counts: \", df[\"within_200ms\"].value_counts())\n# print(\"within_200ms nunique: \", df[\"within_200ms\"].nunique())\n# print(\"within_200ms count: \", df[\"within_200ms\"].count())\n# display(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Set pool\n# num_cores = multiprocessing.cpu_count()\n# print(f\"num_cores={num_cores}\")\n# # args = [(p, floor_map) for p in train_paths[:train_num]]\n# args = [(p, floor_map) for p in grouped_paths_list]\n# pool = Pool(num_cores)\n\n# start = time.time()\n# # w/ 250ms settings, 3 random samples from each site_id\n# # 2 paths -> 18.7 sec\n# # 10 paths -> 315 sec (df len is 1994)\n# # 100 paths -> 708 sec (df len is 7183)\n# # all ~ 600 paths -> \n\n# # errors\n# # grouped_paths_list -> 100 paths -> site_id: 8 errors, 27 correct\n# # grouped_paths_list -> 100 paths -> file_id: 23 errors, 77 correct\n\n# # all in one go -> xxx sec\n# # array_split -> 5891.8 sec\n\n# # all in one go\n# # res = pool.starmap(one_trace_to_rows, args)\n\n# # split the args\n# res = []\n# for arg in tqdm(np.array_split(args, 50)):\n#     res.extend(pool.starmap(one_trace_to_rows, arg))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################## KEEP THIS CELL FOR LATER REF ##############################\n\n# Error in ~20% of the train paths -> caused by not having acces_uncali to create the event timestamps\n\n# error files\n# /5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\n# /5cd56b5ae2acfd2d33b58548/2F/5cf214bbc852a70008c01607.txt\n# /5cd56b5ae2acfd2d33b58548/2F/5cf214bda50dc300099d34cc.txt\n# /5cd56b61e2acfd2d33b58d20/F2/5d085df529994a0008202661.txt\n# /5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\n# /5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\n# /5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\n# /5cd56b5ae2acfd2d33b5854a/B1/5d073ba64a19c000086c559b.txt\n# /5cd56b5ae2acfd2d33b5854a/F1/5d07603e4cae4f000a2db525.txt\n# /5cd56b63e2acfd2d33b591c2/F2/5d0b0668912a980009fe91f2.txt\n# /5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\n# /5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\n# /5cd56b64e2acfd2d33b592b3/F2/5d0c9321c99c56000836df18.txt\n# /5cd56b64e2acfd2d33b592b3/F3/5d0c9952ea565d0008e34e8b.txt\n# /5cd56b64e2acfd2d33b592b3/F4/5d0c9d65ea565d0008e34ea2.txt\n# /5cd56b5ae2acfd2d33b58549/5F/5d0613514a19c000086c432a.txt\n# /5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\n# /5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\n# /5cd56b79e2acfd2d33b5b74e/F3/5d0b015e2f8a26000805ba44.txt\n# /5cd56b79e2acfd2d33b5b74e/F1/5d0af3452f8a26000805b830.txt\n# /5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\n# /5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\n# /5cd56b6be2acfd2d33b59d1f/F3/5d0896415125450008037c76.txt\n\n# base_path = \"../input/indoor-location-navigation/train\"\n# error_files = [\n#     \"/5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\",\n#     \"/5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\",\n#     \"/5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\",\n#     \"/5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\",\n#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\",\n#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\",\n#     \"/5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\",\n#     \"/5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\",\n#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\",\n#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\"\n# ]\n\n# working_path = \"../input/indoor-location-navigation/train/5d2709c303f801723c3299ee/1F/5dad7d6daa1d300006faa80c.txt\"\n# error_paths = [base_path + e for e in error_files]\n# rows = one_trace_to_rows(error_paths[1], floor_map)\n# print(rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start = time.time()\n\n# df_train = pd.DataFrame(res[0], columns=col_names)\n# for r in res[1:]:\n#     df = pd.DataFrame(r, columns=col_names)\n#     df_train = df_train.append(df, ignore_index=True)\n\n# print(\"time to process\", time.time() - start)\n# print(\"length of df made\", len(df_train))\n# display(df_train.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def list_to_df(row_list):\n#     df_train = pd.DataFrame(row_list[0], columns=col_names)\n#     for r in row_list[1:]:\n#         df = pd.DataFrame(r, columns=col_names)\n#         df_train = df_train.append(df)\n#     return df_train\n\n# start = time.time()\n# pool = Pool(num_cores)\n\n# df_train = pool.map(list_to_df, tqdm(res))\n\n# # print(\"train_path count\", len(train_paths[:train_num]))\n# print(\"time to process\", time.time() - start)\n# print(\"length of df made\", len(df_train))\n# display(df_train.head(10))\n# pool.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate moving averages\n# Differencing respect to time (as each timestep is unevenly spaced)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save the file in parquet\n# # https://www.kaggle.com/pedrocouto39/fast-reading-w-pickle-feather-parquet-jay\n# # https://www.kaggle.com/prmohanty/python-how-to-save-and-load-ml-models\n\n# # Saving train data\n# train_file_name = \"indoor_train_4.pkl\"\n\n# with open(train_file_name, \"wb\") as file:\n#     pickle.dump(df_train, file)\n\n# # Save them to output\n# # df_train.to_csv('df_train_2.csv',index=False)\n# # df_test.to_csv('df_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load data it back in\n# with open(train_file_name, \"rb\") as file:\n#     df_train = pickle.load(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"df len: \", len(df_train), \"\\n\")\n# print(\"file_id unique: \", (df_train[\"file_id\"].nunique()), \"\\n\")\n# print(\"site_id unique: \", (df_train[\"site_id\"].nunique()), \"\\n\")\n# print(\"site_id value_counts: \", (df_train[\"site_id\"].value_counts()))\n# display(df_train.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get submission file\n# sub_df = pd.read_csv(\"/kaggle/input/indoor-location-navigation/sample_submission.csv\")\n# sub_df[[\"site\", \"file\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n# sub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n# # grouped_df = sub_df.groupby(\"file\").sample(n=2)\n# # all_file_id = grouped_df[\"file\"].unique()\n# # print(len(grouped_df))\n# # print(len(all_file_id))\n# # display(grouped_df.head())\n# display(sub_df.head())\n\n# test_site_id = sub_df[\"site\"].unique()\n# train_site_id = df_train[\"site_id\"].unique()\n# print(test_site_id, \"\\n\")\n# print(train_site_id, \"\\n\")\n# a = list(set(test_site_id) & set(train_site_id))\n# print(a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}