{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "indoor_model_1ConvTransformer_train_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_deO9NH18_Kb"
      },
      "source": [
        "# Mounting GCS to colab\n",
        "# https://stackoverflow.com/questions/51715268/how-to-import-data-from-google-cloud-storage-to-google-colab\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_w17mjH-0gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8e06e1-9710-43a2-8219-e4f3d33b828c"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  61878      0 --:--:-- --:--:-- --:--:-- 61878\n",
            "OK\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 55 not upgraded.\n",
            "Need to get 10.8 MB of archives.\n",
            "After this operation, 23.1 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.35.0_amd64.deb ...\n",
            "Unpacking gcsfuse (0.35.0) ...\n",
            "Setting up gcsfuse (0.35.0) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoG6QV8P_FC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29732367-1752-4994-f7f8-b55489267f89"
      },
      "source": [
        "!mkdir colab_indoor\n",
        "!gcsfuse indoor-data colab_indoor\n",
        "# !mkdir colab_indoor/train_4\n",
        "# !gcsfuse indoor-data/train_4 colab_indoor/train_4\n",
        "# !mkdir colab_indoor/test_4\n",
        "# !gcsfuse indoor-data/test_4 colab_indoor/test_4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021/05/10 06:46:35.385430 Using mount point: /content/colab_indoor\n",
            "2021/05/10 06:46:35.395573 Opening GCS connection...\n",
            "2021/05/10 06:46:35.655909 Mounting file system \"indoor-data\"...\n",
            "2021/05/10 06:46:35.683506 File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LLEkzgi_hq9",
        "outputId": "599abd0e-9c34-4f96-846b-24905f41f1c5"
      },
      "source": [
        "!ls -la -h ./colab_indoor/train_4_colcut"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.2G\n",
            "-rw-r--r-- 1 root root  43M Apr 14 21:07 5a0546857ecc773753327266_train.csv\n",
            "-rw-r--r-- 1 root root  46M Apr 14 21:07 5c3c44b80379370013e0fd2b_train.csv\n",
            "-rw-r--r-- 1 root root 113M Apr 14 21:08 5d27075f03f801723c2e360f_train.csv\n",
            "-rw-r--r-- 1 root root  43M Apr 14 21:08 5d27096c03f801723c31e5e0_train.csv\n",
            "-rw-r--r-- 1 root root  50M Apr 14 21:08 5d27097f03f801723c320d97_train.csv\n",
            "-rw-r--r-- 1 root root  12M Apr 14 21:08 5d27099f03f801723c32511d_train.csv\n",
            "-rw-r--r-- 1 root root  17M Apr 14 21:08 5d2709a003f801723c3251bf_train.csv\n",
            "-rw-r--r-- 1 root root  72M Apr 14 21:09 5d2709b303f801723c327472_train.csv\n",
            "-rw-r--r-- 1 root root  81M Apr 14 21:09 5d2709bb03f801723c32852c_train.csv\n",
            "-rw-r--r-- 1 root root  48M Apr 14 21:09 5d2709c303f801723c3299ee_train.csv\n",
            "-rw-r--r-- 1 root root  48M Apr 14 21:09 5d2709d403f801723c32bd39_train.csv\n",
            "-rw-r--r-- 1 root root  51M Apr 14 21:09 5d2709e003f801723c32d896_train.csv\n",
            "-rw-r--r-- 1 root root 3.9M Apr 14 21:10 5da138274db8ce0c98bbd3d2_train.csv\n",
            "-rw-r--r-- 1 root root  41M Apr 14 21:10 5da1382d4db8ce0c98bbe92e_train.csv\n",
            "-rw-r--r-- 1 root root  38M Apr 14 21:10 5da138314db8ce0c98bbf3a0_train.csv\n",
            "-rw-r--r-- 1 root root 6.7M Apr 14 21:10 5da138364db8ce0c98bc00f1_train.csv\n",
            "-rw-r--r-- 1 root root  63M Apr 14 21:10 5da1383b4db8ce0c98bc11ab_train.csv\n",
            "-rw-r--r-- 1 root root  35M Apr 14 21:10 5da138754db8ce0c98bca82f_train.csv\n",
            "-rw-r--r-- 1 root root  45M Apr 14 21:10 5da138764db8ce0c98bcaa46_train.csv\n",
            "-rw-r--r-- 1 root root  13M Apr 14 21:11 5da1389e4db8ce0c98bd0547_train.csv\n",
            "-rw-r--r-- 1 root root  82M Apr 14 21:11 5da138b74db8ce0c98bd4774_train.csv\n",
            "-rw-r--r-- 1 root root  71M Apr 14 21:11 5da958dd46f8266d0737457b_train.csv\n",
            "-rw-r--r-- 1 root root  76M Apr 14 21:11 5dbc1d84c1eb61796cf7c010_train.csv\n",
            "-rw-r--r-- 1 root root  75M Apr 14 21:12 5dc8cea7659e181adb076a3f_train.csv\n",
            "drwxr-xr-x 1 root root    0 May 10 06:47 .ipynb_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxo_gtclkLZo",
        "outputId": "64cadf09-086d-4c3b-ad50-d4f08a673788"
      },
      "source": [
        "import random\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import uniform_filter1d\n",
        "from scipy.interpolate import interp1d\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.stats as stats\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import multiprocessing\n",
        "import math\n",
        "\n",
        "EPOCH = 200 # default at 50\n",
        "BATCH_SIZE = 64 # 256 -> learning is too slow. But theoretically we should have big batch size if we are to \"pay attention\" to as many records as possible\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FOLDS = 5\n",
        "\n",
        "NUM_CORES = multiprocessing.cpu_count()\n",
        "print(NUM_CORES)\n",
        "\n",
        "OUTPUT_NAME = \"train_8_colcut_Conv1dTransformer\"\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImcyWYeuWJEP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c1d52699-b1cb-4b97-b121-f39f73cbd5ec"
      },
      "source": [
        "# train paths and test paths\n",
        "imu_train_files = sorted(glob.glob(\"./colab_indoor/train_final_preprocess/imu/train/*\"))\n",
        "imu_test_files = sorted(glob.glob(\"./colab_indoor/train_final_preprocess/imu/test/*\"))\n",
        "wifi_train_files = sorted(glob.glob(\"./colab_indoor/train_final_preprocess/wifi/train/*\"))\n",
        "wifi_test_files = sorted(glob.glob(\"./colab_indoor/train_final_preprocess/wifi/test/*\"))\n",
        "print(imu_train_files[:5])\n",
        "print(imu_test_files[:5])\n",
        "print(wifi_train_files[:5])\n",
        "print(wifi_test_files[:5])\n",
        "\n",
        "# load submission file\n",
        "sub_df = pd.read_csv(\"./colab_indoor/sample_submission.csv\", index_col=0)\n",
        "# sub_df[[\"site\", \"file\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
        "display(sub_df.head())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./colab_indoor/train_final_preprocess/imu/train/5a0546857ecc773753327266_train.csv', './colab_indoor/train_final_preprocess/imu/train/5c3c44b80379370013e0fd2b_train.csv', './colab_indoor/train_final_preprocess/imu/train/5d27075f03f801723c2e360f_train.csv', './colab_indoor/train_final_preprocess/imu/train/5d27096c03f801723c31e5e0_train.csv', './colab_indoor/train_final_preprocess/imu/train/5d27097f03f801723c320d97_train.csv']\n",
            "['./colab_indoor/train_final_preprocess/imu/test/5a0546857ecc773753327266_test.csv', './colab_indoor/train_final_preprocess/imu/test/5c3c44b80379370013e0fd2b_test.csv', './colab_indoor/train_final_preprocess/imu/test/5d27075f03f801723c2e360f_test.csv', './colab_indoor/train_final_preprocess/imu/test/5d27096c03f801723c31e5e0_test.csv', './colab_indoor/train_final_preprocess/imu/test/5d27097f03f801723c320d97_test.csv']\n",
            "['./colab_indoor/train_final_preprocess/wifi/train/5a0546857ecc773753327266_train.csv', './colab_indoor/train_final_preprocess/wifi/train/5c3c44b80379370013e0fd2b_train.csv', './colab_indoor/train_final_preprocess/wifi/train/5d27075f03f801723c2e360f_train.csv', './colab_indoor/train_final_preprocess/wifi/train/5d27096c03f801723c31e5e0_train.csv', './colab_indoor/train_final_preprocess/wifi/train/5d27097f03f801723c320d97_train.csv']\n",
            "['./colab_indoor/train_final_preprocess/wifi/test/5a0546857ecc773753327266_test.csv', './colab_indoor/train_final_preprocess/wifi/test/5c3c44b80379370013e0fd2b_test.csv', './colab_indoor/train_final_preprocess/wifi/test/5d27075f03f801723c2e360f_test.csv', './colab_indoor/train_final_preprocess/wifi/test/5d27096c03f801723c31e5e0_test.csv', './colab_indoor/train_final_preprocess/wifi/test/5d27097f03f801723c320d97_test.csv']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_path_timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000000009</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000009017</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000015326</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000018763</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000022328</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    floor     x     y\n",
              "site_path_timestamp                                                  \n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "B2jyhARLkpkk",
        "outputId": "a2b7dbc1-e36c-49d8-a057-bda3892fd8ae"
      },
      "source": [
        "# Load train csv and test csv\n",
        "train_df = pd.read_csv(wifi_train_files[5])\n",
        "test_df = pd.read_csv(wifi_test_files[5])\n",
        "display(train_df.head())\n",
        "display(test_df.head())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>0083f2d5119bb587ae0aab94f28d20f56c7ceb1e</th>\n",
              "      <th>010a084bf21b8b6e5d20b52ddce771a2096bce49</th>\n",
              "      <th>010e977b0665e2156af81baa15335b401391eb70</th>\n",
              "      <th>015032ad4378ce407ba62048ebfea026d147e551</th>\n",
              "      <th>01505ab29c36463d7de4270c6e3008a2aed95260</th>\n",
              "      <th>015e413ceb2d98c57c7ab01df69ab34dc06487d7</th>\n",
              "      <th>01d576f914f92d9a0b56cb9bf3c63192aabfee9e</th>\n",
              "      <th>01f93ddada4c312d0419f3944864f36e7d23f51d</th>\n",
              "      <th>02b2a90669c8c2c9e60eb767915673f3d7eec163</th>\n",
              "      <th>030d812ff40ec586f2dd16dacf55eb6250992e75</th>\n",
              "      <th>034ad5a7558e2e969020ecda0e93ef0c0d89720b</th>\n",
              "      <th>0392d4ae1c20d7288925446e25edc32be83562c2</th>\n",
              "      <th>03ca47f31c7f46b3410506082b7e7dd05d08983c</th>\n",
              "      <th>03cd7ccd8c4bac1127d298f4380d220f89802758</th>\n",
              "      <th>047c12a550068b3c3ac3a563d0caf712e21cfa9f</th>\n",
              "      <th>0482494bee9bfe532c0039092933752ef0f46981</th>\n",
              "      <th>0490d62107e6ef199f62bef26b3fa5e434b5793c</th>\n",
              "      <th>04f32dde2e2943991c0d01d9510df7bee63e127f</th>\n",
              "      <th>05234f464f8dc4def9958713a73cc12f5f8163ef</th>\n",
              "      <th>05b9b05aa3bf73e59166ef22fe91cd5a4a5de9ea</th>\n",
              "      <th>05cf0c6a59298a43c55b9b9a1921271136cd2844</th>\n",
              "      <th>060b0e7f5cf6517f3ac987269586da77a6958d6e</th>\n",
              "      <th>064419dd1c862bc6c960b365fed666a1a5ff36a9</th>\n",
              "      <th>0792087d811035b2dfc8c4dabe682c274054b888</th>\n",
              "      <th>0812c0cbfe0ea9c86b2cb4dc0c54623e23aca38f</th>\n",
              "      <th>08281cc3d39a230b48383a393679f353c1dba3cd</th>\n",
              "      <th>087d96e17e1e5d70d9bad45c6a7017adca00d70d</th>\n",
              "      <th>08848651828d5e6f2ac4555cb6e4457388de3d1f</th>\n",
              "      <th>09f6276ad1a36dbb50c4ec05986841aa0b689951</th>\n",
              "      <th>0aa6e658c4800f104504623fc99aef6ecf6b63ce</th>\n",
              "      <th>0af3b09601cf6d3508c652d3dcce4dbc1db4aa9b</th>\n",
              "      <th>0c2060ad17e94674bdc3c7224bafb1b57f390f57</th>\n",
              "      <th>0c49a973592b72bb5c9ba07deea6ef9e370eae08</th>\n",
              "      <th>0d0c05ba7421120ec6e2342a3a3ae453fa48d314</th>\n",
              "      <th>0d52129e85c1b5ee4d92756e8b8fb6cc4abd0751</th>\n",
              "      <th>0d567f4cc6b66e93275a8d95562a13439c60b18f</th>\n",
              "      <th>0f4d915c63741c7bca075e78ffa60b9d96b9fef3</th>\n",
              "      <th>...</th>\n",
              "      <th>ee9a507881249dc5c4c7b2f03410a180f2786142</th>\n",
              "      <th>eebb88bbb4d90cde77f694eae2d35d411eb887b3</th>\n",
              "      <th>efd22e067c1601903cd98229b4f6a90471027c38</th>\n",
              "      <th>f023f170364a68f7aec41872cab39cabe9191a2b</th>\n",
              "      <th>f0ad6926b94b799faa30f821f4d02a3962caa90a</th>\n",
              "      <th>f160c36e5fdb825fe6168922697dfecdc8975ab0</th>\n",
              "      <th>f32dfbc329b9c6f73bd2cf869247aba12482d8ca</th>\n",
              "      <th>f37ad0102448558dfdc54141569d3fd61b100b69</th>\n",
              "      <th>f3a064246d90d240aad853839c3cf262c12c2b33</th>\n",
              "      <th>f3ef7a4b7c2a6b041e8955b9277b682efb4ca990</th>\n",
              "      <th>f3f5eeec584dd5583439492e693fb8bedb4a66a6</th>\n",
              "      <th>f51f2b764c6da616a27103238a912571bde22273</th>\n",
              "      <th>f588fbd0488643855266c1ddb7e464384652c9a3</th>\n",
              "      <th>f6128e001d7765f61c5d009bb3b76cdccc020d0a</th>\n",
              "      <th>f6553c2da8dbd1e19b6d4caff460f1c32cf39e5b</th>\n",
              "      <th>f660e775f18b4137756bc5209460d00fa7a43463</th>\n",
              "      <th>f6bf9fc0636694f483774450513af968f56ad62b</th>\n",
              "      <th>f6e4623286e35d481bd1edc74aa1a8f7ed35d36a</th>\n",
              "      <th>f6f885e26b1de2ba6cf5bbd2e2308627b704204a</th>\n",
              "      <th>f8016dcbc2a5f2d81deb0ce87ca3078753e3da99</th>\n",
              "      <th>f81b6d1ed1fab9e8c3967ab49f0330c3d8846866</th>\n",
              "      <th>f8b5dcce370ef547dcea706c3a90f69d390c3881</th>\n",
              "      <th>f8e1979b7be281fdcdb3a606faf6b0fd8c89aa7a</th>\n",
              "      <th>f94dbe44f7ce073b8eb83975a35ba541883eb16c</th>\n",
              "      <th>f973807d4860b99da095906c8d6158c8c12e2a4e</th>\n",
              "      <th>f9788387633ec80ee380841eaedd8095858e7b20</th>\n",
              "      <th>f99614b7d00fa3df33c67d5aea7410887ba75267</th>\n",
              "      <th>faaf4ca464b8daa3822a7a1f48f75ed216e06995</th>\n",
              "      <th>fab79668e5e617d6cc55faa2864e234986d55099</th>\n",
              "      <th>fac5bb53eba73a84b256f49577d8c502d6d99a50</th>\n",
              "      <th>fb61c3e97673a6dec50aca7967fce0dbbc478f6c</th>\n",
              "      <th>fc0f706650d66adeea72f405586bdc5ac8629963</th>\n",
              "      <th>fc27c0656fc13157bb2f58543d51e8ee972fdf66</th>\n",
              "      <th>fc331fa3524a23b59452b78dd7071f82d4663764</th>\n",
              "      <th>fd411d584b11c7df8957e258f1552395fbdfa187</th>\n",
              "      <th>fd7e3b93a87b43aad4ac635ea1c480eeb2907ace</th>\n",
              "      <th>ff20069c9561845ba74de8d4c7253828855a3607</th>\n",
              "      <th>ff81c427a297c4802d6d2bc1585932444fa05d1b</th>\n",
              "      <th>ffdd3cc74cbe835b5057a466510234c99a6ef64f</th>\n",
              "      <th>ffe816ea4cf58e5fd30b65652933d9a7bc42da63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21.195808</td>\n",
              "      <td>46.759228</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>-0.057267</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>-0.896224</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>-0.55696</td>\n",
              "      <td>-0.60274</td>\n",
              "      <td>1.067862</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>-0.549602</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>2.239357</td>\n",
              "      <td>-0.847481</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>1.451962</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>-0.364131</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>0.547372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>2.366177</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>1.471541</td>\n",
              "      <td>-0.264631</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>-0.720085</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>-0.316272</td>\n",
              "      <td>0.491635</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>-0.550536</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>1.464025</td>\n",
              "      <td>0.111239</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>-0.229093</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>-0.557522</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>-0.419357</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>1.425912</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>-0.541633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21.195808</td>\n",
              "      <td>46.759228</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>-0.057267</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>-0.896224</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>-0.55696</td>\n",
              "      <td>-0.60274</td>\n",
              "      <td>1.067862</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>-0.549602</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>2.056292</td>\n",
              "      <td>-0.847481</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>1.451962</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>-0.364131</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>0.547372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>2.366177</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>1.702739</td>\n",
              "      <td>-0.264631</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>-0.720085</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>-0.316272</td>\n",
              "      <td>0.491635</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>-0.550536</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>1.464025</td>\n",
              "      <td>0.111239</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>-0.229093</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>-0.557522</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>-0.419357</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>1.875636</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>-0.541633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.195808</td>\n",
              "      <td>46.759228</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>-0.057267</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>-0.896224</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>-0.55696</td>\n",
              "      <td>-0.60274</td>\n",
              "      <td>1.067862</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>-0.549602</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>2.300379</td>\n",
              "      <td>-0.847481</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>1.451962</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>-0.364131</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>0.547372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>2.366177</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>2.743129</td>\n",
              "      <td>-0.264631</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>-0.720085</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>-0.316272</td>\n",
              "      <td>0.491635</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>-0.550536</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>1.554646</td>\n",
              "      <td>0.111239</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>-0.229093</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>-0.557522</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>-0.419357</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>1.515857</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>-0.541633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21.195808</td>\n",
              "      <td>46.759228</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>-0.057267</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>-0.896224</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>-0.55696</td>\n",
              "      <td>-0.60274</td>\n",
              "      <td>1.067862</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>-0.549602</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.896880</td>\n",
              "      <td>-0.847481</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>1.451962</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>-0.364131</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>-0.007065</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>1.787796</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>2.743129</td>\n",
              "      <td>-0.264631</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>-0.720085</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>-0.316272</td>\n",
              "      <td>0.491635</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>-0.550536</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>1.554646</td>\n",
              "      <td>0.111239</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>-0.229093</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>-0.557522</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>-0.419357</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>1.515857</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>-0.541633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.195808</td>\n",
              "      <td>46.759228</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>-0.057267</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>-0.896224</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>-0.55696</td>\n",
              "      <td>-0.60274</td>\n",
              "      <td>0.948358</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>-0.549602</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.957901</td>\n",
              "      <td>-0.847481</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>1.451962</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>-0.364131</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>-0.007065</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>1.402209</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>2.743129</td>\n",
              "      <td>-0.264631</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>-0.720085</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>-0.316272</td>\n",
              "      <td>0.491635</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>-0.550536</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>1.554646</td>\n",
              "      <td>0.111239</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>-0.229093</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>-0.557522</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>-0.419357</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>0.346576</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>-0.541633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 555 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           x  ...  ffe816ea4cf58e5fd30b65652933d9a7bc42da63\n",
              "0  21.195808  ...                                 -0.541633\n",
              "1  21.195808  ...                                 -0.541633\n",
              "2  21.195808  ...                                 -0.541633\n",
              "3  21.195808  ...                                 -0.541633\n",
              "4  21.195808  ...                                 -0.541633\n",
              "\n",
              "[5 rows x 555 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>0083f2d5119bb587ae0aab94f28d20f56c7ceb1e</th>\n",
              "      <th>010a084bf21b8b6e5d20b52ddce771a2096bce49</th>\n",
              "      <th>010e977b0665e2156af81baa15335b401391eb70</th>\n",
              "      <th>015032ad4378ce407ba62048ebfea026d147e551</th>\n",
              "      <th>01505ab29c36463d7de4270c6e3008a2aed95260</th>\n",
              "      <th>015e413ceb2d98c57c7ab01df69ab34dc06487d7</th>\n",
              "      <th>01d576f914f92d9a0b56cb9bf3c63192aabfee9e</th>\n",
              "      <th>01f93ddada4c312d0419f3944864f36e7d23f51d</th>\n",
              "      <th>02b2a90669c8c2c9e60eb767915673f3d7eec163</th>\n",
              "      <th>030d812ff40ec586f2dd16dacf55eb6250992e75</th>\n",
              "      <th>034ad5a7558e2e969020ecda0e93ef0c0d89720b</th>\n",
              "      <th>0392d4ae1c20d7288925446e25edc32be83562c2</th>\n",
              "      <th>03ca47f31c7f46b3410506082b7e7dd05d08983c</th>\n",
              "      <th>03cd7ccd8c4bac1127d298f4380d220f89802758</th>\n",
              "      <th>047c12a550068b3c3ac3a563d0caf712e21cfa9f</th>\n",
              "      <th>0482494bee9bfe532c0039092933752ef0f46981</th>\n",
              "      <th>0490d62107e6ef199f62bef26b3fa5e434b5793c</th>\n",
              "      <th>04f32dde2e2943991c0d01d9510df7bee63e127f</th>\n",
              "      <th>05234f464f8dc4def9958713a73cc12f5f8163ef</th>\n",
              "      <th>05b9b05aa3bf73e59166ef22fe91cd5a4a5de9ea</th>\n",
              "      <th>05cf0c6a59298a43c55b9b9a1921271136cd2844</th>\n",
              "      <th>060b0e7f5cf6517f3ac987269586da77a6958d6e</th>\n",
              "      <th>064419dd1c862bc6c960b365fed666a1a5ff36a9</th>\n",
              "      <th>0792087d811035b2dfc8c4dabe682c274054b888</th>\n",
              "      <th>0812c0cbfe0ea9c86b2cb4dc0c54623e23aca38f</th>\n",
              "      <th>08281cc3d39a230b48383a393679f353c1dba3cd</th>\n",
              "      <th>087d96e17e1e5d70d9bad45c6a7017adca00d70d</th>\n",
              "      <th>08848651828d5e6f2ac4555cb6e4457388de3d1f</th>\n",
              "      <th>09f6276ad1a36dbb50c4ec05986841aa0b689951</th>\n",
              "      <th>0aa6e658c4800f104504623fc99aef6ecf6b63ce</th>\n",
              "      <th>0af3b09601cf6d3508c652d3dcce4dbc1db4aa9b</th>\n",
              "      <th>0c2060ad17e94674bdc3c7224bafb1b57f390f57</th>\n",
              "      <th>0c49a973592b72bb5c9ba07deea6ef9e370eae08</th>\n",
              "      <th>0d0c05ba7421120ec6e2342a3a3ae453fa48d314</th>\n",
              "      <th>0d52129e85c1b5ee4d92756e8b8fb6cc4abd0751</th>\n",
              "      <th>0d567f4cc6b66e93275a8d95562a13439c60b18f</th>\n",
              "      <th>0f4d915c63741c7bca075e78ffa60b9d96b9fef3</th>\n",
              "      <th>...</th>\n",
              "      <th>ee9a507881249dc5c4c7b2f03410a180f2786142</th>\n",
              "      <th>eebb88bbb4d90cde77f694eae2d35d411eb887b3</th>\n",
              "      <th>efd22e067c1601903cd98229b4f6a90471027c38</th>\n",
              "      <th>f023f170364a68f7aec41872cab39cabe9191a2b</th>\n",
              "      <th>f0ad6926b94b799faa30f821f4d02a3962caa90a</th>\n",
              "      <th>f160c36e5fdb825fe6168922697dfecdc8975ab0</th>\n",
              "      <th>f32dfbc329b9c6f73bd2cf869247aba12482d8ca</th>\n",
              "      <th>f37ad0102448558dfdc54141569d3fd61b100b69</th>\n",
              "      <th>f3a064246d90d240aad853839c3cf262c12c2b33</th>\n",
              "      <th>f3ef7a4b7c2a6b041e8955b9277b682efb4ca990</th>\n",
              "      <th>f3f5eeec584dd5583439492e693fb8bedb4a66a6</th>\n",
              "      <th>f51f2b764c6da616a27103238a912571bde22273</th>\n",
              "      <th>f588fbd0488643855266c1ddb7e464384652c9a3</th>\n",
              "      <th>f6128e001d7765f61c5d009bb3b76cdccc020d0a</th>\n",
              "      <th>f6553c2da8dbd1e19b6d4caff460f1c32cf39e5b</th>\n",
              "      <th>f660e775f18b4137756bc5209460d00fa7a43463</th>\n",
              "      <th>f6bf9fc0636694f483774450513af968f56ad62b</th>\n",
              "      <th>f6e4623286e35d481bd1edc74aa1a8f7ed35d36a</th>\n",
              "      <th>f6f885e26b1de2ba6cf5bbd2e2308627b704204a</th>\n",
              "      <th>f8016dcbc2a5f2d81deb0ce87ca3078753e3da99</th>\n",
              "      <th>f81b6d1ed1fab9e8c3967ab49f0330c3d8846866</th>\n",
              "      <th>f8b5dcce370ef547dcea706c3a90f69d390c3881</th>\n",
              "      <th>f8e1979b7be281fdcdb3a606faf6b0fd8c89aa7a</th>\n",
              "      <th>f94dbe44f7ce073b8eb83975a35ba541883eb16c</th>\n",
              "      <th>f973807d4860b99da095906c8d6158c8c12e2a4e</th>\n",
              "      <th>f9788387633ec80ee380841eaedd8095858e7b20</th>\n",
              "      <th>f99614b7d00fa3df33c67d5aea7410887ba75267</th>\n",
              "      <th>faaf4ca464b8daa3822a7a1f48f75ed216e06995</th>\n",
              "      <th>fab79668e5e617d6cc55faa2864e234986d55099</th>\n",
              "      <th>fac5bb53eba73a84b256f49577d8c502d6d99a50</th>\n",
              "      <th>fb61c3e97673a6dec50aca7967fce0dbbc478f6c</th>\n",
              "      <th>fc0f706650d66adeea72f405586bdc5ac8629963</th>\n",
              "      <th>fc27c0656fc13157bb2f58543d51e8ee972fdf66</th>\n",
              "      <th>fc331fa3524a23b59452b78dd7071f82d4663764</th>\n",
              "      <th>fd411d584b11c7df8957e258f1552395fbdfa187</th>\n",
              "      <th>fd7e3b93a87b43aad4ac635ea1c480eeb2907ace</th>\n",
              "      <th>ff20069c9561845ba74de8d4c7253828855a3607</th>\n",
              "      <th>ff81c427a297c4802d6d2bc1585932444fa05d1b</th>\n",
              "      <th>ffdd3cc74cbe835b5057a466510234c99a6ef64f</th>\n",
              "      <th>ffe816ea4cf58e5fd30b65652933d9a7bc42da63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>1.167021</td>\n",
              "      <td>2.987183</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>0.228199</td>\n",
              "      <td>12.910597</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>0.849424</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>2.110693</td>\n",
              "      <td>-0.602740</td>\n",
              "      <td>-0.605195</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>2.536193</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.042576</td>\n",
              "      <td>0.474395</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>-0.08431</td>\n",
              "      <td>4.145215</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>3.969670</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>2.072074</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>-0.42933</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>0.777947</td>\n",
              "      <td>2.984568</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>0.787919</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>3.896173</td>\n",
              "      <td>0.308202</td>\n",
              "      <td>3.364843</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>2.634701</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>0.104712</td>\n",
              "      <td>0.568215</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>4.982971</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>2.455267</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>1.148922</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>-1.002595</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>0.837033</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>2.490476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>2.987183</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>12.910597</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>0.962046</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>1.920147</td>\n",
              "      <td>-0.602740</td>\n",
              "      <td>-0.605195</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>2.124754</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.042576</td>\n",
              "      <td>0.421520</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>-0.08431</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>3.404392</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>1.887262</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>-0.42933</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>0.777947</td>\n",
              "      <td>3.887124</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>0.787919</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>2.960074</td>\n",
              "      <td>-0.058663</td>\n",
              "      <td>3.364843</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>2.223703</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>0.195333</td>\n",
              "      <td>0.568215</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>3.679955</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>2.143599</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>1.148922</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>0.166686</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>0.970350</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>2.086195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>2.987183</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>12.910597</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>0.962046</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>1.920147</td>\n",
              "      <td>-0.602740</td>\n",
              "      <td>-0.605195</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>2.433333</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.042576</td>\n",
              "      <td>0.474395</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>-0.08431</td>\n",
              "      <td>-0.261908</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>3.404392</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>2.118277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>-0.42933</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>0.777947</td>\n",
              "      <td>3.887124</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>0.787919</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>2.726049</td>\n",
              "      <td>-0.058663</td>\n",
              "      <td>3.364843</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>2.223703</td>\n",
              "      <td>-0.468033</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>0.104712</td>\n",
              "      <td>0.568215</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>4.201161</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>2.351378</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>1.260942</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>-0.103148</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>0.970350</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>2.389406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>-0.608941</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>12.910597</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>1.187291</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>1.443780</td>\n",
              "      <td>1.063073</td>\n",
              "      <td>-0.605195</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>2.433333</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.042576</td>\n",
              "      <td>0.685895</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>-0.08431</td>\n",
              "      <td>3.492308</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>3.404392</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>2.025871</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>-0.42933</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>-0.493641</td>\n",
              "      <td>4.248146</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>0.843771</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>2.726049</td>\n",
              "      <td>-0.058663</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>2.223703</td>\n",
              "      <td>1.547705</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>0.104712</td>\n",
              "      <td>0.476820</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>4.201161</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>2.351378</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>1.260942</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>-0.103148</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>0.970350</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>2.288336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-0.109605</td>\n",
              "      <td>1.224310</td>\n",
              "      <td>-0.281231</td>\n",
              "      <td>-0.522217</td>\n",
              "      <td>-0.652685</td>\n",
              "      <td>12.910597</td>\n",
              "      <td>-0.222054</td>\n",
              "      <td>-0.499727</td>\n",
              "      <td>0.736801</td>\n",
              "      <td>-0.291654</td>\n",
              "      <td>1.539053</td>\n",
              "      <td>1.063073</td>\n",
              "      <td>-0.605195</td>\n",
              "      <td>-0.465369</td>\n",
              "      <td>-0.670374</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>2.433333</td>\n",
              "      <td>-0.139538</td>\n",
              "      <td>-0.300292</td>\n",
              "      <td>-0.300172</td>\n",
              "      <td>0.347684</td>\n",
              "      <td>0.844520</td>\n",
              "      <td>-0.381229</td>\n",
              "      <td>-0.180553</td>\n",
              "      <td>-0.449952</td>\n",
              "      <td>-0.130843</td>\n",
              "      <td>-0.193907</td>\n",
              "      <td>-0.353796</td>\n",
              "      <td>-0.08431</td>\n",
              "      <td>3.492308</td>\n",
              "      <td>-0.459956</td>\n",
              "      <td>-0.640692</td>\n",
              "      <td>-0.197824</td>\n",
              "      <td>-0.098715</td>\n",
              "      <td>3.969670</td>\n",
              "      <td>-0.748022</td>\n",
              "      <td>1.933465</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224093</td>\n",
              "      <td>-0.71575</td>\n",
              "      <td>-0.255141</td>\n",
              "      <td>-0.42933</td>\n",
              "      <td>-0.560265</td>\n",
              "      <td>-0.469429</td>\n",
              "      <td>-0.559031</td>\n",
              "      <td>-0.493641</td>\n",
              "      <td>2.804057</td>\n",
              "      <td>-0.476952</td>\n",
              "      <td>-0.211848</td>\n",
              "      <td>0.843771</td>\n",
              "      <td>-0.269384</td>\n",
              "      <td>2.726049</td>\n",
              "      <td>-0.058663</td>\n",
              "      <td>-0.278272</td>\n",
              "      <td>-0.443698</td>\n",
              "      <td>2.223703</td>\n",
              "      <td>1.547705</td>\n",
              "      <td>-0.491514</td>\n",
              "      <td>0.104712</td>\n",
              "      <td>0.476820</td>\n",
              "      <td>-0.680879</td>\n",
              "      <td>4.201161</td>\n",
              "      <td>-0.634177</td>\n",
              "      <td>2.351378</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.264117</td>\n",
              "      <td>-0.516837</td>\n",
              "      <td>-0.528618</td>\n",
              "      <td>-0.165307</td>\n",
              "      <td>1.260942</td>\n",
              "      <td>-0.741575</td>\n",
              "      <td>-0.271246</td>\n",
              "      <td>-0.103148</td>\n",
              "      <td>-0.293338</td>\n",
              "      <td>-0.603184</td>\n",
              "      <td>0.970350</td>\n",
              "      <td>-0.286707</td>\n",
              "      <td>2.086195</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 555 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       x  ...  ffe816ea4cf58e5fd30b65652933d9a7bc42da63\n",
              "0 -999.0  ...                                  2.490476\n",
              "1 -999.0  ...                                  2.086195\n",
              "2 -999.0  ...                                  2.389406\n",
              "3 -999.0  ...                                  2.288336\n",
              "4 -999.0  ...                                  2.086195\n",
              "\n",
              "[5 rows x 555 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ePebfXPvPui"
      },
      "source": [
        "---\n",
        "## 1Conv + Transformer Implementation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNgXfNr2rnyk"
      },
      "source": [
        "class IndoorDataset(Dataset):\n",
        "    def __init__(self, data, flag=\"train\"):\n",
        "        self.flag = flag\n",
        "        self.data = data\n",
        "\n",
        "        all_cols = list(data.columns)\n",
        "        target_cols = [\"x\", \"y\", \"floor_int\"]\n",
        "        non_target_cols = [col for col in all_cols if col not in target_cols]\n",
        "        self.features = data[non_target_cols]\n",
        "\n",
        "        if self.flag == \"train\":\n",
        "            self.x = data.loc[:, \"x\"]\n",
        "            self.y = data.loc[:, \"y\"]\n",
        "            # self.f = data.loc[:, \"floor_int\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def feat_width(self):\n",
        "        return self.features.shape[1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        features = self.features.iloc[index, :]\n",
        "        features_out = torch.tensor(features.to_numpy())\n",
        "        if self.flag == \"train\":\n",
        "            x = self.x[index]\n",
        "            y = self.y[index]\n",
        "            # f = self.f[index]\n",
        "            x_out = torch.tensor(x)\n",
        "            y_out = torch.tensor(y)\n",
        "            # f_out = torch.tensor(f)\n",
        "            return features_out, x_out, y_out\n",
        "        else:\n",
        "            return features_out"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZNpNu_psGuE",
        "outputId": "6d15db18-1a1f-4729-dee1-8b818e567ad8"
      },
      "source": [
        "# Create train and test Dataset\n",
        "train_ds = IndoorDataset(train_df)\n",
        "test_ds = IndoorDataset(test_df, flag=\"test\")\n",
        "\n",
        "one_train_ds = train_ds.__getitem__(1000)\n",
        "print(\"train ds len: \", train_ds.__len__())\n",
        "# print(\"train ds features: \", one_train_ds[0])\n",
        "print(\"train ds x: \", one_train_ds[1])\n",
        "print(\"train ds y: \", one_train_ds[2])\n",
        "# print(\"train ds f: \", one_train_ds[3])\n",
        "\n",
        "one_test_ds = test_ds.__getitem__(0)\n",
        "print(\"test ds len: \", test_ds.__len__())\n",
        "# print(\"test ds features: \", one_test_ds[0])\n",
        "# print(\"test ds: \", one_test_ds)\n",
        "\n",
        "print(train_ds.feat_width())\n",
        "print(test_ds.feat_width())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train ds len:  4251\n",
            "train ds x:  tensor(9.3723, dtype=torch.float64)\n",
            "train ds y:  tensor(67.5588, dtype=torch.float64)\n",
            "test ds len:  49\n",
            "552\n",
            "552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDBm9wbv8JS"
      },
      "source": [
        "# Create Dataloader\n",
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # it should be shuffled if we are not masking and sequentially processing the data\n",
        "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51XdnWYay8SP"
      },
      "source": [
        "class Conv1dMaxPool(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, stride, padding):\n",
        "        super(Conv1dMaxPool, self).__init__()\n",
        "        self.conv_11 = nn.Conv1d(in_channels, 16, kernel_size, stride, padding)\n",
        "        self.conv_12 = nn.Conv1d(16, 16, kernel_size, stride, padding)\n",
        "        self.max_pool_1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_21 = nn.Conv1d(16, 32, kernel_size, stride, padding)\n",
        "        self.conv_22 = nn.Conv1d(32, 32, kernel_size, stride, padding)\n",
        "        self.max_pool_2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_31 = nn.Conv1d(32, 64, kernel_size, stride, padding)\n",
        "        self.conv_32 = nn.Conv1d(64, 64, kernel_size, stride, padding)\n",
        "        self.max_pool_3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        # self.fc_encoder = nn.Linear(256, 512) # fix the input dim accordingly\n",
        "        self.batch_norm = nn.BatchNorm1d(512)\n",
        "        self.out_encoder = nn.Linear(512, 128)\n",
        "\n",
        "    def forward(self, x, prints=False):\n",
        "        x = x.unsqueeze(0)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        if prints: print(\"before conv 1: \", x.shape)\n",
        "        x = F.relu(self.conv_11(x))\n",
        "        x = F.relu(self.conv_12(x))\n",
        "        x = self.max_pool_1(x)\n",
        "        if prints: print(\"after conv & max_pool 1: \", x.shape)\n",
        "\n",
        "        x = F.relu(self.conv_21(x))\n",
        "        x = F.relu(self.conv_22(x))\n",
        "        x = self.max_pool_2(x)\n",
        "        if prints: print(\"after conv & max_pool 2: \", x.shape)\n",
        "\n",
        "        x = F.relu(self.conv_31(x))\n",
        "        x = F.relu(self.conv_32(x))\n",
        "        x = self.max_pool_3(x)\n",
        "        if prints: print(\"after conv & max_pool 3: \", x.shape)\n",
        "\n",
        "        # if prints: print(\"checking reshaping: \", x[0])\n",
        "        x = x.view(x.size(0), -1) # flatten last two dimensions\n",
        "        # if prints: print(\"checking reshaping: \", x[0])\n",
        "        if prints: print(\"flatten last two dims: \", x.shape)\n",
        "        self.fc_encoder = nn.Linear(x.shape[1], 512) # not elegant but needs to be there in order to adapt to varying input size\n",
        "        x = self.batch_norm(self.fc_encoder(x))\n",
        "        x = self.out_encoder(x)\n",
        "        if prints: print(\"final output: \", x.shape)\n",
        "        return x"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJPs3O4323xO",
        "outputId": "f6501821-6410-4e2f-f8c3-7069f1807286"
      },
      "source": [
        "train_length = train_ds.__len__()\n",
        "train_width = train_ds.feat_width()\n",
        "\n",
        "conv_1d_maxpool = Conv1dMaxPool(in_channels=1, kernel_size=3, stride=2, padding=1).to(DEVICE)\n",
        "print(conv_1d_maxpool)\n",
        "\n",
        "# input_size = num of features, so length of columns\n",
        "# sequence_length = 5\n",
        "\n",
        "# Check if it works\n",
        "train_batch_sample = next(iter(train_dataloader))\n",
        "print(\"feature shape: \", train_batch_sample[0].shape)\n",
        "print(\"x shape: \", train_batch_sample[1].shape)\n",
        "print(\"y shape: \", train_batch_sample[2].shape)\n",
        "train_batch_sample = train_batch_sample[0]\n",
        "print(\"input shape: \", train_batch_sample.shape)\n",
        "outputs = conv_1d_maxpool(train_batch_sample.float(), prints=True)\n",
        "print(\"final output shape: \", outputs.shape)\n",
        "print(\"final output: \", outputs)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv1dMaxPool(\n",
            "  (conv_11): Conv1d(1, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_12): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_1): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv_21): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_22): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_2): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv_31): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_32): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_3): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (out_encoder): Linear(in_features=512, out_features=128, bias=True)\n",
            ")\n",
            "feature shape:  torch.Size([64, 552])\n",
            "x shape:  torch.Size([64])\n",
            "y shape:  torch.Size([64])\n",
            "input shape:  torch.Size([64, 552])\n",
            "before conv 1:  torch.Size([64, 1, 552])\n",
            "after conv & max_pool 1:  torch.Size([64, 16, 69])\n",
            "after conv & max_pool 2:  torch.Size([64, 32, 9])\n",
            "after conv & max_pool 3:  torch.Size([64, 64, 2])\n",
            "flatten last two dims:  torch.Size([64, 128])\n",
            "final output:  torch.Size([64, 128])\n",
            "final output shape:  torch.Size([64, 128])\n",
            "final output:  tensor([[ 0.0189,  0.0925,  0.0665,  ..., -0.1077,  0.2080,  0.1239],\n",
            "        [ 0.0136, -0.0957,  0.2553,  ..., -0.1071, -0.0916, -0.0412],\n",
            "        [ 0.3054,  0.0489, -0.0594,  ...,  0.0296, -0.0307, -0.0059],\n",
            "        ...,\n",
            "        [ 0.0083, -0.1855, -0.0713,  ..., -0.0032,  0.0507, -0.0392],\n",
            "        [-0.0640,  0.1263,  0.1886,  ...,  0.0336, -0.1593,  0.0381],\n",
            "        [-0.2893, -0.0604,  0.0428,  ..., -0.0242, -0.0153, -0.0163]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IREBqLnPERaz"
      },
      "source": [
        "# Transformer: -> conv1d output -> pe -> (embed thru TransformerEncoderLayer)\n",
        "# -> encoder -> decoder\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, feature_dim, batch_len):\n",
        "        super(PositionalEncoding, self).__init__()       \n",
        "        pe = torch.zeros(batch_len, feature_dim)\n",
        "        # print(\"pe shape\", pe.shape)\n",
        "        position = torch.arange(0, batch_len, dtype=torch.float).unsqueeze(1)\n",
        "        # print(\"position shape\", position.shape)\n",
        "        div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * (-math.log(10000.0) / feature_dim))\n",
        "        # print(\"div_term shape\", div_term.shape)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, feature_dim, batch_len, num_layers=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        # self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_dim, batch_len)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=2, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=feature_dim, nhead=2)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)        \n",
        "        self.decoder = nn.Linear(feature_dim, 2)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, prints=False):\n",
        "        if prints: print(\"x dims: \", x.shape)\n",
        "        \n",
        "        # # Take off the mask entirely -> Not to consider any order/sequence of the input\n",
        "        # if self.src_mask is None or self.src_mask.size(0) != len(x):\n",
        "        #     mask = self._generate_square_subsequent_mask(len(x)).to(DEVICE)\n",
        "        #     if prints: print(\"mask shape\", mask.shape)\n",
        "        #     # print(\"mask 0\", mask[0])\n",
        "        #     # print(\"mask 1\", mask[1])\n",
        "        #     # print(\"mask -2\", mask[-2])\n",
        "        #     # print(\"mask -1\", mask[-1])\n",
        "        #     self.src_mask = mask\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "        if prints: print(\"x after pos_encoder: \", x.shape)\n",
        "        x = x.unsqueeze(1)\n",
        "        if prints: print(\"x after unsqueeze: \", x.shape)\n",
        "        # output = self.transformer_encoder(x, self.src_mask)\n",
        "        enc_out = self.transformer_encoder(x) # Having mask has no effect to the result (~1200 loss, pretty much same as with-mask result)\n",
        "        if prints: print(\"enc_out after transformer_encoder: \", enc_out.shape)\n",
        "        dec_out = self.transformer_decoder(tgt=x, memory=enc_out)\n",
        "        if prints: print(\"dec_out after transformer_decoder: \", dec_out.shape)\n",
        "        output = self.decoder(dec_out)\n",
        "        output = output.squeeze(1)\n",
        "        return output\n",
        "\n",
        "    # def _generate_square_subsequent_mask(self, sz):\n",
        "    #     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    #     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    #     return mask"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrgkU-Zlvw0D",
        "outputId": "5f3b27a3-359a-49d6-b814-b4166d1cc430"
      },
      "source": [
        "# testing transformer block\n",
        "Conv = Conv1dMaxPool(in_channels=1, kernel_size=3, stride=2, padding=1).to(DEVICE)\n",
        "\n",
        "# Check if it works\n",
        "train_batch_sample = next(iter(train_dataloader))\n",
        "train_batch_sample = train_batch_sample[0]\n",
        "print(train_batch_sample)\n",
        "conv_outputs = Conv(train_batch_sample.float(), prints=True)\n",
        "print(\"final output shape: \", conv_outputs.shape)\n",
        "print(\"final output: \", conv_outputs[0])\n",
        "print(conv_outputs.shape[1])\n",
        "print(conv_outputs.requires_grad)\n",
        "\n",
        "# check pe\n",
        "batch_len = conv_outputs.shape[0]\n",
        "feature_dim = conv_outputs.shape[1]\n",
        "pe = PositionalEncoding(feature_dim, batch_len)\n",
        "pe_out = pe(conv_outputs)\n",
        "print(pe_out[0])\n",
        "print(pe_out.shape)\n",
        "print(pe_out.requires_grad)\n",
        "\n",
        "# check transformer\n",
        "ts = Transformer(feature_dim, batch_len, num_layers=1, dropout=0.1)\n",
        "ts_out = ts(conv_outputs, prints=True)\n",
        "print(ts_out[:3])\n",
        "print(ts_out.shape)\n",
        "print(ts_out.requires_grad)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0545, -0.1667, -0.1140,  ..., -0.3586, -0.2395, -0.2236],\n",
            "        [-0.0545, -0.1667, -0.1140,  ..., -0.3586, -0.2395, -0.2236],\n",
            "        [-0.0545, -0.1667, -0.1140,  ..., -0.3586, -0.2395, -0.2236],\n",
            "        ...,\n",
            "        [-0.0545, -0.1667, -0.1140,  ..., -0.3586, -0.2395, -0.2236],\n",
            "        [-0.0545, -0.1667, -0.1140,  ...,  1.1936, -0.2395, -0.2236],\n",
            "        [-0.0545, -0.1667, -0.1140,  ..., -0.3586, -0.2395, -0.2236]],\n",
            "       dtype=torch.float64)\n",
            "before conv 1:  torch.Size([64, 1, 4321])\n",
            "after conv & max_pool 1:  torch.Size([64, 16, 541])\n",
            "after conv & max_pool 2:  torch.Size([64, 32, 68])\n",
            "after conv & max_pool 3:  torch.Size([64, 64, 9])\n",
            "flatten last two dims:  torch.Size([64, 576])\n",
            "final output:  torch.Size([64, 128])\n",
            "final output shape:  torch.Size([64, 128])\n",
            "final output:  tensor([ 0.5043,  0.2690,  0.6949,  0.5531, -0.4167,  0.2413, -0.2541, -0.1393,\n",
            "        -0.2746,  0.2558,  0.2427,  0.9166,  0.5431,  0.5518, -0.0765,  0.3249,\n",
            "         0.5203,  0.0049,  0.1772, -0.0731,  0.1895,  0.2881,  0.2025,  0.0752,\n",
            "         0.0018,  0.3570,  0.1975,  0.4363, -0.4243,  0.1633, -0.1450,  0.4268,\n",
            "         0.5941,  0.9672, -0.2221, -0.6883, -0.3442,  0.5622,  0.0456, -0.7704,\n",
            "         0.6276, -0.9797, -0.2425,  0.8642,  0.0169,  0.1319, -0.2500, -0.1982,\n",
            "         0.7205,  0.5414,  0.1555, -0.0059, -0.1728,  0.1871,  0.2143,  0.4338,\n",
            "         0.0027,  0.0376,  0.1612,  0.3290,  0.2179, -0.3450, -0.8999,  0.3237,\n",
            "        -0.6634, -0.5671, -0.1145, -0.3047, -0.6966, -0.1290, -0.1443,  0.1220,\n",
            "        -0.1131,  0.6146, -0.0825, -0.3318,  0.5388,  0.0883,  0.0310, -0.1724,\n",
            "        -0.0847,  0.1959,  0.3911,  0.0548,  0.6513,  0.3449,  0.4976, -0.4147,\n",
            "         0.6159, -0.2257, -0.4556,  0.0655,  0.0505,  0.4172,  0.1479,  0.2834,\n",
            "         0.6286, -0.1906,  0.2369,  0.1593, -0.5142,  0.0166, -0.1982, -0.2492,\n",
            "        -0.0631,  0.2595, -0.6325, -0.8415,  0.0284, -0.4177,  0.1788,  0.4087,\n",
            "         0.3241,  0.3299,  0.2217,  0.0275,  0.0520,  0.1813,  0.5357,  0.4081,\n",
            "        -0.2936, -0.5850,  0.0828, -0.0792,  0.6047,  0.1966,  0.0223, -0.2712],\n",
            "       grad_fn=<SelectBackward>)\n",
            "128\n",
            "True\n",
            "tensor([ 5.0434e-01,  1.2690e+00,  6.9485e-01,  1.5531e+00, -4.1674e-01,\n",
            "         1.2413e+00, -2.5406e-01,  8.6074e-01, -2.7456e-01,  1.2558e+00,\n",
            "         2.4273e-01,  1.9166e+00,  5.4308e-01,  1.5518e+00, -7.6467e-02,\n",
            "         1.3249e+00,  5.2027e-01,  1.0049e+00,  1.7718e-01,  9.2691e-01,\n",
            "         1.8949e-01,  1.2881e+00,  2.0249e-01,  1.0752e+00,  1.7726e-03,\n",
            "         1.3570e+00,  1.9751e-01,  1.4363e+00, -4.2432e-01,  1.1633e+00,\n",
            "        -1.4497e-01,  1.4268e+00,  5.9410e-01,  1.9672e+00, -2.2215e-01,\n",
            "         3.1166e-01, -3.4422e-01,  1.5622e+00,  4.5554e-02,  2.2961e-01,\n",
            "         6.2758e-01,  2.0289e-02, -2.4248e-01,  1.8642e+00,  1.6879e-02,\n",
            "         1.1319e+00, -2.5005e-01,  8.0178e-01,  7.2047e-01,  1.5414e+00,\n",
            "         1.5546e-01,  9.9407e-01, -1.7276e-01,  1.1871e+00,  2.1427e-01,\n",
            "         1.4338e+00,  2.6756e-03,  1.0376e+00,  1.6119e-01,  1.3290e+00,\n",
            "         2.1787e-01,  6.5497e-01, -8.9987e-01,  1.3237e+00, -6.6338e-01,\n",
            "         4.3294e-01, -1.1452e-01,  6.9533e-01, -6.9655e-01,  8.7103e-01,\n",
            "        -1.4427e-01,  1.1220e+00, -1.1310e-01,  1.6146e+00, -8.2545e-02,\n",
            "         6.6818e-01,  5.3879e-01,  1.0883e+00,  3.1021e-02,  8.2764e-01,\n",
            "        -8.4703e-02,  1.1959e+00,  3.9107e-01,  1.0548e+00,  6.5133e-01,\n",
            "         1.3449e+00,  4.9756e-01,  5.8532e-01,  6.1588e-01,  7.7434e-01,\n",
            "        -4.5558e-01,  1.0655e+00,  5.0525e-02,  1.4172e+00,  1.4788e-01,\n",
            "         1.2834e+00,  6.2862e-01,  8.0935e-01,  2.3687e-01,  1.1593e+00,\n",
            "        -5.1416e-01,  1.0166e+00, -1.9821e-01,  7.5079e-01, -6.3095e-02,\n",
            "         1.2595e+00, -6.3252e-01,  1.5850e-01,  2.8405e-02,  5.8232e-01,\n",
            "         1.7880e-01,  1.4087e+00,  3.2409e-01,  1.3299e+00,  2.2168e-01,\n",
            "         1.0275e+00,  5.2049e-02,  1.1813e+00,  5.3573e-01,  1.4081e+00,\n",
            "        -2.9359e-01,  4.1496e-01,  8.2843e-02,  9.2076e-01,  6.0473e-01,\n",
            "         1.1966e+00,  2.2256e-02,  7.2878e-01], grad_fn=<SelectBackward>)\n",
            "torch.Size([64, 128])\n",
            "True\n",
            "x dims:  torch.Size([64, 128])\n",
            "x after pos_encoder:  torch.Size([64, 128])\n",
            "x after unsqueeze:  torch.Size([64, 1, 128])\n",
            "enc_out after transformer_encoder:  torch.Size([64, 1, 128])\n",
            "dec_out after transformer_decoder:  torch.Size([64, 1, 128])\n",
            "tensor([[-0.2129,  0.3136],\n",
            "        [ 1.9121, -1.9910],\n",
            "        [ 1.4224, -2.8822]], grad_fn=<SliceBackward>)\n",
            "torch.Size([64, 2])\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4LYWCfobjKA"
      },
      "source": [
        "# Define model\n",
        "class Conv1dTransformer(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, stride, padding, feature_dim, batch_len, num_layers=1, dropout=0.1):\n",
        "        super(Conv1dTransformer, self).__init__()\n",
        "        self.conv = Conv1dMaxPool(in_channels, kernel_size, stride, padding)\n",
        "        self.transformer = Transformer(feature_dim, batch_len, num_layers, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.transformer(x)\n",
        "        return x"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWQGc5DVdqq1",
        "outputId": "3261f257-dcf4-4649-c88e-1262a44035bf"
      },
      "source": [
        "# Run conv to output the feature_dim\n",
        "Conv = Conv1dMaxPool(in_channels=1, kernel_size=3, stride=2, padding=1).to(DEVICE)\n",
        "\n",
        "# Check that it works\n",
        "train_batch_sample = next(iter(train_dataloader))\n",
        "train_batch_sample = train_batch_sample[0]\n",
        "conv_outputs = Conv(train_batch_sample.float())\n",
        "feature_dim = conv_outputs.shape[1]\n",
        "print(feature_dim)\n",
        "\n",
        "# set model with params\n",
        "model = Conv1dTransformer(in_channels=1, kernel_size=3, stride=2, padding=1, \\\n",
        "                             feature_dim=feature_dim, batch_len=BATCH_SIZE, num_layers=3, dropout=0.3).to(DEVICE)\n",
        "print(model)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "Conv1dTransformer(\n",
            "  (conv): Conv1dMaxPool(\n",
            "    (conv_11): Conv1d(1, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (conv_12): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (max_pool_1): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (conv_21): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (conv_22): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (max_pool_2): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (conv_31): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (conv_32): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (max_pool_3): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (out_encoder): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (transformer): Transformer(\n",
            "    (pos_encoder): PositionalEncoding()\n",
            "    (encoder_layer): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.3, inplace=False)\n",
            "      (dropout2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (transformer_encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder_layer): TransformerDecoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (multihead_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (dropout3): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wM_NRmwNpoyl",
        "outputId": "1bb0e0cc-034f-4071-918a-02e650d3eb7f"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) # default lr: 5e-3\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "data_dict ={}\n",
        "best_loss = 1000\n",
        "best_epoch = 0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    print(\"epoch: \", epoch)\n",
        "    losses = []\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # print(data[0])\n",
        "        features = Variable(data[0].float(), requires_grad=True).to(DEVICE)\n",
        "        # print(\"Backward prop-able\", features.requires_grad)\n",
        "        # print(features[0])\n",
        "        # features = features.unsqueeze(0)\n",
        "        # print(\"imu shape: \", imu_features.shape)\n",
        "        x = data[1].float().unsqueeze(-1).to(DEVICE)\n",
        "        # print(\"x shape: \", x.shape)\n",
        "        y = data[2].float().unsqueeze(-1).to(DEVICE)\n",
        "        # print(\"y shape: \", y.shape)\n",
        "\n",
        "        # output = model(features, prints=False)\n",
        "        # print(\"output\", output)\n",
        "        # print(\"output shape after concat: \", output.shape)\n",
        "        # conv_outputs = (features)\n",
        "        # print(\"conv output\", conv_outputs[0])\n",
        "        # ts_out = ts(conv_outputs)\n",
        "        output = model(features)\n",
        "        # print(\"batch \", i, \" : \", ts_out)\n",
        "\n",
        "        x = Variable(x, requires_grad=True)\n",
        "        y = Variable(y, requires_grad=True)\n",
        "        label = torch.cat([x, y], dim=-1)\n",
        "        # print(\"ts_out[0]: \", ts_out[0])\n",
        "        # print(\"label[0]: \", label[0])\n",
        "        # print(\"label\", label)\n",
        "        # print(\"label shape after concat: \", label.shape)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        mean_loss = np.mean(losses)\n",
        "        print(f\"mean loss:{mean_loss} at iteration {i}\")\n",
        "\n",
        "    scheduler.step(mean_loss)\n",
        "    print(\"Current LR: \", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "mean loss:7674.5615234375 at iteration 0\n",
            "mean loss:6831.4755859375 at iteration 1\n",
            "mean loss:6308.854817708333 at iteration 2\n",
            "mean loss:5967.080810546875 at iteration 3\n",
            "mean loss:5864.74794921875 at iteration 4\n",
            "mean loss:5770.685953776042 at iteration 5\n",
            "mean loss:5887.339704241072 at iteration 6\n",
            "mean loss:5617.594909667969 at iteration 7\n",
            "mean loss:5708.903374565973 at iteration 8\n",
            "mean loss:5594.8458984375 at iteration 9\n",
            "mean loss:5611.204634232955 at iteration 10\n",
            "mean loss:5525.122233072917 at iteration 11\n",
            "mean loss:5567.725811298077 at iteration 12\n",
            "mean loss:5517.38982282366 at iteration 13\n",
            "mean loss:5482.741048177083 at iteration 14\n",
            "mean loss:5456.4122314453125 at iteration 15\n",
            "mean loss:5457.842543658088 at iteration 16\n",
            "mean loss:5536.387098524306 at iteration 17\n",
            "mean loss:5534.611173930921 at iteration 18\n",
            "mean loss:5549.255810546875 at iteration 19\n",
            "mean loss:5535.085635230655 at iteration 20\n",
            "mean loss:5550.742653586648 at iteration 21\n",
            "mean loss:5565.10744310462 at iteration 22\n",
            "mean loss:5579.105326334636 at iteration 23\n",
            "mean loss:5591.61294921875 at iteration 24\n",
            "mean loss:5551.379957932692 at iteration 25\n",
            "mean loss:5554.7958080150465 at iteration 26\n",
            "mean loss:5553.068394252232 at iteration 27\n",
            "mean loss:5517.748989762931 at iteration 28\n",
            "mean loss:5502.63701171875 at iteration 29\n",
            "mean loss:5495.319241431452 at iteration 30\n",
            "mean loss:5483.24169921875 at iteration 31\n",
            "mean loss:5497.152935606061 at iteration 32\n",
            "mean loss:5504.745404411765 at iteration 33\n",
            "mean loss:5533.363434709821 at iteration 34\n",
            "mean loss:5544.8708089192705 at iteration 35\n",
            "mean loss:5551.7432300464525 at iteration 36\n",
            "mean loss:5574.6100046258225 at iteration 37\n",
            "mean loss:5536.163993639823 at iteration 38\n",
            "mean loss:5524.187176513672 at iteration 39\n",
            "mean loss:5520.286984327363 at iteration 40\n",
            "mean loss:5486.498389834449 at iteration 41\n",
            "mean loss:5508.104554642078 at iteration 42\n",
            "mean loss:5495.481817072088 at iteration 43\n",
            "mean loss:5493.373833550348 at iteration 44\n",
            "mean loss:5461.5622399371605 at iteration 45\n",
            "mean loss:5500.32406811004 at iteration 46\n",
            "mean loss:5476.76579284668 at iteration 47\n",
            "mean loss:5476.602922712053 at iteration 48\n",
            "mean loss:5478.724790039062 at iteration 49\n",
            "mean loss:5458.38276941636 at iteration 50\n",
            "mean loss:5448.939223069411 at iteration 51\n",
            "mean loss:5438.509116118809 at iteration 52\n",
            "mean loss:5441.675279405382 at iteration 53\n",
            "mean loss:5456.990567294034 at iteration 54\n",
            "mean loss:5437.630933489118 at iteration 55\n",
            "mean loss:5446.051650733279 at iteration 56\n",
            "mean loss:5436.424236429149 at iteration 57\n",
            "mean loss:5419.469283798993 at iteration 58\n",
            "mean loss:5413.681823730469 at iteration 59\n",
            "mean loss:5432.46149382044 at iteration 60\n",
            "mean loss:5412.779930853075 at iteration 61\n",
            "mean loss:5435.352062406994 at iteration 62\n",
            "mean loss:5433.702793121338 at iteration 63\n",
            "mean loss:5429.623005558894 at iteration 64\n",
            "mean loss:5399.350852272727 at iteration 65\n",
            "mean loss:5400.295767257463 at iteration 66\n",
            "mean loss:5402.713106043198 at iteration 67\n",
            "mean loss:5395.640058876812 at iteration 68\n",
            "mean loss:5400.349204799107 at iteration 69\n",
            "mean loss:5390.288498569542 at iteration 70\n",
            "mean loss:5370.766994900174 at iteration 71\n",
            "mean loss:5361.387233786387 at iteration 72\n",
            "mean loss:5356.299296611064 at iteration 73\n",
            "mean loss:5369.654498697917 at iteration 74\n",
            "mean loss:5365.489476254112 at iteration 75\n",
            "mean loss:5377.672318892045 at iteration 76\n",
            "mean loss:5372.824481670673 at iteration 77\n",
            "mean loss:5385.4782436708865 at iteration 78\n",
            "mean loss:5393.123364257813 at iteration 79\n",
            "mean loss:5383.167691454475 at iteration 80\n",
            "mean loss:5387.122117949695 at iteration 81\n",
            "mean loss:5369.215090832078 at iteration 82\n",
            "mean loss:5365.129772367932 at iteration 83\n",
            "mean loss:5369.727309283088 at iteration 84\n",
            "mean loss:5369.373421602471 at iteration 85\n",
            "mean loss:5352.219737787356 at iteration 86\n",
            "mean loss:5345.3023015802555 at iteration 87\n",
            "mean loss:5343.6380464360955 at iteration 88\n",
            "mean loss:5347.9461046006945 at iteration 89\n",
            "mean loss:5341.02145754636 at iteration 90\n",
            "mean loss:5346.426439368207 at iteration 91\n",
            "mean loss:5354.915154569892 at iteration 92\n",
            "mean loss:5351.18148998504 at iteration 93\n",
            "mean loss:5339.667896792763 at iteration 94\n",
            "mean loss:5339.4252522786455 at iteration 95\n",
            "mean loss:5333.659335736147 at iteration 96\n",
            "mean loss:5338.2355359135845 at iteration 97\n",
            "mean loss:5331.092783301768 at iteration 98\n",
            "mean loss:5321.767666015625 at iteration 99\n",
            "mean loss:5324.293007425743 at iteration 100\n",
            "mean loss:5317.397355621936 at iteration 101\n",
            "mean loss:5311.941102851942 at iteration 102\n",
            "mean loss:5314.59530874399 at iteration 103\n",
            "mean loss:5314.353413318452 at iteration 104\n",
            "mean loss:5308.1719394899765 at iteration 105\n",
            "mean loss:5292.159636025117 at iteration 106\n",
            "mean loss:5284.316248010706 at iteration 107\n",
            "mean loss:5289.227431551032 at iteration 108\n",
            "mean loss:5293.349573863637 at iteration 109\n",
            "mean loss:5286.751495636261 at iteration 110\n",
            "mean loss:5279.862779889788 at iteration 111\n",
            "mean loss:5285.628932176438 at iteration 112\n",
            "mean loss:5275.232473273027 at iteration 113\n",
            "mean loss:5273.632375169837 at iteration 114\n",
            "mean loss:5265.614956559806 at iteration 115\n",
            "mean loss:5262.542167467948 at iteration 116\n",
            "mean loss:5255.446769067797 at iteration 117\n",
            "mean loss:5247.384355304622 at iteration 118\n",
            "mean loss:5238.8233357747395 at iteration 119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-412671a584e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLBL0hNWr8r-"
      },
      "source": [
        "# def evaluate(model, data_loader,  device='cuda'):\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "#     x_list = []\n",
        "#     y_list = []\n",
        "#     floor_list = []\n",
        "#     prexs_list = []\n",
        "#     preys_list = []\n",
        "#     prefloors_list = []\n",
        "#     for d in tqdm(data_loader):\n",
        "#         data_dict['BSSID_FEATS'] = d['BSSID_FEATS'].to(device).long()\n",
        "#         data_dict['RSSI_FEATS'] = d['RSSI_FEATS'].to(device).float()\n",
        "#         data_dict['site_id'] = d['site_id'].to(device).long()\n",
        "#         x = d['x'].to(device).float()\n",
        "#         y = d['y'].to(device).float()\n",
        "#         floor = d['floor'].to(device).long()\n",
        "#         x_list.append(x.cpu().detach().numpy())\n",
        "#         y_list.append(y.cpu().detach().numpy())\n",
        "#         floor_list.append(floor.cpu().detach().numpy())\n",
        "#         xy, floor = model(data_dict)\n",
        "#         prexs_list.append(xy[:, 0].cpu().detach().numpy())\n",
        "#         preys_list.append(xy[:, 1].cpu().detach().numpy())\n",
        "#         prefloors_list.append(floor.squeeze().cpu().detach().numpy())\n",
        "#     x = np.concatenate(x_list)\n",
        "#     y = np.concatenate(y_list)\n",
        "#     floor = np.concatenate(floor_list)\n",
        "#     prexs = np.concatenate(prexs_list)\n",
        "#     preys =np.concatenate(preys_list)\n",
        "#     prefloors = np.concatenate(prefloors_list)\n",
        "#     eval_score = comp_metric(x, y, floor, prexs, preys, prefloors)\n",
        "#     return eval_score\n",
        "\n",
        "# def get_result(model, data_loader, device='cuda'):\n",
        "#     model.eval()\n",
        "#     model.to(device)\n",
        "#     prexs_list = []\n",
        "#     preys_list = []\n",
        "#     prefloors_list = []\n",
        "#     data_dict = {}\n",
        "#     for d in tqdm(data_loader):\n",
        "#         data_dict['BSSID_FEATS'] = d['BSSID_FEATS'].to(device).long()\n",
        "#         data_dict['RSSI_FEATS'] = d['RSSI_FEATS'].to(device).float()\n",
        "#         data_dict['site_id'] = d['site_id'].to(device).long()\n",
        "#         xy, floor = model(data_dict)\n",
        "#         prexs_list.append(xy[:, 0].cpu().detach().numpy())\n",
        "#         preys_list.append(xy[:, 1].cpu().detach().numpy())\n",
        "#         prefloors_list.append(floor.squeeze(-1).cpu().detach().numpy())\n",
        "#     prexs = np.concatenate(prexs_list)\n",
        "#     preys =np.concatenate(preys_list)\n",
        "#     prefloors = np.concatenate(prefloors_list)\n",
        "#     return prexs, preys, prefloors"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}