{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "indoor_model_1ConvTransformer_train_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_deO9NH18_Kb"
      },
      "source": [
        "# Mounting GCS to colab\n",
        "# https://stackoverflow.com/questions/51715268/how-to-import-data-from-google-cloud-storage-to-google-colab\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_w17mjH-0gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0032dc65-657a-4a13-a565-76d72230ff34"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  74617      0 --:--:-- --:--:-- --:--:-- 74617\n",
            "OK\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 10.6 MB of archives.\n",
            "After this operation, 22.7 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.34.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.34.1) ...\n",
            "Setting up gcsfuse (0.34.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoG6QV8P_FC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad4d00c-0231-48aa-a88f-1e801b855fa6"
      },
      "source": [
        "!mkdir colab_indoor\n",
        "!gcsfuse indoor-data colab_indoor\n",
        "# !mkdir colab_indoor/train_4\n",
        "# !gcsfuse indoor-data/train_4 colab_indoor/train_4\n",
        "# !mkdir colab_indoor/test_4\n",
        "# !gcsfuse indoor-data/test_4 colab_indoor/test_4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021/04/23 05:21:54.433739 Using mount point: /content/colab_indoor\n",
            "2021/04/23 05:21:54.443064 Opening GCS connection...\n",
            "2021/04/23 05:21:54.629457 Mounting file system \"indoor-data\"...\n",
            "2021/04/23 05:21:54.643897 File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LLEkzgi_hq9",
        "outputId": "15f721bf-0b4c-48d9-de9e-f95ac4b880ce"
      },
      "source": [
        "!ls -la -h ./colab_indoor/train_4_colcut"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.2G\n",
            "-rw-r--r-- 1 root root  43M Apr 14 21:07 5a0546857ecc773753327266_train.csv\n",
            "-rw-r--r-- 1 root root  46M Apr 14 21:07 5c3c44b80379370013e0fd2b_train.csv\n",
            "-rw-r--r-- 1 root root 113M Apr 14 21:08 5d27075f03f801723c2e360f_train.csv\n",
            "-rw-r--r-- 1 root root  43M Apr 14 21:08 5d27096c03f801723c31e5e0_train.csv\n",
            "-rw-r--r-- 1 root root  50M Apr 14 21:08 5d27097f03f801723c320d97_train.csv\n",
            "-rw-r--r-- 1 root root  12M Apr 14 21:08 5d27099f03f801723c32511d_train.csv\n",
            "-rw-r--r-- 1 root root  17M Apr 14 21:08 5d2709a003f801723c3251bf_train.csv\n",
            "-rw-r--r-- 1 root root  72M Apr 14 21:09 5d2709b303f801723c327472_train.csv\n",
            "-rw-r--r-- 1 root root  81M Apr 14 21:09 5d2709bb03f801723c32852c_train.csv\n",
            "-rw-r--r-- 1 root root  48M Apr 14 21:09 5d2709c303f801723c3299ee_train.csv\n",
            "-rw-r--r-- 1 root root  48M Apr 14 21:09 5d2709d403f801723c32bd39_train.csv\n",
            "-rw-r--r-- 1 root root  51M Apr 14 21:09 5d2709e003f801723c32d896_train.csv\n",
            "-rw-r--r-- 1 root root 3.9M Apr 14 21:10 5da138274db8ce0c98bbd3d2_train.csv\n",
            "-rw-r--r-- 1 root root  41M Apr 14 21:10 5da1382d4db8ce0c98bbe92e_train.csv\n",
            "-rw-r--r-- 1 root root  38M Apr 14 21:10 5da138314db8ce0c98bbf3a0_train.csv\n",
            "-rw-r--r-- 1 root root 6.7M Apr 14 21:10 5da138364db8ce0c98bc00f1_train.csv\n",
            "-rw-r--r-- 1 root root  63M Apr 14 21:10 5da1383b4db8ce0c98bc11ab_train.csv\n",
            "-rw-r--r-- 1 root root  35M Apr 14 21:10 5da138754db8ce0c98bca82f_train.csv\n",
            "-rw-r--r-- 1 root root  45M Apr 14 21:10 5da138764db8ce0c98bcaa46_train.csv\n",
            "-rw-r--r-- 1 root root  13M Apr 14 21:11 5da1389e4db8ce0c98bd0547_train.csv\n",
            "-rw-r--r-- 1 root root  82M Apr 14 21:11 5da138b74db8ce0c98bd4774_train.csv\n",
            "-rw-r--r-- 1 root root  71M Apr 14 21:11 5da958dd46f8266d0737457b_train.csv\n",
            "-rw-r--r-- 1 root root  76M Apr 14 21:11 5dbc1d84c1eb61796cf7c010_train.csv\n",
            "-rw-r--r-- 1 root root  75M Apr 14 21:12 5dc8cea7659e181adb076a3f_train.csv\n",
            "drwxr-xr-x 1 root root    0 Apr 23 05:21 .ipynb_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxo_gtclkLZo",
        "outputId": "631defe6-3d3c-403c-ee51-6be3e5d97724"
      },
      "source": [
        "import random\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import uniform_filter1d\n",
        "from scipy.interpolate import interp1d\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.stats as stats\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import multiprocessing\n",
        "import math\n",
        "\n",
        "EPOCH = 200 # default at 50\n",
        "BATCH_SIZE = 512\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FOLDS = 5\n",
        "\n",
        "NUM_CORES = multiprocessing.cpu_count()\n",
        "print(NUM_CORES)\n",
        "\n",
        "OUTPUT_NAME = \"train_8_colcut_Conv1dTransformer\"\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImcyWYeuWJEP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c98c493f-f21a-4964-cd70-871e160174f3"
      },
      "source": [
        "# train paths and test paths\n",
        "train_files = sorted(glob.glob(\"./colab_indoor/train_4_colcut/*\"))\n",
        "test_files = sorted(glob.glob(\"./colab_indoor/test_4_colcut/*\"))\n",
        "\n",
        "# load submission file\n",
        "sub_df = pd.read_csv(\"./colab_indoor/sample_submission.csv\", index_col=0)\n",
        "# sub_df[[\"site\", \"file\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
        "display(sub_df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_path_timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000000009</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000009017</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000015326</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000018763</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000022328</th>\n",
              "      <td>0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    floor     x     y\n",
              "site_path_timestamp                                                  \n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0\n",
              "5a0546857ecc773753327266_046cfa46be49fc10834815...      0  75.0  75.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "B2jyhARLkpkk",
        "outputId": "64852511-921e-40cc-a127-2501ec842a75"
      },
      "source": [
        "# Load train csv and test csv\n",
        "train_df = pd.read_csv(train_files[10])\n",
        "test_df = pd.read_csv(test_files[10])\n",
        "display(train_df.head())\n",
        "display(test_df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wifi_ts</th>\n",
              "      <th>wps_diff</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>file_id</th>\n",
              "      <th>site_id</th>\n",
              "      <th>b1847b7279f12430b70cf177ae9419b2c6563c7e</th>\n",
              "      <th>e78f6a70bb684764884bf5bfc22876354dc4978f</th>\n",
              "      <th>7f92c838def910dc333d6ec1785cc9de7c428514</th>\n",
              "      <th>49baf7f0e5495ee2926baa0e941d265d725dbdd5</th>\n",
              "      <th>9419ecb5700d351a0ca4c6f6dc4f7804c5800ebe</th>\n",
              "      <th>6c598641414c17dfca87a3acc982507ba08902cb</th>\n",
              "      <th>bf8984bf3344f3267b1fb3cb5dc95ddf7e2117d3</th>\n",
              "      <th>24a2a9c3edd47eb479f05d4067368c9e14b8964b</th>\n",
              "      <th>2b3f7594cc8ab2b0413d45d6eea1149a9a7809a7</th>\n",
              "      <th>1cecf9fc6a1ceba0ad7063fdb5816ec43e90e94f</th>\n",
              "      <th>018a066872b67d5b33570ddeb6142c602a22e451</th>\n",
              "      <th>55ee0fec7b7be3e76132b20fca01e8486743a3f6</th>\n",
              "      <th>4f32282342971b17eb8611ac5f145d0b8768f62d</th>\n",
              "      <th>ee2a7ce46e83e2de489c9f78f4e550156d57f484</th>\n",
              "      <th>606fc3f5664630412548bbd099f9c8ad1dc5acb6</th>\n",
              "      <th>40ab9973a268cc79a4a8210e67f9b912723ed8c3</th>\n",
              "      <th>c81b745f26735105787cf9e7b547a7c37a65a851</th>\n",
              "      <th>832743c77af1c7eb12fcd909c438dcf39b6008f3</th>\n",
              "      <th>7b42d018fdfe2ab118d8ec85bcf1d6983750750c</th>\n",
              "      <th>ef99ef623422c284a774e35fc1b1cf3e721dbd03</th>\n",
              "      <th>c1591135c00bca7a723a49df8c9d8de34326fbd3</th>\n",
              "      <th>3956907df9b51fae849a7afced246f127f259504</th>\n",
              "      <th>1359034ef247ed40bf5b3723915b9fc60cc74837</th>\n",
              "      <th>2a5b0895595bcf131a0b5c3e5dc4b9f39e06bd3f</th>\n",
              "      <th>ca7a7877b1dbad8f6ef696b1e88f34dc6f635b0d</th>\n",
              "      <th>396640532a81255d6b75b41e7a184c1393b74ee4</th>\n",
              "      <th>aebc6ae6f9dfe7b548779e39cc9fd03c60a67ecc</th>\n",
              "      <th>70e578e027d0ab0bfeb5959e18f3c8045a32236d</th>\n",
              "      <th>07ccdba1f0a266adf501c867319ac7007e65d290</th>\n",
              "      <th>d7b2ba98b972b65650af520075c9712a3d8994a7</th>\n",
              "      <th>2b844704051b64e47b4800e1d6e932bbdd5356b6</th>\n",
              "      <th>1c390c87d362e628cebc103eb247d2b7aa813542</th>\n",
              "      <th>...</th>\n",
              "      <th>46e35eba345a6366234c028567bbb91c99b872e5</th>\n",
              "      <th>298e1870fda64fe8018ec79e835cf308f81181fa</th>\n",
              "      <th>31b0561e7e9a03156b248da215d7ab7bf8d6d8c8</th>\n",
              "      <th>3ab6773bc6bce7fb154b3d46f6058f0192bfd01b</th>\n",
              "      <th>41d0c04a20a77e8daaba6c585f083ca0ef57affc</th>\n",
              "      <th>a3dfc65c01d171a671cd615236b79c61810de354</th>\n",
              "      <th>06028150b43acb7e11121888ab434cdd5f679f1d</th>\n",
              "      <th>f7853f850bbb98d20bb2b4dd6380c071b5a1cb02</th>\n",
              "      <th>078a75dfda9e932f608fca0837610b97d11d3dd7</th>\n",
              "      <th>fc8fd87104388e513937ac13696b6f72af2add53</th>\n",
              "      <th>422765063ea5b0e5667a240e3b802041ab93b6e3</th>\n",
              "      <th>6cfaf9601b6e2587a321d3b803c20f9eb05fd452</th>\n",
              "      <th>35c0ef8cb2af11a81092979d55c4324b8101332f</th>\n",
              "      <th>764cbb8b20a46c411fe5807df9867b92c0935c08</th>\n",
              "      <th>78209eb1032640dd8f611e310c8619ab2c9cb071</th>\n",
              "      <th>1fc19e52666a95169b365f5ad1352d9369cb045a</th>\n",
              "      <th>14d3489b6de7d970babc8a951bf6ed5bd289dea0</th>\n",
              "      <th>6646a9365ace5164d2ef3dfb39b88a8a924568d3</th>\n",
              "      <th>a7ac17ed71cc594bed820bfbbca833f3f9972592</th>\n",
              "      <th>976e778d614b1e8e9efa8bc7eeaf552b7fe5ccb5</th>\n",
              "      <th>5749b4541786aea857ff1552dfac8cf292044c29</th>\n",
              "      <th>b1f4814d1a8c7885960118015e2c8f47fcdd0f4f</th>\n",
              "      <th>75df7c8888ddbf148407aa76c3d6b72406bc4c27</th>\n",
              "      <th>fc88f6a4f0351f8a2b34d256fcda35579d9e0ead</th>\n",
              "      <th>21f079033442b71f1a870ca0988fa9afcc7dc810</th>\n",
              "      <th>3f8321ce0dd6348fa440eb25acae2be996c3b2e7</th>\n",
              "      <th>9054af7ed2ceb6b130b4fd2a09f36b9b1f611f11</th>\n",
              "      <th>77de2f0f3005949e35e2e2aef7af1ddc5d164695</th>\n",
              "      <th>588f35babb83fab68a9c539567140ba5c17284c7</th>\n",
              "      <th>68962923d8621f6868a762b2344ce1ab8209ecdd</th>\n",
              "      <th>7abbb83f5b877ce5c1c9e835422365cd8bac3b0d</th>\n",
              "      <th>acb4abc93968ad51368a0351fa7fda5401598069</th>\n",
              "      <th>f20fed9f248d4352def744d9a8262b49042ee7f3</th>\n",
              "      <th>437d45cd7c45b040b95abdb483b02d601c761d7d</th>\n",
              "      <th>b083b8279418c48d60a05cff039b0536339a1e4d</th>\n",
              "      <th>fa0d8c168b62f11f34fdf314d81cebdf1ca9ebd0</th>\n",
              "      <th>955eb46cfdee70adab1de9ce909f0cb059fd29d2</th>\n",
              "      <th>48d751207622a3606df95184fd58b32347057af3</th>\n",
              "      <th>5238e869a2d0d2a851ba1f9484c874c57419148e</th>\n",
              "      <th>63781517942f8b63c2189c376cc0cf3db1d27ed1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1573789444679</td>\n",
              "      <td>1918</td>\n",
              "      <td>88.03561</td>\n",
              "      <td>109.90122</td>\n",
              "      <td>B1</td>\n",
              "      <td>-1</td>\n",
              "      <td>5dce2cf294e4900006124d2a</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-76</td>\n",
              "      <td>-74</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-49</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-46</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1573789446609</td>\n",
              "      <td>3848</td>\n",
              "      <td>88.03561</td>\n",
              "      <td>109.90122</td>\n",
              "      <td>B1</td>\n",
              "      <td>-1</td>\n",
              "      <td>5dce2cf294e4900006124d2a</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-79</td>\n",
              "      <td>-79</td>\n",
              "      <td>-83</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-44</td>\n",
              "      <td>-88</td>\n",
              "      <td>-78</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-59</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1573789448539</td>\n",
              "      <td>5298</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>B1</td>\n",
              "      <td>-1</td>\n",
              "      <td>5dce2cf294e4900006124d2a</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-68</td>\n",
              "      <td>-67</td>\n",
              "      <td>-67</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-78</td>\n",
              "      <td>-85</td>\n",
              "      <td>-46</td>\n",
              "      <td>-88</td>\n",
              "      <td>-78</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-48</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1573789450452</td>\n",
              "      <td>3385</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>B1</td>\n",
              "      <td>-1</td>\n",
              "      <td>5dce2cf294e4900006124d2a</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-73</td>\n",
              "      <td>-73</td>\n",
              "      <td>-77</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-74</td>\n",
              "      <td>-91</td>\n",
              "      <td>-55</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-42</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-83</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1573789452370</td>\n",
              "      <td>1467</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>B1</td>\n",
              "      <td>-1</td>\n",
              "      <td>5dce2cf294e4900006124d2a</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-72</td>\n",
              "      <td>-71</td>\n",
              "      <td>-70</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-76</td>\n",
              "      <td>-81</td>\n",
              "      <td>-91</td>\n",
              "      <td>-48</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-78</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-47</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-86</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 999 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         wifi_ts  ...  63781517942f8b63c2189c376cc0cf3db1d27ed1\n",
              "0  1573789444679  ...                                      -999\n",
              "1  1573789446609  ...                                      -999\n",
              "2  1573789448539  ...                                      -999\n",
              "3  1573789450452  ...                                      -999\n",
              "4  1573789452370  ...                                      -999\n",
              "\n",
              "[5 rows x 999 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site_path_timestamp</th>\n",
              "      <th>correct_wps_ts</th>\n",
              "      <th>wifi_ts</th>\n",
              "      <th>wps_diff</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>file_id</th>\n",
              "      <th>site_id</th>\n",
              "      <th>b1847b7279f12430b70cf177ae9419b2c6563c7e</th>\n",
              "      <th>e78f6a70bb684764884bf5bfc22876354dc4978f</th>\n",
              "      <th>7f92c838def910dc333d6ec1785cc9de7c428514</th>\n",
              "      <th>49baf7f0e5495ee2926baa0e941d265d725dbdd5</th>\n",
              "      <th>9419ecb5700d351a0ca4c6f6dc4f7804c5800ebe</th>\n",
              "      <th>6c598641414c17dfca87a3acc982507ba08902cb</th>\n",
              "      <th>bf8984bf3344f3267b1fb3cb5dc95ddf7e2117d3</th>\n",
              "      <th>24a2a9c3edd47eb479f05d4067368c9e14b8964b</th>\n",
              "      <th>2b3f7594cc8ab2b0413d45d6eea1149a9a7809a7</th>\n",
              "      <th>1cecf9fc6a1ceba0ad7063fdb5816ec43e90e94f</th>\n",
              "      <th>018a066872b67d5b33570ddeb6142c602a22e451</th>\n",
              "      <th>55ee0fec7b7be3e76132b20fca01e8486743a3f6</th>\n",
              "      <th>4f32282342971b17eb8611ac5f145d0b8768f62d</th>\n",
              "      <th>ee2a7ce46e83e2de489c9f78f4e550156d57f484</th>\n",
              "      <th>606fc3f5664630412548bbd099f9c8ad1dc5acb6</th>\n",
              "      <th>40ab9973a268cc79a4a8210e67f9b912723ed8c3</th>\n",
              "      <th>c81b745f26735105787cf9e7b547a7c37a65a851</th>\n",
              "      <th>832743c77af1c7eb12fcd909c438dcf39b6008f3</th>\n",
              "      <th>7b42d018fdfe2ab118d8ec85bcf1d6983750750c</th>\n",
              "      <th>ef99ef623422c284a774e35fc1b1cf3e721dbd03</th>\n",
              "      <th>c1591135c00bca7a723a49df8c9d8de34326fbd3</th>\n",
              "      <th>3956907df9b51fae849a7afced246f127f259504</th>\n",
              "      <th>1359034ef247ed40bf5b3723915b9fc60cc74837</th>\n",
              "      <th>2a5b0895595bcf131a0b5c3e5dc4b9f39e06bd3f</th>\n",
              "      <th>ca7a7877b1dbad8f6ef696b1e88f34dc6f635b0d</th>\n",
              "      <th>396640532a81255d6b75b41e7a184c1393b74ee4</th>\n",
              "      <th>aebc6ae6f9dfe7b548779e39cc9fd03c60a67ecc</th>\n",
              "      <th>70e578e027d0ab0bfeb5959e18f3c8045a32236d</th>\n",
              "      <th>07ccdba1f0a266adf501c867319ac7007e65d290</th>\n",
              "      <th>d7b2ba98b972b65650af520075c9712a3d8994a7</th>\n",
              "      <th>...</th>\n",
              "      <th>46e35eba345a6366234c028567bbb91c99b872e5</th>\n",
              "      <th>298e1870fda64fe8018ec79e835cf308f81181fa</th>\n",
              "      <th>31b0561e7e9a03156b248da215d7ab7bf8d6d8c8</th>\n",
              "      <th>3ab6773bc6bce7fb154b3d46f6058f0192bfd01b</th>\n",
              "      <th>41d0c04a20a77e8daaba6c585f083ca0ef57affc</th>\n",
              "      <th>a3dfc65c01d171a671cd615236b79c61810de354</th>\n",
              "      <th>06028150b43acb7e11121888ab434cdd5f679f1d</th>\n",
              "      <th>f7853f850bbb98d20bb2b4dd6380c071b5a1cb02</th>\n",
              "      <th>078a75dfda9e932f608fca0837610b97d11d3dd7</th>\n",
              "      <th>fc8fd87104388e513937ac13696b6f72af2add53</th>\n",
              "      <th>422765063ea5b0e5667a240e3b802041ab93b6e3</th>\n",
              "      <th>6cfaf9601b6e2587a321d3b803c20f9eb05fd452</th>\n",
              "      <th>35c0ef8cb2af11a81092979d55c4324b8101332f</th>\n",
              "      <th>764cbb8b20a46c411fe5807df9867b92c0935c08</th>\n",
              "      <th>78209eb1032640dd8f611e310c8619ab2c9cb071</th>\n",
              "      <th>1fc19e52666a95169b365f5ad1352d9369cb045a</th>\n",
              "      <th>14d3489b6de7d970babc8a951bf6ed5bd289dea0</th>\n",
              "      <th>6646a9365ace5164d2ef3dfb39b88a8a924568d3</th>\n",
              "      <th>a7ac17ed71cc594bed820bfbbca833f3f9972592</th>\n",
              "      <th>976e778d614b1e8e9efa8bc7eeaf552b7fe5ccb5</th>\n",
              "      <th>5749b4541786aea857ff1552dfac8cf292044c29</th>\n",
              "      <th>b1f4814d1a8c7885960118015e2c8f47fcdd0f4f</th>\n",
              "      <th>75df7c8888ddbf148407aa76c3d6b72406bc4c27</th>\n",
              "      <th>fc88f6a4f0351f8a2b34d256fcda35579d9e0ead</th>\n",
              "      <th>21f079033442b71f1a870ca0988fa9afcc7dc810</th>\n",
              "      <th>3f8321ce0dd6348fa440eb25acae2be996c3b2e7</th>\n",
              "      <th>9054af7ed2ceb6b130b4fd2a09f36b9b1f611f11</th>\n",
              "      <th>77de2f0f3005949e35e2e2aef7af1ddc5d164695</th>\n",
              "      <th>588f35babb83fab68a9c539567140ba5c17284c7</th>\n",
              "      <th>68962923d8621f6868a762b2344ce1ab8209ecdd</th>\n",
              "      <th>7abbb83f5b877ce5c1c9e835422365cd8bac3b0d</th>\n",
              "      <th>acb4abc93968ad51368a0351fa7fda5401598069</th>\n",
              "      <th>f20fed9f248d4352def744d9a8262b49042ee7f3</th>\n",
              "      <th>437d45cd7c45b040b95abdb483b02d601c761d7d</th>\n",
              "      <th>b083b8279418c48d60a05cff039b0536339a1e4d</th>\n",
              "      <th>fa0d8c168b62f11f34fdf314d81cebdf1ca9ebd0</th>\n",
              "      <th>955eb46cfdee70adab1de9ce909f0cb059fd29d2</th>\n",
              "      <th>48d751207622a3606df95184fd58b32347057af3</th>\n",
              "      <th>5238e869a2d0d2a851ba1f9484c874c57419148e</th>\n",
              "      <th>63781517942f8b63c2189c376cc0cf3db1d27ed1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5d2709d403f801723c32bd39_06882da3694b7160c0f10...</td>\n",
              "      <td>1.573706e+12</td>\n",
              "      <td>1573705854189</td>\n",
              "      <td>5859</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>06882da3694b7160c0f105f5</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-78</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-88</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5d2709d403f801723c32bd39_06882da3694b7160c0f10...</td>\n",
              "      <td>1.573706e+12</td>\n",
              "      <td>1573705854189</td>\n",
              "      <td>174</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>06882da3694b7160c0f105f5</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-78</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-88</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5d2709d403f801723c32bd39_06882da3694b7160c0f10...</td>\n",
              "      <td>1.573706e+12</td>\n",
              "      <td>1573705856155</td>\n",
              "      <td>67</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>06882da3694b7160c0f105f5</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-90</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5d2709d403f801723c32bd39_06882da3694b7160c0f10...</td>\n",
              "      <td>1.573706e+12</td>\n",
              "      <td>1573705862090</td>\n",
              "      <td>898</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>06882da3694b7160c0f105f5</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5d2709d403f801723c32bd39_06882da3694b7160c0f10...</td>\n",
              "      <td>1.573706e+12</td>\n",
              "      <td>1573705869915</td>\n",
              "      <td>720</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>06882da3694b7160c0f105f5</td>\n",
              "      <td>5d2709d403f801723c32bd39</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-71</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-74</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1001 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 site_path_timestamp  ...  63781517942f8b63c2189c376cc0cf3db1d27ed1\n",
              "0  5d2709d403f801723c32bd39_06882da3694b7160c0f10...  ...                                      -999\n",
              "1  5d2709d403f801723c32bd39_06882da3694b7160c0f10...  ...                                      -999\n",
              "2  5d2709d403f801723c32bd39_06882da3694b7160c0f10...  ...                                      -999\n",
              "3  5d2709d403f801723c32bd39_06882da3694b7160c0f10...  ...                                      -999\n",
              "4  5d2709d403f801723c32bd39_06882da3694b7160c0f10...  ...                                      -999\n",
              "\n",
              "[5 rows x 1001 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "oaMr1ftymrUB",
        "outputId": "9c09d41b-62b9-445d-f24e-829a44669767"
      },
      "source": [
        "# Match train and test columns\n",
        "all_train_cols = list(train_df.columns)\n",
        "all_test_cols = list(test_df.columns)\n",
        "print(\"all train cols: \", len(all_train_cols), \"\\n\", \"all test cols: \", len(all_test_cols))\n",
        "\n",
        "# get all non-overlapping columns\n",
        "no_overlap_col = list(set(all_train_cols) ^ set(all_test_cols))\n",
        "no_overlap_col += [\"floor\", \"file_id\", \"site_id\"] # add other columns to exclude\n",
        "train_cols = [x for x in all_train_cols if x not in no_overlap_col]\n",
        "test_cols = [x for x in all_test_cols if x not in no_overlap_col]\n",
        "# test_cols += [\"site_path_timestamp\"] # test_df needs to keep \"site_path_timestamp\"\n",
        "\n",
        "# filter out the df by the columns to leave\n",
        "train_df = train_df[train_cols]\n",
        "test_df = test_df[test_cols]\n",
        "\n",
        "# # Drop some columns not necessary as a feature\n",
        "# drop_cols = [\"wifi_ts\", \"floor\", \"file_id\", \"site_id\"]\n",
        "# for df in [train_df, test_df]:\n",
        "#     df = df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# Convert df object columns to integers and then the whole thing to tensors\n",
        "for df in [train_df, test_df]:\n",
        "    obj_col = list(df.select_dtypes(include=['object']).columns)\n",
        "    for col in obj_col:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].values)\n",
        "\n",
        "# # Try without using scalers for now. \n",
        "# # Apply MinMaxScaler to each IMU columns\n",
        "# current_cols = list(train_df.columns)\n",
        "# imu_cols = current_cols[:23]\n",
        "# exception_columns = [\"wps_diff\", \"x\", \"y\", \"floor_int\", \"rel_diff\", \"rel_x\", \"rel_y\"]\n",
        "# imu_cols = [x for x in imu_cols if x not in exception_columns]\n",
        "# print(\"imu_cols: \", imu_cols)\n",
        "# for col in imu_cols:\n",
        "#     ss_scaler = StandardScaler()\n",
        "#     train_df[col] = ss_scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n",
        "#     test_df[col] = ss_scaler.transform(test_df[col].values.reshape(-1, 1))\n",
        "\n",
        "print(len(train_df.columns))\n",
        "print(len(test_df.columns))\n",
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(\"object dtype columns in train\", train_df.select_dtypes(include=['object']).columns)\n",
        "print(\"object dtype columns in test\", test_df.select_dtypes(include=['object']).columns)\n",
        "display(train_df.head())\n",
        "display(test_df.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all train cols:  999 \n",
            " all test cols:  1001\n",
            "996\n",
            "996\n",
            "10027\n",
            "1223\n",
            "object dtype columns in train Index([], dtype='object')\n",
            "object dtype columns in test Index([], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wifi_ts</th>\n",
              "      <th>wps_diff</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>b1847b7279f12430b70cf177ae9419b2c6563c7e</th>\n",
              "      <th>e78f6a70bb684764884bf5bfc22876354dc4978f</th>\n",
              "      <th>7f92c838def910dc333d6ec1785cc9de7c428514</th>\n",
              "      <th>49baf7f0e5495ee2926baa0e941d265d725dbdd5</th>\n",
              "      <th>9419ecb5700d351a0ca4c6f6dc4f7804c5800ebe</th>\n",
              "      <th>6c598641414c17dfca87a3acc982507ba08902cb</th>\n",
              "      <th>bf8984bf3344f3267b1fb3cb5dc95ddf7e2117d3</th>\n",
              "      <th>24a2a9c3edd47eb479f05d4067368c9e14b8964b</th>\n",
              "      <th>2b3f7594cc8ab2b0413d45d6eea1149a9a7809a7</th>\n",
              "      <th>1cecf9fc6a1ceba0ad7063fdb5816ec43e90e94f</th>\n",
              "      <th>018a066872b67d5b33570ddeb6142c602a22e451</th>\n",
              "      <th>55ee0fec7b7be3e76132b20fca01e8486743a3f6</th>\n",
              "      <th>4f32282342971b17eb8611ac5f145d0b8768f62d</th>\n",
              "      <th>ee2a7ce46e83e2de489c9f78f4e550156d57f484</th>\n",
              "      <th>606fc3f5664630412548bbd099f9c8ad1dc5acb6</th>\n",
              "      <th>40ab9973a268cc79a4a8210e67f9b912723ed8c3</th>\n",
              "      <th>c81b745f26735105787cf9e7b547a7c37a65a851</th>\n",
              "      <th>832743c77af1c7eb12fcd909c438dcf39b6008f3</th>\n",
              "      <th>7b42d018fdfe2ab118d8ec85bcf1d6983750750c</th>\n",
              "      <th>ef99ef623422c284a774e35fc1b1cf3e721dbd03</th>\n",
              "      <th>c1591135c00bca7a723a49df8c9d8de34326fbd3</th>\n",
              "      <th>3956907df9b51fae849a7afced246f127f259504</th>\n",
              "      <th>1359034ef247ed40bf5b3723915b9fc60cc74837</th>\n",
              "      <th>2a5b0895595bcf131a0b5c3e5dc4b9f39e06bd3f</th>\n",
              "      <th>ca7a7877b1dbad8f6ef696b1e88f34dc6f635b0d</th>\n",
              "      <th>396640532a81255d6b75b41e7a184c1393b74ee4</th>\n",
              "      <th>aebc6ae6f9dfe7b548779e39cc9fd03c60a67ecc</th>\n",
              "      <th>70e578e027d0ab0bfeb5959e18f3c8045a32236d</th>\n",
              "      <th>07ccdba1f0a266adf501c867319ac7007e65d290</th>\n",
              "      <th>d7b2ba98b972b65650af520075c9712a3d8994a7</th>\n",
              "      <th>2b844704051b64e47b4800e1d6e932bbdd5356b6</th>\n",
              "      <th>1c390c87d362e628cebc103eb247d2b7aa813542</th>\n",
              "      <th>6d70c76ca1d2e44f634d98810313c1c91b07ffa3</th>\n",
              "      <th>ec097c076cbcdbbcda43d0eb6372101151a914e0</th>\n",
              "      <th>eb61afa796d2ebdf9ec651593837743dc6c3ef11</th>\n",
              "      <th>...</th>\n",
              "      <th>46e35eba345a6366234c028567bbb91c99b872e5</th>\n",
              "      <th>298e1870fda64fe8018ec79e835cf308f81181fa</th>\n",
              "      <th>31b0561e7e9a03156b248da215d7ab7bf8d6d8c8</th>\n",
              "      <th>3ab6773bc6bce7fb154b3d46f6058f0192bfd01b</th>\n",
              "      <th>41d0c04a20a77e8daaba6c585f083ca0ef57affc</th>\n",
              "      <th>a3dfc65c01d171a671cd615236b79c61810de354</th>\n",
              "      <th>06028150b43acb7e11121888ab434cdd5f679f1d</th>\n",
              "      <th>f7853f850bbb98d20bb2b4dd6380c071b5a1cb02</th>\n",
              "      <th>078a75dfda9e932f608fca0837610b97d11d3dd7</th>\n",
              "      <th>fc8fd87104388e513937ac13696b6f72af2add53</th>\n",
              "      <th>422765063ea5b0e5667a240e3b802041ab93b6e3</th>\n",
              "      <th>6cfaf9601b6e2587a321d3b803c20f9eb05fd452</th>\n",
              "      <th>35c0ef8cb2af11a81092979d55c4324b8101332f</th>\n",
              "      <th>764cbb8b20a46c411fe5807df9867b92c0935c08</th>\n",
              "      <th>78209eb1032640dd8f611e310c8619ab2c9cb071</th>\n",
              "      <th>1fc19e52666a95169b365f5ad1352d9369cb045a</th>\n",
              "      <th>14d3489b6de7d970babc8a951bf6ed5bd289dea0</th>\n",
              "      <th>6646a9365ace5164d2ef3dfb39b88a8a924568d3</th>\n",
              "      <th>a7ac17ed71cc594bed820bfbbca833f3f9972592</th>\n",
              "      <th>976e778d614b1e8e9efa8bc7eeaf552b7fe5ccb5</th>\n",
              "      <th>5749b4541786aea857ff1552dfac8cf292044c29</th>\n",
              "      <th>b1f4814d1a8c7885960118015e2c8f47fcdd0f4f</th>\n",
              "      <th>75df7c8888ddbf148407aa76c3d6b72406bc4c27</th>\n",
              "      <th>fc88f6a4f0351f8a2b34d256fcda35579d9e0ead</th>\n",
              "      <th>21f079033442b71f1a870ca0988fa9afcc7dc810</th>\n",
              "      <th>3f8321ce0dd6348fa440eb25acae2be996c3b2e7</th>\n",
              "      <th>9054af7ed2ceb6b130b4fd2a09f36b9b1f611f11</th>\n",
              "      <th>77de2f0f3005949e35e2e2aef7af1ddc5d164695</th>\n",
              "      <th>588f35babb83fab68a9c539567140ba5c17284c7</th>\n",
              "      <th>68962923d8621f6868a762b2344ce1ab8209ecdd</th>\n",
              "      <th>7abbb83f5b877ce5c1c9e835422365cd8bac3b0d</th>\n",
              "      <th>acb4abc93968ad51368a0351fa7fda5401598069</th>\n",
              "      <th>f20fed9f248d4352def744d9a8262b49042ee7f3</th>\n",
              "      <th>437d45cd7c45b040b95abdb483b02d601c761d7d</th>\n",
              "      <th>b083b8279418c48d60a05cff039b0536339a1e4d</th>\n",
              "      <th>fa0d8c168b62f11f34fdf314d81cebdf1ca9ebd0</th>\n",
              "      <th>955eb46cfdee70adab1de9ce909f0cb059fd29d2</th>\n",
              "      <th>48d751207622a3606df95184fd58b32347057af3</th>\n",
              "      <th>5238e869a2d0d2a851ba1f9484c874c57419148e</th>\n",
              "      <th>63781517942f8b63c2189c376cc0cf3db1d27ed1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1573789444679</td>\n",
              "      <td>1918</td>\n",
              "      <td>88.03561</td>\n",
              "      <td>109.90122</td>\n",
              "      <td>-1</td>\n",
              "      <td>-76</td>\n",
              "      <td>-74</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-49</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-46</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1573789446609</td>\n",
              "      <td>3848</td>\n",
              "      <td>88.03561</td>\n",
              "      <td>109.90122</td>\n",
              "      <td>-1</td>\n",
              "      <td>-79</td>\n",
              "      <td>-79</td>\n",
              "      <td>-83</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-44</td>\n",
              "      <td>-88</td>\n",
              "      <td>-78</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-59</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1573789448539</td>\n",
              "      <td>5298</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>-1</td>\n",
              "      <td>-68</td>\n",
              "      <td>-67</td>\n",
              "      <td>-67</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-78</td>\n",
              "      <td>-85</td>\n",
              "      <td>-46</td>\n",
              "      <td>-88</td>\n",
              "      <td>-78</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-48</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1573789450452</td>\n",
              "      <td>3385</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>-1</td>\n",
              "      <td>-73</td>\n",
              "      <td>-73</td>\n",
              "      <td>-77</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-89</td>\n",
              "      <td>-74</td>\n",
              "      <td>-91</td>\n",
              "      <td>-55</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-42</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-83</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1573789452370</td>\n",
              "      <td>1467</td>\n",
              "      <td>91.62208</td>\n",
              "      <td>108.70687</td>\n",
              "      <td>-1</td>\n",
              "      <td>-72</td>\n",
              "      <td>-71</td>\n",
              "      <td>-70</td>\n",
              "      <td>-78</td>\n",
              "      <td>-73</td>\n",
              "      <td>-76</td>\n",
              "      <td>-81</td>\n",
              "      <td>-91</td>\n",
              "      <td>-48</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-78</td>\n",
              "      <td>-76</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-47</td>\n",
              "      <td>-66</td>\n",
              "      <td>-75</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-77</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-86</td>\n",
              "      <td>-84</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 996 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         wifi_ts  ...  63781517942f8b63c2189c376cc0cf3db1d27ed1\n",
              "0  1573789444679  ...                                      -999\n",
              "1  1573789446609  ...                                      -999\n",
              "2  1573789448539  ...                                      -999\n",
              "3  1573789450452  ...                                      -999\n",
              "4  1573789452370  ...                                      -999\n",
              "\n",
              "[5 rows x 996 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wifi_ts</th>\n",
              "      <th>wps_diff</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>floor_int</th>\n",
              "      <th>b1847b7279f12430b70cf177ae9419b2c6563c7e</th>\n",
              "      <th>e78f6a70bb684764884bf5bfc22876354dc4978f</th>\n",
              "      <th>7f92c838def910dc333d6ec1785cc9de7c428514</th>\n",
              "      <th>49baf7f0e5495ee2926baa0e941d265d725dbdd5</th>\n",
              "      <th>9419ecb5700d351a0ca4c6f6dc4f7804c5800ebe</th>\n",
              "      <th>6c598641414c17dfca87a3acc982507ba08902cb</th>\n",
              "      <th>bf8984bf3344f3267b1fb3cb5dc95ddf7e2117d3</th>\n",
              "      <th>24a2a9c3edd47eb479f05d4067368c9e14b8964b</th>\n",
              "      <th>2b3f7594cc8ab2b0413d45d6eea1149a9a7809a7</th>\n",
              "      <th>1cecf9fc6a1ceba0ad7063fdb5816ec43e90e94f</th>\n",
              "      <th>018a066872b67d5b33570ddeb6142c602a22e451</th>\n",
              "      <th>55ee0fec7b7be3e76132b20fca01e8486743a3f6</th>\n",
              "      <th>4f32282342971b17eb8611ac5f145d0b8768f62d</th>\n",
              "      <th>ee2a7ce46e83e2de489c9f78f4e550156d57f484</th>\n",
              "      <th>606fc3f5664630412548bbd099f9c8ad1dc5acb6</th>\n",
              "      <th>40ab9973a268cc79a4a8210e67f9b912723ed8c3</th>\n",
              "      <th>c81b745f26735105787cf9e7b547a7c37a65a851</th>\n",
              "      <th>832743c77af1c7eb12fcd909c438dcf39b6008f3</th>\n",
              "      <th>7b42d018fdfe2ab118d8ec85bcf1d6983750750c</th>\n",
              "      <th>ef99ef623422c284a774e35fc1b1cf3e721dbd03</th>\n",
              "      <th>c1591135c00bca7a723a49df8c9d8de34326fbd3</th>\n",
              "      <th>3956907df9b51fae849a7afced246f127f259504</th>\n",
              "      <th>1359034ef247ed40bf5b3723915b9fc60cc74837</th>\n",
              "      <th>2a5b0895595bcf131a0b5c3e5dc4b9f39e06bd3f</th>\n",
              "      <th>ca7a7877b1dbad8f6ef696b1e88f34dc6f635b0d</th>\n",
              "      <th>396640532a81255d6b75b41e7a184c1393b74ee4</th>\n",
              "      <th>aebc6ae6f9dfe7b548779e39cc9fd03c60a67ecc</th>\n",
              "      <th>70e578e027d0ab0bfeb5959e18f3c8045a32236d</th>\n",
              "      <th>07ccdba1f0a266adf501c867319ac7007e65d290</th>\n",
              "      <th>d7b2ba98b972b65650af520075c9712a3d8994a7</th>\n",
              "      <th>2b844704051b64e47b4800e1d6e932bbdd5356b6</th>\n",
              "      <th>1c390c87d362e628cebc103eb247d2b7aa813542</th>\n",
              "      <th>6d70c76ca1d2e44f634d98810313c1c91b07ffa3</th>\n",
              "      <th>ec097c076cbcdbbcda43d0eb6372101151a914e0</th>\n",
              "      <th>eb61afa796d2ebdf9ec651593837743dc6c3ef11</th>\n",
              "      <th>...</th>\n",
              "      <th>46e35eba345a6366234c028567bbb91c99b872e5</th>\n",
              "      <th>298e1870fda64fe8018ec79e835cf308f81181fa</th>\n",
              "      <th>31b0561e7e9a03156b248da215d7ab7bf8d6d8c8</th>\n",
              "      <th>3ab6773bc6bce7fb154b3d46f6058f0192bfd01b</th>\n",
              "      <th>41d0c04a20a77e8daaba6c585f083ca0ef57affc</th>\n",
              "      <th>a3dfc65c01d171a671cd615236b79c61810de354</th>\n",
              "      <th>06028150b43acb7e11121888ab434cdd5f679f1d</th>\n",
              "      <th>f7853f850bbb98d20bb2b4dd6380c071b5a1cb02</th>\n",
              "      <th>078a75dfda9e932f608fca0837610b97d11d3dd7</th>\n",
              "      <th>fc8fd87104388e513937ac13696b6f72af2add53</th>\n",
              "      <th>422765063ea5b0e5667a240e3b802041ab93b6e3</th>\n",
              "      <th>6cfaf9601b6e2587a321d3b803c20f9eb05fd452</th>\n",
              "      <th>35c0ef8cb2af11a81092979d55c4324b8101332f</th>\n",
              "      <th>764cbb8b20a46c411fe5807df9867b92c0935c08</th>\n",
              "      <th>78209eb1032640dd8f611e310c8619ab2c9cb071</th>\n",
              "      <th>1fc19e52666a95169b365f5ad1352d9369cb045a</th>\n",
              "      <th>14d3489b6de7d970babc8a951bf6ed5bd289dea0</th>\n",
              "      <th>6646a9365ace5164d2ef3dfb39b88a8a924568d3</th>\n",
              "      <th>a7ac17ed71cc594bed820bfbbca833f3f9972592</th>\n",
              "      <th>976e778d614b1e8e9efa8bc7eeaf552b7fe5ccb5</th>\n",
              "      <th>5749b4541786aea857ff1552dfac8cf292044c29</th>\n",
              "      <th>b1f4814d1a8c7885960118015e2c8f47fcdd0f4f</th>\n",
              "      <th>75df7c8888ddbf148407aa76c3d6b72406bc4c27</th>\n",
              "      <th>fc88f6a4f0351f8a2b34d256fcda35579d9e0ead</th>\n",
              "      <th>21f079033442b71f1a870ca0988fa9afcc7dc810</th>\n",
              "      <th>3f8321ce0dd6348fa440eb25acae2be996c3b2e7</th>\n",
              "      <th>9054af7ed2ceb6b130b4fd2a09f36b9b1f611f11</th>\n",
              "      <th>77de2f0f3005949e35e2e2aef7af1ddc5d164695</th>\n",
              "      <th>588f35babb83fab68a9c539567140ba5c17284c7</th>\n",
              "      <th>68962923d8621f6868a762b2344ce1ab8209ecdd</th>\n",
              "      <th>7abbb83f5b877ce5c1c9e835422365cd8bac3b0d</th>\n",
              "      <th>acb4abc93968ad51368a0351fa7fda5401598069</th>\n",
              "      <th>f20fed9f248d4352def744d9a8262b49042ee7f3</th>\n",
              "      <th>437d45cd7c45b040b95abdb483b02d601c761d7d</th>\n",
              "      <th>b083b8279418c48d60a05cff039b0536339a1e4d</th>\n",
              "      <th>fa0d8c168b62f11f34fdf314d81cebdf1ca9ebd0</th>\n",
              "      <th>955eb46cfdee70adab1de9ce909f0cb059fd29d2</th>\n",
              "      <th>48d751207622a3606df95184fd58b32347057af3</th>\n",
              "      <th>5238e869a2d0d2a851ba1f9484c874c57419148e</th>\n",
              "      <th>63781517942f8b63c2189c376cc0cf3db1d27ed1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1573705854189</td>\n",
              "      <td>5859</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-78</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-88</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1573705854189</td>\n",
              "      <td>174</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-78</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-88</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1573705856155</td>\n",
              "      <td>67</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-80</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-76</td>\n",
              "      <td>-85</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-90</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1573705862090</td>\n",
              "      <td>898</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-68</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1573705869915</td>\n",
              "      <td>720</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-999</td>\n",
              "      <td>-71</td>\n",
              "      <td>-999</td>\n",
              "      <td>-87</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>...</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-73</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-89</td>\n",
              "      <td>-70</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-91</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-74</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 996 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         wifi_ts  ...  63781517942f8b63c2189c376cc0cf3db1d27ed1\n",
              "0  1573705854189  ...                                      -999\n",
              "1  1573705854189  ...                                      -999\n",
              "2  1573705856155  ...                                      -999\n",
              "3  1573705862090  ...                                      -999\n",
              "4  1573705869915  ...                                      -999\n",
              "\n",
              "[5 rows x 996 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnDOQsyFny1q"
      },
      "source": [
        "# # Use when we need to consider timetamps\n",
        "\n",
        "# # get timestamp and sort by time\n",
        "# test_df[[\"site\", \"file\", \"timestamp\"]] = test_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
        "# test_df = test_df.drop(columns=[\"site_path_timestamp\", \"site\", \"file\"])\n",
        "# test_df[\"timestamp\"] = test_df[\"timestamp\"].astype(int)\n",
        "# # display(test_df.head())\n",
        "\n",
        "# # sort by time\n",
        "# train_df = train_df.sort_values(by=[\"file_id\", \"wifi_ts\"])\n",
        "# test_df = test_df.sort_values(by=[\"file_id\", \"timestamp\"])\n",
        "# # display(train_df.head(20))\n",
        "# # display(test_df.head(20))\n",
        "# # print(len(test_df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ePebfXPvPui"
      },
      "source": [
        "---\n",
        "## 1Conv + Transformer Implementation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNgXfNr2rnyk"
      },
      "source": [
        "class IndoorDataset(Dataset):\n",
        "    def __init__(self, data, flag=\"train\"):\n",
        "        self.flag = flag\n",
        "        self.data = data\n",
        "\n",
        "        all_cols = list(data.columns)\n",
        "        target_cols = [\"x\", \"y\", \"floor_int\"]\n",
        "        non_target_cols = [col for col in all_cols if col not in target_cols]\n",
        "        self.features = data[non_target_cols]\n",
        "\n",
        "        if self.flag == \"train\":\n",
        "            self.x = data.loc[:, \"x\"]\n",
        "            self.y = data.loc[:, \"y\"]\n",
        "            # self.f = data.loc[:, \"floor_int\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def feat_width(self):\n",
        "        return self.features.shape[1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        features = self.features.iloc[index, :]\n",
        "        features_out = torch.tensor(features.to_numpy())\n",
        "        if self.flag == \"train\":\n",
        "            x = self.x[index]\n",
        "            y = self.y[index]\n",
        "            # f = self.f[index]\n",
        "            x_out = torch.tensor(x)\n",
        "            y_out = torch.tensor(y)\n",
        "            # f_out = torch.tensor(f)\n",
        "            return features_out, x_out, y_out\n",
        "        else:\n",
        "            return features_out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZNpNu_psGuE",
        "outputId": "e2300808-53e2-41e2-832b-23c12f856cfd"
      },
      "source": [
        "# Create train and test Dataset\n",
        "train_ds = IndoorDataset(train_df)\n",
        "test_ds = IndoorDataset(test_df, flag=\"test\")\n",
        "\n",
        "one_train_ds = train_ds.__getitem__(1000)\n",
        "print(\"train ds len: \", train_ds.__len__())\n",
        "# print(\"train ds features: \", one_train_ds[0])\n",
        "print(\"train ds x: \", one_train_ds[1])\n",
        "print(\"train ds y: \", one_train_ds[2])\n",
        "# print(\"train ds f: \", one_train_ds[3])\n",
        "\n",
        "one_test_ds = test_ds.__getitem__(0)\n",
        "print(\"test ds len: \", test_ds.__len__())\n",
        "# print(\"test ds features: \", one_test_ds[0])\n",
        "# print(\"test ds: \", one_test_ds)\n",
        "\n",
        "print(train_ds.feat_width())\n",
        "print(test_ds.feat_width())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train ds len:  10027\n",
            "train ds x:  tensor(104.6150, dtype=torch.float64)\n",
            "train ds y:  tensor(83.2759, dtype=torch.float64)\n",
            "test ds len:  1223\n",
            "993\n",
            "993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDBm9wbv8JS"
      },
      "source": [
        "# Create Dataloader\n",
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51XdnWYay8SP"
      },
      "source": [
        "class Conv1dMaxPool(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, stride, padding):\n",
        "        super(Conv1dMaxPool, self).__init__()\n",
        "        self.conv_11 = nn.Conv1d(in_channels, 16, kernel_size, stride, padding)\n",
        "        self.conv_12 = nn.Conv1d(16, 16, kernel_size, stride, padding)\n",
        "        self.max_pool_1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_21 = nn.Conv1d(16, 32, kernel_size, stride, padding)\n",
        "        self.conv_22 = nn.Conv1d(32, 32, kernel_size, stride, padding)\n",
        "        self.max_pool_2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_31 = nn.Conv1d(32, 64, kernel_size, stride, padding)\n",
        "        self.conv_32 = nn.Conv1d(64, 64, kernel_size, stride, padding)\n",
        "        self.max_pool_3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        self.fc_encoder = nn.Linear(64*2, 512)\n",
        "        self.batch_norm = nn.BatchNorm1d(512)\n",
        "        self.out_encoder = nn.Linear(512, 64)\n",
        "\n",
        "    def forward(self, x, prints=False):\n",
        "        x = x.unsqueeze(0)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        if prints: print(\"before conv 1: \", x.shape)\n",
        "        x = F.relu(self.conv_11(x))\n",
        "        x = F.relu(self.conv_12(x))\n",
        "        x = self.max_pool_1(x)\n",
        "        if prints: print(\"after conv & max_pool 1: \", x.shape)\n",
        "\n",
        "        x = F.relu(self.conv_21(x))\n",
        "        x = F.relu(self.conv_22(x))\n",
        "        x = self.max_pool_2(x)\n",
        "        if prints: print(\"after conv & max_pool 2: \", x.shape)\n",
        "\n",
        "        x = F.relu(self.conv_31(x))\n",
        "        x = F.relu(self.conv_32(x))\n",
        "        x = self.max_pool_3(x)\n",
        "        if prints: print(\"after conv & max_pool 3: \", x.shape)\n",
        "\n",
        "        # if prints: print(\"checking reshaping: \", x[0])\n",
        "        x = x.view(x.size(0), -1) # flatten last two dimensions\n",
        "        # if prints: print(\"checking reshaping: \", x[0])\n",
        "        if prints: print(\"flatten last two dims: \", x.shape)\n",
        "        x = self.batch_norm(self.fc_encoder(x))\n",
        "        x = self.out_encoder(x)\n",
        "        if prints: print(\"final output: \", x.shape)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJPs3O4323xO",
        "outputId": "d6df3e44-3a6c-49fc-e38c-602b7bf67411"
      },
      "source": [
        "train_length = train_ds.__len__()\n",
        "train_width = train_ds.feat_width()\n",
        "\n",
        "model = Conv1dMaxPool(in_channels=1, kernel_size=3, stride=2, padding=1).to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# input_size = num of features, so length of columns\n",
        "# sequence_length = 5\n",
        "\n",
        "# Check if it works\n",
        "train_batch_sample = next(iter(train_dataloader))\n",
        "print(\"feature shape: \", train_batch_sample[0].shape)\n",
        "print(\"x shape: \", train_batch_sample[1].shape)\n",
        "print(\"y shape: \", train_batch_sample[2].shape)\n",
        "train_batch_sample = train_batch_sample[0]\n",
        "print(\"input shape: \", train_batch_sample.shape)\n",
        "outputs = model(train_batch_sample.float(), prints=True)\n",
        "print(\"final output shape: \", outputs.shape)\n",
        "print(\"final output: \", outputs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv1dMaxPool(\n",
            "  (conv_11): Conv1d(1, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_12): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_1): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv_21): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_22): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_2): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv_31): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (conv_32): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  (max_pool_3): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (fc_encoder): Linear(in_features=128, out_features=512, bias=True)\n",
            "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (out_encoder): Linear(in_features=512, out_features=64, bias=True)\n",
            ")\n",
            "feature shape:  torch.Size([32, 993])\n",
            "x shape:  torch.Size([32])\n",
            "y shape:  torch.Size([32])\n",
            "input shape:  torch.Size([32, 993])\n",
            "before conv 1:  torch.Size([32, 1, 993])\n",
            "after conv & max_pool 1:  torch.Size([32, 16, 125])\n",
            "after conv & max_pool 2:  torch.Size([32, 32, 16])\n",
            "after conv & max_pool 3:  torch.Size([32, 64, 2])\n",
            "flatten last two dims:  torch.Size([32, 128])\n",
            "final output:  torch.Size([32, 64])\n",
            "final output shape:  torch.Size([32, 64])\n",
            "final output:  tensor([[ 0.1078, -0.0362,  0.5025,  ..., -0.8683, -0.3129, -1.2062],\n",
            "        [ 0.0592, -0.0416,  0.4828,  ..., -0.8893, -0.3200, -1.1969],\n",
            "        [ 0.0535, -0.0487,  0.5132,  ..., -0.9322, -0.2926, -1.1361],\n",
            "        ...,\n",
            "        [-0.1729,  0.0475, -1.1347,  ...,  1.7404,  0.6982,  2.9563],\n",
            "        [-0.1741,  0.0363, -1.2327,  ...,  1.7963,  0.6760,  2.8840],\n",
            "        [-0.0559,  0.1489, -1.1209,  ...,  1.8438,  0.7098,  2.8047]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IREBqLnPERaz"
      },
      "source": [
        "# Transformer: -> conv1d output -> pe -> (embed thru TransformerEncoderLayer)\n",
        "# -> encoder -> decoder\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, feature_dim, batch_len):\n",
        "        super(PositionalEncoding, self).__init__()       \n",
        "        pe = torch.zeros(batch_len, feature_dim)\n",
        "        position = torch.arange(0, batch_len, dtype=torch.float)\n",
        "        div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * (-math.log(10000.0) / feature_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, feature_dim, batch_len, num_layers=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_dim, batch_len)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=2, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
        "        self.decoder = nn.Linear(feature_dim, 2)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, prints=False):\n",
        "        if prints: print(\"x dims: \", x.shape)\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(x):\n",
        "            mask = self._generate_square_subsequent_mask(len(x)).to(DEVICE)\n",
        "            if prints: print(\"mask shape\", mask.shape)\n",
        "            # print(\"mask 0\", mask[0])\n",
        "            # print(\"mask 1\", mask[1])\n",
        "            # print(\"mask -2\", mask[-2])\n",
        "            # print(\"mask -1\", mask[-1])\n",
        "            self.src_mask = mask\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "        if prints: print(\"x after pos_encoder: \", x.shape)\n",
        "        x = x.unsqueeze(1)\n",
        "        if prints: print(\"x after unsqueeze: \", x.shape)\n",
        "        output = self.transformer_encoder(x, self.src_mask)\n",
        "        if prints: print(\"x after transformer_encoder: \", x.shape)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrgkU-Zlvw0D",
        "outputId": "63bcd22e-346b-424d-e795-4f7f858d702a"
      },
      "source": [
        "# testing transformer block\n",
        "Conv = Conv1dMaxPool(in_channels=1, kernel_size=3, stride=2, padding=1).to(DEVICE)\n",
        "\n",
        "# Check if it works\n",
        "train_batch_sample = next(iter(train_dataloader))\n",
        "train_batch_sample = train_batch_sample[0]\n",
        "conv_outputs = Conv(train_batch_sample.float(), prints=True)\n",
        "print(\"final output shape: \", conv_outputs.shape)\n",
        "print(\"final output: \", conv_outputs[0])\n",
        "print(conv_outputs.shape[1])\n",
        "\n",
        "# check pe\n",
        "batch_len = conv_outputs.shape[0]\n",
        "feature_dim = conv_outputs.shape[1]\n",
        "pe = PositionalEncoding(feature_dim, batch_len)\n",
        "pe_out = pe(conv_outputs)\n",
        "print(pe_out[0])\n",
        "print(pe_out.shape)\n",
        "\n",
        "# check transformer\n",
        "ts = Transformer(feature_dim, batch_len, num_layers=1, dropout=0.1)\n",
        "ts_out = ts(conv_outputs, prints=True)\n",
        "print(ts_out[:3])\n",
        "print(ts_out.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before conv 1:  torch.Size([32, 1, 993])\n",
            "after conv & max_pool 1:  torch.Size([32, 16, 125])\n",
            "after conv & max_pool 2:  torch.Size([32, 32, 16])\n",
            "after conv & max_pool 3:  torch.Size([32, 64, 2])\n",
            "flatten last two dims:  torch.Size([32, 128])\n",
            "final output:  torch.Size([32, 64])\n",
            "final output shape:  torch.Size([32, 64])\n",
            "final output:  tensor([ 0.0498, -0.1006, -0.2215, -0.2394, -0.2636, -0.2973,  0.0730, -0.5185,\n",
            "         0.2821, -0.5098, -0.1908, -0.1029, -0.1398,  0.4668, -0.3321,  0.5823,\n",
            "        -0.2440,  0.1907,  0.4223,  0.0706, -0.5124,  0.0820,  0.2576, -0.2131,\n",
            "        -0.5327,  0.4582,  0.1800, -0.1212, -0.1160, -0.6453,  0.5437, -0.4335,\n",
            "         0.0090,  0.2374, -0.3075, -0.4912, -1.2667, -0.7817,  0.7282, -0.3841,\n",
            "         0.2393, -0.4029, -0.7974, -0.6590, -0.0578,  0.2426,  0.3448, -0.0834,\n",
            "         0.2931, -0.0273,  0.6298, -0.1748,  0.0064, -0.6345, -0.3252, -0.0627,\n",
            "        -0.3140,  0.2336,  0.4446,  0.0758, -0.1880, -0.3130, -0.2104, -0.2542],\n",
            "       grad_fn=<SelectBackward>)\n",
            "64\n",
            "tensor([ 0.0498,  0.8994,  0.4601,  0.4924,  0.6385,  0.1341,  1.0266, -0.2175,\n",
            "         1.2357, -0.2087,  0.7360,  0.2728,  0.7359,  0.9495,  0.4715,  1.1774,\n",
            "         0.4734,  0.8874,  1.0472,  0.8514,  0.0208,  0.9280,  0.7050,  0.6813,\n",
            "        -0.1623,  1.3870,  0.4834,  0.8317,  0.1304,  0.3239,  0.7424,  0.5466,\n",
            "         0.1683,  1.2247, -0.1803,  0.5007, -1.1657,  0.2131,  0.8082,  0.6126,\n",
            "         0.3025,  0.5951, -0.7476,  0.3397, -0.0187,  1.2419,  0.3755,  0.9161,\n",
            "         0.3171,  0.9724,  0.6485,  0.8250,  0.0210,  0.3654, -0.3138,  0.9372,\n",
            "        -0.3051,  1.2336,  0.4515,  1.0758, -0.1827,  0.6870, -0.2063,  0.7458],\n",
            "       grad_fn=<SelectBackward>)\n",
            "torch.Size([32, 64])\n",
            "x dims:  torch.Size([32, 64])\n",
            "mask shape torch.Size([32, 32])\n",
            "x after pos_encoder:  torch.Size([32, 64])\n",
            "x after unsqueeze:  torch.Size([32, 1, 64])\n",
            "x after transformer_encoder:  torch.Size([32, 1, 64])\n",
            "tensor([[[ 1.4465, -2.2568]],\n",
            "\n",
            "        [[-0.0826, -1.6855]],\n",
            "\n",
            "        [[ 1.0971, -1.7936]]], grad_fn=<SliceBackward>)\n",
            "torch.Size([32, 1, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wM_NRmwNpoyl",
        "outputId": "24171d51-adcd-4d55-d6e5-4e356cb08977"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # default lr: 5e-3\n",
        "\n",
        "data_dict ={}\n",
        "best_loss = 1000\n",
        "best_epoch = 0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    print(\"epoch: \", epoch)\n",
        "    losses = []\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        features = data[0].to(DEVICE).float()\n",
        "        # features = features.unsqueeze(0)\n",
        "        # print(\"imu shape: \", imu_features.shape)\n",
        "        x = data[1].to(DEVICE).float().unsqueeze(-1)\n",
        "        # print(\"x shape: \", x.shape)\n",
        "        y = data[2].to(DEVICE).float().unsqueeze(-1)\n",
        "        # print(\"y shape: \", y.shape)\n",
        "\n",
        "        # output = model(features, prints=False)\n",
        "        # print(\"output\", output)\n",
        "        # print(\"output shape after concat: \", output.shape)\n",
        "        conv_outputs = Conv(features)\n",
        "        ts_out = ts(conv_outputs)\n",
        "        # print(\"conv output\", conv_outputs[0])\n",
        "\n",
        "        label = torch.cat([x, y], dim=-1)\n",
        "        # print(\"label\", label)\n",
        "        # print(\"label shape after concat: \", label.shape)\n",
        "        loss = criterion(ts_out, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        print(f\"loss:{np.mean(losses)} at iteration {i}\")\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "loss:10014.0908203125 at iteration 0\n",
            "loss:8363.648681640625 at iteration 1\n",
            "loss:8075.400065104167 at iteration 2\n",
            "loss:7862.7867431640625 at iteration 3\n",
            "loss:8248.45908203125 at iteration 4\n",
            "loss:9027.296956380209 at iteration 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss:9699.308384486607 at iteration 6\n",
            "loss:9935.033996582031 at iteration 7\n",
            "loss:9984.43896484375 at iteration 8\n",
            "loss:9617.7822265625 at iteration 9\n",
            "loss:9316.70556640625 at iteration 10\n",
            "loss:9208.313680013021 at iteration 11\n",
            "loss:9523.752817007211 at iteration 12\n",
            "loss:9277.057931082589 at iteration 13\n",
            "loss:9336.962337239584 at iteration 14\n",
            "loss:9705.54995727539 at iteration 15\n",
            "loss:10019.485322840073 at iteration 16\n",
            "loss:9808.179606119791 at iteration 17\n",
            "loss:9895.936240748355 at iteration 18\n",
            "loss:10023.341625976562 at iteration 19\n",
            "loss:9846.19482421875 at iteration 20\n",
            "loss:9605.258944424715 at iteration 21\n",
            "loss:9406.540803328804 at iteration 22\n",
            "loss:9215.71826171875 at iteration 23\n",
            "loss:9141.21515625 at iteration 24\n",
            "loss:9025.837289663461 at iteration 25\n",
            "loss:8942.295337818286 at iteration 26\n",
            "loss:8717.77721296038 at iteration 27\n",
            "loss:8745.11467032597 at iteration 28\n",
            "loss:8733.35020345052 at iteration 29\n",
            "loss:8743.263805758568 at iteration 30\n",
            "loss:8745.880867004395 at iteration 31\n",
            "loss:8721.973817767519 at iteration 32\n",
            "loss:8705.82872099035 at iteration 33\n",
            "loss:8693.476081194196 at iteration 34\n",
            "loss:8587.533508300781 at iteration 35\n",
            "loss:8625.808917071368 at iteration 36\n",
            "loss:8753.61105186061 at iteration 37\n",
            "loss:8630.514241536459 at iteration 38\n",
            "loss:8542.496966552735 at iteration 39\n",
            "loss:8519.298548256478 at iteration 40\n",
            "loss:8469.693795340401 at iteration 41\n",
            "loss:8394.854622774346 at iteration 42\n",
            "loss:8322.547757235441 at iteration 43\n",
            "loss:8274.793972439236 at iteration 44\n",
            "loss:8359.457928201427 at iteration 45\n",
            "loss:8464.205872880651 at iteration 46\n",
            "loss:8551.828089396158 at iteration 47\n",
            "loss:8653.249785754146 at iteration 48\n",
            "loss:8766.914262695313 at iteration 49\n",
            "loss:8933.09099743413 at iteration 50\n",
            "loss:9013.065716083232 at iteration 51\n",
            "loss:9086.886115308078 at iteration 52\n",
            "loss:9224.836864330151 at iteration 53\n",
            "loss:9354.387291370738 at iteration 54\n",
            "loss:9299.627550397601 at iteration 55\n",
            "loss:9192.219251130757 at iteration 56\n",
            "loss:9075.868669181034 at iteration 57\n",
            "loss:9191.352803893009 at iteration 58\n",
            "loss:9285.02421875 at iteration 59\n",
            "loss:9200.748190957991 at iteration 60\n",
            "loss:9128.20910250756 at iteration 61\n",
            "loss:9080.00502232143 at iteration 62\n",
            "loss:9053.809692382812 at iteration 63\n",
            "loss:9012.027178485578 at iteration 64\n",
            "loss:8966.853641394413 at iteration 65\n",
            "loss:8939.466753148321 at iteration 66\n",
            "loss:8972.536434397978 at iteration 67\n",
            "loss:9012.615305140398 at iteration 68\n",
            "loss:9031.364773995536 at iteration 69\n",
            "loss:8966.567320917693 at iteration 70\n",
            "loss:8934.816155327691 at iteration 71\n",
            "loss:8914.830900845462 at iteration 72\n",
            "loss:8903.301487278293 at iteration 73\n",
            "loss:8812.510481770832 at iteration 74\n",
            "loss:8733.287793611225 at iteration 75\n",
            "loss:8719.067817192574 at iteration 76\n",
            "loss:8753.191697340746 at iteration 77\n",
            "loss:8739.76361933841 at iteration 78\n",
            "loss:8751.933102416991 at iteration 79\n",
            "loss:8760.40658456308 at iteration 80\n",
            "loss:8777.02338688548 at iteration 81\n",
            "loss:8805.819685970444 at iteration 82\n",
            "loss:8857.615324474516 at iteration 83\n",
            "loss:8814.997061695773 at iteration 84\n",
            "loss:8864.951254201489 at iteration 85\n",
            "loss:8918.946830661818 at iteration 86\n",
            "loss:8960.809861616655 at iteration 87\n",
            "loss:8963.726587188377 at iteration 88\n",
            "loss:8906.444240993924 at iteration 89\n",
            "loss:8883.098055996737 at iteration 90\n",
            "loss:8877.475524902344 at iteration 91\n",
            "loss:8845.445315125167 at iteration 92\n",
            "loss:8810.465589158079 at iteration 93\n",
            "loss:8800.96603361431 at iteration 94\n",
            "loss:8811.62257639567 at iteration 95\n",
            "loss:8848.229731294297 at iteration 96\n",
            "loss:8906.457247987086 at iteration 97\n",
            "loss:8921.602112432924 at iteration 98\n",
            "loss:8935.138386230468 at iteration 99\n",
            "loss:8887.48553768951 at iteration 100\n",
            "loss:8825.004334692861 at iteration 101\n",
            "loss:8783.555415181281 at iteration 102\n",
            "loss:8738.206303523137 at iteration 103\n",
            "loss:8727.666596912202 at iteration 104\n",
            "loss:8743.011345629422 at iteration 105\n",
            "loss:8761.482691114194 at iteration 106\n",
            "loss:8783.815795898438 at iteration 107\n",
            "loss:8808.642107762327 at iteration 108\n",
            "loss:8847.958722478694 at iteration 109\n",
            "loss:8936.338475682714 at iteration 110\n",
            "loss:8947.169307163784 at iteration 111\n",
            "loss:8956.597142042312 at iteration 112\n",
            "loss:8941.517522443804 at iteration 113\n",
            "loss:8924.738654891304 at iteration 114\n",
            "loss:8907.304140288254 at iteration 115\n",
            "loss:8902.692499666133 at iteration 116\n",
            "loss:8894.706274000266 at iteration 117\n",
            "loss:8885.60010996586 at iteration 118\n",
            "loss:8922.519645182292 at iteration 119\n",
            "loss:8920.551225142046 at iteration 120\n",
            "loss:8965.368796426741 at iteration 121\n",
            "loss:9034.076132177337 at iteration 122\n",
            "loss:9047.926513671875 at iteration 123\n",
            "loss:9070.595734375 at iteration 124\n",
            "loss:9117.626379588293 at iteration 125\n",
            "loss:9108.676392562746 at iteration 126\n",
            "loss:9100.304691314697 at iteration 127\n",
            "loss:9087.374068859011 at iteration 128\n",
            "loss:9080.684675480768 at iteration 129\n",
            "loss:9087.517995586832 at iteration 130\n",
            "loss:9100.719948508522 at iteration 131\n",
            "loss:9111.415516329887 at iteration 132\n",
            "loss:9122.4931640625 at iteration 133\n",
            "loss:9086.52982494213 at iteration 134\n",
            "loss:9059.702184340533 at iteration 135\n",
            "loss:9034.949938697537 at iteration 136\n",
            "loss:9027.898005831068 at iteration 137\n",
            "loss:9002.457761915468 at iteration 138\n",
            "loss:8972.239732142858 at iteration 139\n",
            "loss:8986.297761524822 at iteration 140\n",
            "loss:8992.68985200264 at iteration 141\n",
            "loss:8991.101972246504 at iteration 142\n",
            "loss:8997.021491156684 at iteration 143\n",
            "loss:9028.42491244612 at iteration 144\n",
            "loss:9067.713024400686 at iteration 145\n",
            "loss:9069.551644876701 at iteration 146\n",
            "loss:9050.87660341005 at iteration 147\n",
            "loss:9027.831251310821 at iteration 148\n",
            "loss:8988.589332682292 at iteration 149\n",
            "loss:8952.32539256519 at iteration 150\n",
            "loss:8914.56163908306 at iteration 151\n",
            "loss:8892.978397543913 at iteration 152\n",
            "loss:8861.44896826806 at iteration 153\n",
            "loss:8833.025374873992 at iteration 154\n",
            "loss:8822.645442082332 at iteration 155\n",
            "loss:8801.21625261246 at iteration 156\n",
            "loss:8764.058535032635 at iteration 157\n",
            "loss:8728.099873476809 at iteration 158\n",
            "loss:8692.658442687989 at iteration 159\n",
            "loss:8657.83049058618 at iteration 160\n",
            "loss:8645.923864293982 at iteration 161\n",
            "loss:8618.148698116373 at iteration 162\n",
            "loss:8589.560201505335 at iteration 163\n",
            "loss:8595.047768702652 at iteration 164\n",
            "loss:8571.493846479669 at iteration 165\n",
            "loss:8568.354527273577 at iteration 166\n",
            "loss:8548.266706194196 at iteration 167\n",
            "loss:8515.198734802607 at iteration 168\n",
            "loss:8480.752664005055 at iteration 169\n",
            "loss:8476.331852784631 at iteration 170\n",
            "loss:8478.07366659475 at iteration 171\n",
            "loss:8444.642785573971 at iteration 172\n",
            "loss:8404.067704124012 at iteration 173\n",
            "loss:8412.16351702009 at iteration 174\n",
            "loss:8419.507824984465 at iteration 175\n",
            "loss:8426.385809774452 at iteration 176\n",
            "loss:8428.95000466336 at iteration 177\n",
            "loss:8413.28133592659 at iteration 178\n",
            "loss:8395.992609320747 at iteration 179\n",
            "loss:8378.027207516834 at iteration 180\n",
            "loss:8356.118414910285 at iteration 181\n",
            "loss:8334.829767279287 at iteration 182\n",
            "loss:8312.29205720321 at iteration 183\n",
            "loss:8309.315855943834 at iteration 184\n",
            "loss:8287.442504882812 at iteration 185\n",
            "loss:8268.20186774106 at iteration 186\n",
            "loss:8258.542809019698 at iteration 187\n",
            "loss:8277.82365167204 at iteration 188\n",
            "loss:8269.661686626234 at iteration 189\n",
            "loss:8253.762351470468 at iteration 190\n",
            "loss:8249.588329315186 at iteration 191\n",
            "loss:8239.101945788132 at iteration 192\n",
            "loss:8234.79158318903 at iteration 193\n",
            "loss:8227.102845803285 at iteration 194\n",
            "loss:8201.339973294005 at iteration 195\n",
            "loss:8174.3681231658475 at iteration 196\n",
            "loss:8170.814925376815 at iteration 197\n",
            "loss:8164.607261159312 at iteration 198\n",
            "loss:8153.801488037109 at iteration 199\n",
            "loss:8140.101019560401 at iteration 200\n",
            "loss:8126.676178884978 at iteration 201\n",
            "loss:8130.414958484067 at iteration 202\n",
            "loss:8137.305799297258 at iteration 203\n",
            "loss:8140.812505954649 at iteration 204\n",
            "loss:8137.360743846708 at iteration 205\n",
            "loss:8122.503213928518 at iteration 206\n",
            "loss:8104.516501793494 at iteration 207\n",
            "loss:8091.177135120739 at iteration 208\n",
            "loss:8080.930118815104 at iteration 209\n",
            "loss:8062.1995924818575 at iteration 210\n",
            "loss:8053.769419544148 at iteration 211\n",
            "loss:8051.329547434346 at iteration 212\n",
            "loss:8047.433807087836 at iteration 213\n",
            "loss:8033.718907839753 at iteration 214\n",
            "loss:8024.368330213759 at iteration 215\n",
            "loss:8018.8029762654805 at iteration 216\n",
            "loss:8014.739310448323 at iteration 217\n",
            "loss:8005.637243819563 at iteration 218\n",
            "loss:7995.578297008167 at iteration 219\n",
            "loss:7996.544156311864 at iteration 220\n",
            "loss:7987.922599723747 at iteration 221\n",
            "loss:7996.252398709011 at iteration 222\n",
            "loss:7993.700434003557 at iteration 223\n",
            "loss:7980.243642578125 at iteration 224\n",
            "loss:7973.268611941718 at iteration 225\n",
            "loss:7972.153738685642 at iteration 226\n",
            "loss:7970.018230237459 at iteration 227\n",
            "loss:7969.330650629435 at iteration 228\n",
            "loss:7966.475693147079 at iteration 229\n",
            "loss:7965.457687576096 at iteration 230\n",
            "loss:7963.485710407125 at iteration 231\n",
            "loss:7957.5028615796 at iteration 232\n",
            "loss:7953.766600519164 at iteration 233\n",
            "loss:7952.0159273188165 at iteration 234\n",
            "loss:7937.886167364605 at iteration 235\n",
            "loss:7941.989780088014 at iteration 236\n",
            "loss:7936.569548278296 at iteration 237\n",
            "loss:7926.53146145234 at iteration 238\n",
            "loss:7914.27014058431 at iteration 239\n",
            "loss:7898.925285877529 at iteration 240\n",
            "loss:7884.091323726433 at iteration 241\n",
            "loss:7869.798178088027 at iteration 242\n",
            "loss:7857.68559590324 at iteration 243\n",
            "loss:7854.169946787309 at iteration 244\n",
            "loss:7864.322945447472 at iteration 245\n",
            "loss:7873.876404549911 at iteration 246\n",
            "loss:7881.421734225365 at iteration 247\n",
            "loss:7878.2953817222015 at iteration 248\n",
            "loss:7885.530860351562 at iteration 249\n",
            "loss:7893.064317923618 at iteration 250\n",
            "loss:7898.919155544705 at iteration 251\n",
            "loss:7902.73705861691 at iteration 252\n",
            "loss:7903.024862935224 at iteration 253\n",
            "loss:7900.575050742954 at iteration 254\n",
            "loss:7901.737748146057 at iteration 255\n",
            "loss:7899.760984428198 at iteration 256\n",
            "loss:7885.921240045119 at iteration 257\n",
            "loss:7876.9567644863055 at iteration 258\n",
            "loss:7875.529134427584 at iteration 259\n",
            "loss:7862.352627925946 at iteration 260\n",
            "loss:7857.842847052421 at iteration 261\n",
            "loss:7846.343747215126 at iteration 262\n",
            "loss:7828.65200620709 at iteration 263\n",
            "loss:7816.934033203125 at iteration 264\n",
            "loss:7817.5816081341045 at iteration 265\n",
            "loss:7800.476224177785 at iteration 266\n",
            "loss:7796.365074043843 at iteration 267\n",
            "loss:7797.423058492101 at iteration 268\n",
            "loss:7800.654286024305 at iteration 269\n",
            "loss:7802.033595912131 at iteration 270\n",
            "loss:7813.027573529412 at iteration 271\n",
            "loss:7820.553503462683 at iteration 272\n",
            "loss:7809.592987283303 at iteration 273\n",
            "loss:7810.008061079546 at iteration 274\n",
            "loss:7816.994735054348 at iteration 275\n",
            "loss:7825.358976618908 at iteration 276\n",
            "loss:7820.178208604991 at iteration 277\n",
            "loss:7818.207265765009 at iteration 278\n",
            "loss:7813.372692871094 at iteration 279\n",
            "loss:7809.366671416259 at iteration 280\n",
            "loss:7793.6728550254875 at iteration 281\n",
            "loss:7780.164443807973 at iteration 282\n",
            "loss:7764.590115399428 at iteration 283\n",
            "loss:7757.180823396381 at iteration 284\n",
            "loss:7743.253484552557 at iteration 285\n",
            "loss:7733.7481897865855 at iteration 286\n",
            "loss:7720.040849473741 at iteration 287\n",
            "loss:7706.829437783846 at iteration 288\n",
            "loss:7698.248634496228 at iteration 289\n",
            "loss:7687.322639806164 at iteration 290\n",
            "loss:7673.7593642979455 at iteration 291\n",
            "loss:7657.199618707338 at iteration 292\n",
            "loss:7642.373049366231 at iteration 293\n",
            "loss:7636.438606494969 at iteration 294\n",
            "loss:7625.8157769280515 at iteration 295\n",
            "loss:7620.135738077389 at iteration 296\n",
            "loss:7621.753114021864 at iteration 297\n",
            "loss:7620.362377358121 at iteration 298\n",
            "loss:7618.182642415364 at iteration 299\n",
            "loss:7609.137408994757 at iteration 300\n",
            "loss:7594.285866844733 at iteration 301\n",
            "loss:7581.991357583024 at iteration 302\n",
            "loss:7567.63835144043 at iteration 303\n",
            "loss:7555.515346439549 at iteration 304\n",
            "loss:7550.645694508272 at iteration 305\n",
            "loss:7540.943477071458 at iteration 306\n",
            "loss:7540.404360288149 at iteration 307\n",
            "loss:7541.570051767294 at iteration 308\n",
            "loss:7536.175346522177 at iteration 309\n",
            "loss:7534.608850607918 at iteration 310\n",
            "loss:7535.440736428285 at iteration 311\n",
            "loss:7529.498647476537 at iteration 312\n",
            "loss:7515.030589109773 at iteration 313\n",
            "epoch:  1\n",
            "loss:9980.544921875 at iteration 0\n",
            "loss:8354.127685546875 at iteration 1\n",
            "loss:8075.656412760417 at iteration 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([11, 2])) that is different to the input size (torch.Size([11, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss:7865.8533935546875 at iteration 3\n",
            "loss:8258.80087890625 at iteration 4\n",
            "loss:9032.594970703125 at iteration 5\n",
            "loss:9712.60302734375 at iteration 6\n",
            "loss:9948.300598144531 at iteration 7\n",
            "loss:9993.910210503473 at iteration 8\n",
            "loss:9626.846337890625 at iteration 9\n",
            "loss:9324.724343039772 at iteration 10\n",
            "loss:9215.372884114584 at iteration 11\n",
            "loss:9532.164663461539 at iteration 12\n",
            "loss:9284.55747767857 at iteration 13\n",
            "loss:9342.020442708334 at iteration 14\n",
            "loss:9709.603271484375 at iteration 15\n",
            "loss:10021.672104779413 at iteration 16\n",
            "loss:9811.208957248264 at iteration 17\n",
            "loss:9899.420718544408 at iteration 18\n",
            "loss:10026.908764648437 at iteration 19\n",
            "loss:9848.811453683036 at iteration 20\n",
            "loss:9607.801624644886 at iteration 21\n",
            "loss:9408.95509935462 at iteration 22\n",
            "loss:9219.537190755209 at iteration 23\n",
            "loss:9145.4453125 at iteration 24\n",
            "loss:9029.725717397836 at iteration 25\n",
            "loss:8945.824417679398 at iteration 26\n",
            "loss:8721.618103027344 at iteration 27\n",
            "loss:8748.688922750538 at iteration 28\n",
            "loss:8737.637573242188 at iteration 29\n",
            "loss:8748.365494266633 at iteration 30\n",
            "loss:8751.425727844238 at iteration 31\n",
            "loss:8727.002374822443 at iteration 32\n",
            "loss:8710.512329101562 at iteration 33\n",
            "loss:8697.48937639509 at iteration 34\n",
            "loss:8592.301289876303 at iteration 35\n",
            "loss:8630.749940614443 at iteration 36\n",
            "loss:8758.681865491366 at iteration 37\n",
            "loss:8635.769456129809 at iteration 38\n",
            "loss:8547.26669921875 at iteration 39\n",
            "loss:8523.184451219513 at iteration 40\n",
            "loss:8473.451997302827 at iteration 41\n",
            "loss:8398.24859193314 at iteration 42\n",
            "loss:8325.623424183239 at iteration 43\n",
            "loss:8278.009830729166 at iteration 44\n",
            "loss:8362.931852921196 at iteration 45\n",
            "loss:8467.130526928191 at iteration 46\n",
            "loss:8554.743428548178 at iteration 47\n",
            "loss:8656.061423788266 at iteration 48\n",
            "loss:8770.68009765625 at iteration 49\n",
            "loss:8936.658413756128 at iteration 50\n",
            "loss:9016.267409104566 at iteration 51\n",
            "loss:9090.348301149765 at iteration 52\n",
            "loss:9228.69359447338 at iteration 53\n",
            "loss:9358.822780539773 at iteration 54\n",
            "loss:9303.558175223214 at iteration 55\n",
            "loss:9196.35858689693 at iteration 56\n",
            "loss:9080.266239560884 at iteration 57\n",
            "loss:9195.720843816207 at iteration 58\n",
            "loss:9288.756404622396 at iteration 59\n",
            "loss:9204.535740586578 at iteration 60\n",
            "loss:9131.969947076614 at iteration 61\n",
            "loss:9083.958255828373 at iteration 62\n",
            "loss:9057.214836120605 at iteration 63\n",
            "loss:9015.75410907452 at iteration 64\n",
            "loss:8970.324240944603 at iteration 65\n",
            "loss:8942.548587628266 at iteration 66\n",
            "loss:8976.437306123622 at iteration 67\n",
            "loss:9016.316738847374 at iteration 68\n",
            "loss:9034.624909319196 at iteration 69\n",
            "loss:8969.714224801937 at iteration 70\n",
            "loss:8937.738945855035 at iteration 71\n",
            "loss:8917.849034139555 at iteration 72\n",
            "loss:8906.371516047297 at iteration 73\n",
            "loss:8815.297731119792 at iteration 74\n",
            "loss:8736.114868164062 at iteration 75\n",
            "loss:8722.088137936282 at iteration 76\n",
            "loss:8756.22079702524 at iteration 77\n",
            "loss:8742.760488775712 at iteration 78\n",
            "loss:8754.899932861328 at iteration 79\n",
            "loss:8763.574333285109 at iteration 80\n",
            "loss:8780.694842082698 at iteration 81\n",
            "loss:8809.47213267131 at iteration 82\n",
            "loss:8861.019943963915 at iteration 83\n",
            "loss:8817.997897518382 at iteration 84\n",
            "loss:8868.226165061773 at iteration 85\n",
            "loss:8921.650761045259 at iteration 86\n",
            "loss:8962.965387517756 at iteration 87\n",
            "loss:8965.8827247191 at iteration 88\n",
            "loss:8908.57099609375 at iteration 89\n",
            "loss:8885.222833319025 at iteration 90\n",
            "loss:8879.88204292629 at iteration 91\n",
            "loss:8848.163836735552 at iteration 92\n",
            "loss:8813.343318858046 at iteration 93\n",
            "loss:8803.887535978618 at iteration 94\n",
            "loss:8814.448928833008 at iteration 95\n",
            "loss:8851.015146786405 at iteration 96\n",
            "loss:8909.096415617028 at iteration 97\n",
            "loss:8924.242961845013 at iteration 98\n",
            "loss:8937.368676757813 at iteration 99\n",
            "loss:8889.352800123763 at iteration 100\n",
            "loss:8826.90663057215 at iteration 101\n",
            "loss:8785.330568776548 at iteration 102\n",
            "loss:8739.928818922777 at iteration 103\n",
            "loss:8729.130296688989 at iteration 104\n",
            "loss:8744.477488391803 at iteration 105\n",
            "loss:8762.609379563377 at iteration 106\n",
            "loss:8784.873340747974 at iteration 107\n",
            "loss:8809.696123315654 at iteration 108\n",
            "loss:8849.163773970171 at iteration 109\n",
            "loss:8937.185824007602 at iteration 110\n",
            "loss:8947.967315673828 at iteration 111\n",
            "loss:8957.451219406803 at iteration 112\n",
            "loss:8942.320603755483 at iteration 113\n",
            "loss:8925.654424252718 at iteration 114\n",
            "loss:8908.208281418372 at iteration 115\n",
            "loss:8903.557821681357 at iteration 116\n",
            "loss:8895.79300599179 at iteration 117\n",
            "loss:8886.705755153624 at iteration 118\n",
            "loss:8923.66504313151 at iteration 119\n",
            "loss:8921.721296326188 at iteration 120\n",
            "loss:8966.574823098104 at iteration 121\n",
            "loss:9035.602812976373 at iteration 122\n",
            "loss:9049.625972624748 at iteration 123\n",
            "loss:9072.00273046875 at iteration 124\n",
            "loss:9118.881708054316 at iteration 125\n",
            "loss:9109.957481083908 at iteration 126\n",
            "loss:9101.395919799805 at iteration 127\n",
            "loss:9088.706031976744 at iteration 128\n",
            "loss:9082.15987079327 at iteration 129\n",
            "loss:9088.765267175573 at iteration 130\n",
            "loss:9102.061316287878 at iteration 131\n",
            "loss:9112.976048519737 at iteration 132\n",
            "loss:9124.24513176306 at iteration 133\n",
            "loss:9088.210282841435 at iteration 134\n",
            "loss:9061.345552332261 at iteration 135\n",
            "loss:9036.622373260721 at iteration 136\n",
            "loss:9029.416355298914 at iteration 137\n",
            "loss:9004.11062556205 at iteration 138\n",
            "loss:8973.971177455358 at iteration 139\n",
            "loss:8987.958818151596 at iteration 140\n",
            "loss:8994.318923305458 at iteration 141\n",
            "loss:8992.73386964598 at iteration 142\n",
            "loss:8998.651082356771 at iteration 143\n",
            "loss:9030.097575431035 at iteration 144\n",
            "loss:9069.598960562927 at iteration 145\n",
            "loss:9071.45079985119 at iteration 146\n",
            "loss:9052.95290065456 at iteration 147\n",
            "loss:9030.05112861787 at iteration 148\n",
            "loss:8990.836056315104 at iteration 149\n",
            "loss:8954.56950570416 at iteration 150\n",
            "loss:8916.848814311781 at iteration 151\n",
            "loss:8895.360582937603 at iteration 152\n",
            "loss:8863.672050971489 at iteration 153\n",
            "loss:8835.188953818044 at iteration 154\n",
            "loss:8824.687457744893 at iteration 155\n",
            "loss:8803.205036139032 at iteration 156\n",
            "loss:8766.013690417325 at iteration 157\n",
            "loss:8729.977907576651 at iteration 158\n",
            "loss:8694.552569580079 at iteration 159\n",
            "loss:8659.767720666732 at iteration 160\n",
            "loss:8647.855746045525 at iteration 161\n",
            "loss:8620.218606211656 at iteration 162\n",
            "loss:8591.584681068978 at iteration 163\n",
            "loss:8597.175864109848 at iteration 164\n",
            "loss:8573.476836055159 at iteration 165\n",
            "loss:8570.238857246444 at iteration 166\n",
            "loss:8550.202616373697 at iteration 167\n",
            "loss:8517.009234005178 at iteration 168\n",
            "loss:8482.66183794807 at iteration 169\n",
            "loss:8478.298940915114 at iteration 170\n",
            "loss:8479.823098825855 at iteration 171\n",
            "loss:8446.34313753161 at iteration 172\n",
            "loss:8405.761193286413 at iteration 173\n",
            "loss:8413.906386021205 at iteration 174\n",
            "loss:8421.380633961071 at iteration 175\n",
            "loss:8428.23577294646 at iteration 176\n",
            "loss:8430.87426140603 at iteration 177\n",
            "loss:8415.186705520033 at iteration 178\n",
            "loss:8397.781667073568 at iteration 179\n",
            "loss:8379.715261216981 at iteration 180\n",
            "loss:8357.762023925781 at iteration 181\n",
            "loss:8336.515426885886 at iteration 182\n",
            "loss:8313.928436942722 at iteration 183\n",
            "loss:8310.892873073268 at iteration 184\n",
            "loss:8288.878459315147 at iteration 185\n",
            "loss:8269.635618811622 at iteration 186\n",
            "loss:8259.796004275058 at iteration 187\n",
            "loss:8279.115132972676 at iteration 188\n",
            "loss:8270.961772075452 at iteration 189\n",
            "loss:8255.062703876596 at iteration 190\n",
            "loss:8250.871767044067 at iteration 191\n",
            "loss:8240.424626958185 at iteration 192\n",
            "loss:8236.150999088877 at iteration 193\n",
            "loss:8228.488756385217 at iteration 194\n",
            "loss:8202.706882398956 at iteration 195\n",
            "loss:8175.704804861001 at iteration 196\n",
            "loss:8172.121300283105 at iteration 197\n",
            "loss:8166.017287977976 at iteration 198\n",
            "loss:8155.224555053711 at iteration 199\n",
            "loss:8141.52663379878 at iteration 200\n",
            "loss:8128.006569437462 at iteration 201\n",
            "loss:8131.869452716094 at iteration 202\n",
            "loss:8138.8355443617875 at iteration 203\n",
            "loss:8142.23676579173 at iteration 204\n",
            "loss:8138.861161611612 at iteration 205\n",
            "loss:8124.0417462777405 at iteration 206\n",
            "loss:8106.1320231511045 at iteration 207\n",
            "loss:8092.86332739141 at iteration 208\n",
            "loss:8082.569925944011 at iteration 209\n",
            "loss:8063.880817142143 at iteration 210\n",
            "loss:8055.453502151201 at iteration 211\n",
            "loss:8053.0433876861425 at iteration 212\n",
            "loss:8049.260054828965 at iteration 213\n",
            "loss:8035.449611078307 at iteration 214\n",
            "loss:8026.106946591978 at iteration 215\n",
            "loss:8020.666843115459 at iteration 216\n",
            "loss:8016.502082474734 at iteration 217\n",
            "loss:8007.450584934182 at iteration 218\n",
            "loss:7997.315655517578 at iteration 219\n",
            "loss:7998.172844381894 at iteration 220\n",
            "loss:7989.566599253061 at iteration 221\n",
            "loss:7997.862422378608 at iteration 222\n",
            "loss:7995.332889556885 at iteration 223\n",
            "loss:7981.804005533854 at iteration 224\n",
            "loss:7974.848632272366 at iteration 225\n",
            "loss:7973.782290555307 at iteration 226\n",
            "loss:7971.604971367016 at iteration 227\n",
            "loss:7970.851250660992 at iteration 228\n",
            "loss:7968.068129564368 at iteration 229\n",
            "loss:7967.06159054975 at iteration 230\n",
            "loss:7965.128665792531 at iteration 231\n",
            "loss:7959.137841482531 at iteration 232\n",
            "loss:7955.551187629374 at iteration 233\n",
            "loss:7953.935627389462 at iteration 234\n",
            "loss:7939.757367150258 at iteration 235\n",
            "loss:7943.8252174602785 at iteration 236\n",
            "loss:7938.2721239618895 at iteration 237\n",
            "loss:7928.327295024026 at iteration 238\n",
            "loss:7916.001734924316 at iteration 239\n",
            "loss:7900.675345139879 at iteration 240\n",
            "loss:7885.766405342039 at iteration 241\n",
            "loss:7871.50200285735 at iteration 242\n",
            "loss:7859.487508304784 at iteration 243\n",
            "loss:7855.93889459104 at iteration 244\n",
            "loss:7866.176253155964 at iteration 245\n",
            "loss:7875.727570197843 at iteration 246\n",
            "loss:7883.116533340946 at iteration 247\n",
            "loss:7880.133272837444 at iteration 248\n",
            "loss:7887.293513183594 at iteration 249\n",
            "loss:7894.856617961746 at iteration 250\n",
            "loss:7900.606156122117 at iteration 251\n",
            "loss:7904.6284483657055 at iteration 252\n",
            "loss:7904.782083826741 at iteration 253\n",
            "loss:7902.3170606426165 at iteration 254\n",
            "loss:7903.456641674042 at iteration 255\n",
            "loss:7901.425676279031 at iteration 256\n",
            "loss:7887.537773191467 at iteration 257\n",
            "loss:7878.484708218961 at iteration 258\n",
            "loss:7876.897428541917 at iteration 259\n",
            "loss:7863.7980812043525 at iteration 260\n",
            "loss:7859.387553207747 at iteration 261\n",
            "loss:7847.877741708502 at iteration 262\n",
            "loss:7830.249314279267 at iteration 263\n",
            "loss:7818.439497807341 at iteration 264\n",
            "loss:7819.248102403225 at iteration 265\n",
            "loss:7802.125506111745 at iteration 266\n",
            "loss:7797.958788971403 at iteration 267\n",
            "loss:7798.994846273089 at iteration 268\n",
            "loss:7802.22958396629 at iteration 269\n",
            "loss:7803.6490838870795 at iteration 270\n",
            "loss:7814.569699904498 at iteration 271\n",
            "loss:7822.135657677283 at iteration 272\n",
            "loss:7811.099204404511 at iteration 273\n",
            "loss:7811.577458718039 at iteration 274\n",
            "loss:7818.614489126897 at iteration 275\n",
            "loss:7827.107077698415 at iteration 276\n",
            "loss:7821.968287625758 at iteration 277\n",
            "loss:7820.05614140555 at iteration 278\n",
            "loss:7815.165322875977 at iteration 279\n",
            "loss:7811.0834730698125 at iteration 280\n",
            "loss:7795.424396487838 at iteration 281\n",
            "loss:7781.886102359624 at iteration 282\n",
            "loss:7766.300894293987 at iteration 283\n",
            "loss:7758.865958658854 at iteration 284\n",
            "loss:7744.896903938346 at iteration 285\n",
            "loss:7735.378609793527 at iteration 286\n",
            "loss:7721.635045793321 at iteration 287\n",
            "loss:7708.378912585829 at iteration 288\n",
            "loss:7699.713733752021 at iteration 289\n",
            "loss:7688.810655521773 at iteration 290\n",
            "loss:7675.251671778013 at iteration 291\n",
            "loss:7658.662485584871 at iteration 292\n",
            "loss:7643.844632310932 at iteration 293\n",
            "loss:7637.958890442929 at iteration 294\n",
            "loss:7627.340982385584 at iteration 295\n",
            "loss:7621.613012037695 at iteration 296\n",
            "loss:7623.180928275089 at iteration 297\n",
            "loss:7621.695443552075 at iteration 298\n",
            "loss:7619.346772867839 at iteration 299\n",
            "loss:7610.227690332355 at iteration 300\n",
            "loss:7595.311679056938 at iteration 301\n",
            "loss:7583.010534708256 at iteration 302\n",
            "loss:7568.691301446212 at iteration 303\n",
            "loss:7556.591580350282 at iteration 304\n",
            "loss:7551.754971772238 at iteration 305\n",
            "loss:7542.133655063492 at iteration 306\n",
            "loss:7541.560445017629 at iteration 307\n",
            "loss:7542.7105230930165 at iteration 308\n",
            "loss:7537.336453739289 at iteration 309\n",
            "loss:7535.679975994147 at iteration 310\n",
            "loss:7536.493745852739 at iteration 311\n",
            "loss:7530.629857462435 at iteration 312\n",
            "loss:7516.19588669698 at iteration 313\n",
            "epoch:  2\n",
            "loss:9970.49609375 at iteration 0\n",
            "loss:8348.8095703125 at iteration 1\n",
            "loss:8067.829264322917 at iteration 2\n",
            "loss:7859.751953125 at iteration 3\n",
            "loss:8253.253515625 at iteration 4\n",
            "loss:9028.8623046875 at iteration 5\n",
            "loss:9711.82924107143 at iteration 6\n",
            "loss:9948.001220703125 at iteration 7\n",
            "loss:9992.970052083334 at iteration 8\n",
            "loss:9627.6068359375 at iteration 9\n",
            "loss:9324.590775923296 at iteration 10\n",
            "loss:9214.860595703125 at iteration 11\n",
            "loss:9529.871619591346 at iteration 12\n",
            "loss:9283.10072544643 at iteration 13\n",
            "loss:9340.243033854167 at iteration 14\n",
            "loss:9710.932800292969 at iteration 15\n",
            "loss:10023.711224724264 at iteration 16\n",
            "loss:9811.703586154514 at iteration 17\n",
            "loss:9899.93562397204 at iteration 18\n",
            "loss:10027.675903320312 at iteration 19\n",
            "loss:9850.688058035714 at iteration 20\n",
            "loss:9609.838600852272 at iteration 21\n",
            "loss:9411.437160326086 at iteration 22\n",
            "loss:9221.252766927084 at iteration 23\n",
            "loss:9146.3996484375 at iteration 24\n",
            "loss:9030.142427884615 at iteration 25\n",
            "loss:8946.676920572916 at iteration 26\n",
            "loss:8722.35378592355 at iteration 27\n",
            "loss:8748.97645305765 at iteration 28\n",
            "loss:8737.240681966146 at iteration 29\n",
            "loss:8749.021098475303 at iteration 30\n",
            "loss:8751.427299499512 at iteration 31\n",
            "loss:8727.566102923769 at iteration 32\n",
            "loss:8710.210126091451 at iteration 33\n",
            "loss:8697.331856863839 at iteration 34\n",
            "loss:8591.955423990885 at iteration 35\n",
            "loss:8630.39563318201 at iteration 36\n",
            "loss:8758.42857601768 at iteration 37\n",
            "loss:8635.400165264424 at iteration 38\n",
            "loss:8546.380334472657 at iteration 39\n",
            "loss:8522.609398818597 at iteration 40\n",
            "loss:8473.493396577382 at iteration 41\n",
            "loss:8398.224041606105 at iteration 42\n",
            "loss:8325.60537997159 at iteration 43\n",
            "loss:8277.276323784723 at iteration 44\n",
            "loss:8361.913807744566 at iteration 45\n",
            "loss:8466.43344830452 at iteration 46\n",
            "loss:8553.775349934896 at iteration 47\n",
            "loss:8654.90736607143 at iteration 48\n",
            "loss:8769.44162109375 at iteration 49\n",
            "loss:8934.869427849264 at iteration 50\n",
            "loss:9015.025390625 at iteration 51\n",
            "loss:9089.160266804245 at iteration 52\n",
            "loss:9227.054000289352 at iteration 53\n",
            "loss:9356.731338778409 at iteration 54\n",
            "loss:9301.68053327288 at iteration 55\n",
            "loss:9194.576608758223 at iteration 56\n",
            "loss:9078.537210398707 at iteration 57\n",
            "loss:9193.987370895127 at iteration 58\n",
            "loss:9286.7955078125 at iteration 59\n",
            "loss:9202.270755955431 at iteration 60\n",
            "loss:9129.385726436492 at iteration 61\n",
            "loss:9081.098524305555 at iteration 62\n",
            "loss:9054.784538269043 at iteration 63\n",
            "loss:9012.987169471155 at iteration 64\n",
            "loss:8967.556337298769 at iteration 65\n",
            "loss:8940.321318213619 at iteration 66\n",
            "loss:8974.363884420956 at iteration 67\n",
            "loss:9014.54983299366 at iteration 68\n",
            "loss:9032.928125 at iteration 69\n",
            "loss:8967.868631712148 at iteration 70\n",
            "loss:8935.915568033854 at iteration 71\n",
            "loss:8915.811904698203 at iteration 72\n",
            "loss:8904.144379486908 at iteration 73\n",
            "loss:8813.271360677083 at iteration 74\n",
            "loss:8734.067408511513 at iteration 75\n",
            "loss:8720.22511033888 at iteration 76\n",
            "loss:8754.265368339344 at iteration 77\n",
            "loss:8740.651280656646 at iteration 78\n",
            "loss:8752.595593261718 at iteration 79\n",
            "loss:8761.157443576389 at iteration 80\n",
            "loss:8777.950231040397 at iteration 81\n",
            "loss:8806.699748211597 at iteration 82\n",
            "loss:8858.143938337054 at iteration 83\n",
            "loss:8815.359248621324 at iteration 84\n",
            "loss:8865.1620071766 at iteration 85\n",
            "loss:8918.781216325431 at iteration 86\n",
            "loss:8960.420154918324 at iteration 87\n",
            "loss:8963.441285551264 at iteration 88\n",
            "loss:8906.232703993055 at iteration 89\n",
            "loss:8882.98160628434 at iteration 90\n",
            "loss:8877.829823369566 at iteration 91\n",
            "loss:8846.017079343077 at iteration 92\n",
            "loss:8811.20522357048 at iteration 93\n",
            "loss:8801.688543379934 at iteration 94\n",
            "loss:8812.166783650717 at iteration 95\n",
            "loss:8848.939337346972 at iteration 96\n",
            "loss:8907.244265186544 at iteration 97\n",
            "loss:8921.876839685921 at iteration 98\n",
            "loss:8935.072534179688 at iteration 99\n",
            "loss:8887.27396929146 at iteration 100\n",
            "loss:8824.971564797794 at iteration 101\n",
            "loss:8783.622776661105 at iteration 102\n",
            "loss:8738.21607853816 at iteration 103\n",
            "loss:8727.645205543155 at iteration 104\n",
            "loss:8742.982960826947 at iteration 105\n",
            "loss:8761.237610433704 at iteration 106\n",
            "loss:8783.944783528646 at iteration 107\n",
            "loss:8808.768102243406 at iteration 108\n",
            "loss:8848.498770419033 at iteration 109\n",
            "loss:8937.068794869088 at iteration 110\n",
            "loss:8947.942770821708 at iteration 111\n",
            "loss:8957.4464489422 at iteration 112\n",
            "loss:8942.457343921327 at iteration 113\n",
            "loss:8925.71691576087 at iteration 114\n",
            "loss:8908.15737388874 at iteration 115\n",
            "loss:8903.572778946314 at iteration 116\n",
            "loss:8895.424647444386 at iteration 117\n",
            "loss:8886.356634059874 at iteration 118\n",
            "loss:8923.328792317709 at iteration 119\n",
            "loss:8921.426217071281 at iteration 120\n",
            "loss:8966.080142161885 at iteration 121\n",
            "loss:9034.674320376016 at iteration 122\n",
            "loss:9048.557483303932 at iteration 123\n",
            "loss:9070.8631328125 at iteration 124\n",
            "loss:9118.088657924107 at iteration 125\n",
            "loss:9109.218788447342 at iteration 126\n",
            "loss:9100.635032653809 at iteration 127\n",
            "loss:9087.852376302084 at iteration 128\n",
            "loss:9081.151453575721 at iteration 129\n",
            "loss:9087.849471463502 at iteration 130\n",
            "loss:9101.111598159328 at iteration 131\n",
            "loss:9111.913867921758 at iteration 132\n",
            "loss:9122.998874038012 at iteration 133\n",
            "loss:9086.983297164352 at iteration 134\n",
            "loss:9060.163017721738 at iteration 135\n",
            "loss:9035.513308337135 at iteration 136\n",
            "loss:9028.399739583334 at iteration 137\n",
            "loss:9002.877304406475 at iteration 138\n",
            "loss:8972.834144810267 at iteration 139\n",
            "loss:8986.873898769947 at iteration 140\n",
            "loss:8993.21883940361 at iteration 141\n",
            "loss:8991.624781468532 at iteration 142\n",
            "loss:8997.797268337674 at iteration 143\n",
            "loss:9029.264722521551 at iteration 144\n",
            "loss:9068.637815710616 at iteration 145\n",
            "loss:9070.600738732994 at iteration 146\n",
            "loss:9052.065370301943 at iteration 147\n",
            "loss:9029.166831611787 at iteration 148\n",
            "loss:8989.890895182292 at iteration 149\n",
            "loss:8953.635585355443 at iteration 150\n",
            "loss:8915.80800909745 at iteration 151\n",
            "loss:8894.264846303104 at iteration 152\n",
            "loss:8862.647432401584 at iteration 153\n",
            "loss:8834.165199722782 at iteration 154\n",
            "loss:8823.890124198719 at iteration 155\n",
            "loss:8802.568682822452 at iteration 156\n",
            "loss:8765.324076592167 at iteration 157\n",
            "loss:8729.34038116647 at iteration 158\n",
            "loss:8693.859252929688 at iteration 159\n",
            "loss:8659.113329774846 at iteration 160\n",
            "loss:8647.107244044175 at iteration 161\n",
            "loss:8619.340544718174 at iteration 162\n",
            "loss:8590.707647556212 at iteration 163\n",
            "loss:8596.28359670928 at iteration 164\n",
            "loss:8572.600430040475 at iteration 165\n",
            "loss:8569.23063248503 at iteration 166\n",
            "loss:8549.136282784599 at iteration 167\n",
            "loss:8516.028294309357 at iteration 168\n",
            "loss:8481.545209099264 at iteration 169\n",
            "loss:8477.145956117507 at iteration 170\n",
            "loss:8478.695201785065 at iteration 171\n",
            "loss:8445.354952244401 at iteration 172\n",
            "loss:8404.755549989897 at iteration 173\n",
            "loss:8412.879859793527 at iteration 174\n",
            "loss:8420.33321241899 at iteration 175\n",
            "loss:8427.191907634842 at iteration 176\n",
            "loss:8429.900746549114 at iteration 177\n",
            "loss:8414.311244517065 at iteration 178\n",
            "loss:8396.8401034885 at iteration 179\n",
            "loss:8378.774742505828 at iteration 180\n",
            "loss:8356.81438269982 at iteration 181\n",
            "loss:8335.399258639643 at iteration 182\n",
            "loss:8312.918068927267 at iteration 183\n",
            "loss:8309.827211122256 at iteration 184\n",
            "loss:8287.947422478788 at iteration 185\n",
            "loss:8268.636067925927 at iteration 186\n",
            "loss:8258.893130687957 at iteration 187\n",
            "loss:8278.159594984912 at iteration 188\n",
            "loss:8269.917777934827 at iteration 189\n",
            "loss:8254.03497921609 at iteration 190\n",
            "loss:8249.828549067179 at iteration 191\n",
            "loss:8239.355622444746 at iteration 192\n",
            "loss:8235.057754988522 at iteration 193\n",
            "loss:8227.255207707332 at iteration 194\n",
            "loss:8201.425698416573 at iteration 195\n",
            "loss:8174.4962182988975 at iteration 196\n",
            "loss:8170.921190049913 at iteration 197\n",
            "loss:8164.808395615774 at iteration 198\n",
            "loss:8153.928445434571 at iteration 199\n",
            "loss:8140.301476625661 at iteration 200\n",
            "loss:8126.72658244218 at iteration 201\n",
            "loss:8130.585115479718 at iteration 202\n",
            "loss:8137.552749334597 at iteration 203\n",
            "loss:8141.12250917016 at iteration 204\n",
            "loss:8137.702371838023 at iteration 205\n",
            "loss:8122.8320577870245 at iteration 206\n",
            "loss:8104.950731130747 at iteration 207\n",
            "loss:8091.7232385662755 at iteration 208\n",
            "loss:8081.396119907924 at iteration 209\n",
            "loss:8062.666532254332 at iteration 210\n",
            "loss:8054.298340419553 at iteration 211\n",
            "loss:8051.857078588065 at iteration 212\n",
            "loss:8048.10725816388 at iteration 213\n",
            "loss:8034.335813158612 at iteration 214\n",
            "loss:8024.917252717195 at iteration 215\n",
            "loss:8019.478844708561 at iteration 216\n",
            "loss:8015.361577305225 at iteration 217\n",
            "loss:8006.426225496753 at iteration 218\n",
            "loss:7996.351753928445 at iteration 219\n",
            "loss:7997.326875022094 at iteration 220\n",
            "loss:7988.7348792273715 at iteration 221\n",
            "loss:7997.193292044738 at iteration 222\n",
            "loss:7994.709595271519 at iteration 223\n",
            "loss:7981.204965820312 at iteration 224\n",
            "loss:7974.243391458967 at iteration 225\n",
            "loss:7973.075249625723 at iteration 226\n",
            "loss:7970.914981775117 at iteration 227\n",
            "loss:7970.082157584778 at iteration 228\n",
            "loss:7967.199921981148 at iteration 229\n",
            "loss:7966.299486036424 at iteration 230\n",
            "loss:7964.443185214339 at iteration 231\n",
            "loss:7958.430184687668 at iteration 232\n",
            "loss:7954.809174366486 at iteration 233\n",
            "loss:7953.043958298704 at iteration 234\n",
            "loss:7938.903776007183 at iteration 235\n",
            "loss:7943.016045807786 at iteration 236\n",
            "loss:7937.504912047827 at iteration 237\n",
            "loss:7927.376818796581 at iteration 238\n",
            "loss:7915.036576843262 at iteration 239\n",
            "loss:7899.743172673269 at iteration 240\n",
            "loss:7884.959689053622 at iteration 241\n",
            "loss:7870.60728774836 at iteration 242\n",
            "loss:7858.545490702645 at iteration 243\n",
            "loss:7854.976234155772 at iteration 244\n",
            "loss:7865.148154157933 at iteration 245\n",
            "loss:7874.6402123331545 at iteration 246\n",
            "loss:7882.045613442698 at iteration 247\n",
            "loss:7879.053403555628 at iteration 248\n",
            "loss:7886.204057128906 at iteration 249\n",
            "loss:7893.802923559668 at iteration 250\n",
            "loss:7899.606563991971 at iteration 251\n",
            "loss:7903.5902746147785 at iteration 252\n",
            "loss:7903.809159406527 at iteration 253\n",
            "loss:7901.269905120252 at iteration 254\n",
            "loss:7902.479254245758 at iteration 255\n",
            "loss:7900.5412516909355 at iteration 256\n",
            "loss:7886.627532722414 at iteration 257\n",
            "loss:7877.558401925223 at iteration 258\n",
            "loss:7876.103576190655 at iteration 259\n",
            "loss:7863.0178629557295 at iteration 260\n",
            "loss:7858.614137139939 at iteration 261\n",
            "loss:7847.142383647963 at iteration 262\n",
            "loss:7829.479853312175 at iteration 263\n",
            "loss:7817.667759157576 at iteration 264\n",
            "loss:7818.351010430128 at iteration 265\n",
            "loss:7801.290534201633 at iteration 266\n",
            "loss:7797.15495960748 at iteration 267\n",
            "loss:7798.1463623046875 at iteration 268\n",
            "loss:7801.4332370334205 at iteration 269\n",
            "loss:7802.802843832882 at iteration 270\n",
            "loss:7813.728348227109 at iteration 271\n",
            "loss:7821.287497048849 at iteration 272\n",
            "loss:7810.28455703624 at iteration 273\n",
            "loss:7810.80667746804 at iteration 274\n",
            "loss:7817.828784445058 at iteration 275\n",
            "loss:7826.261501491285 at iteration 276\n",
            "loss:7821.21426720928 at iteration 277\n",
            "loss:7819.229300987763 at iteration 278\n",
            "loss:7814.35066746303 at iteration 279\n",
            "loss:7810.2428143592915 at iteration 280\n",
            "loss:7794.52177050435 at iteration 281\n",
            "loss:7781.025735268744 at iteration 282\n",
            "loss:7765.45552041497 at iteration 283\n",
            "loss:7758.099940463952 at iteration 284\n",
            "loss:7744.118560577606 at iteration 285\n",
            "loss:7734.6271560084115 at iteration 286\n",
            "loss:7720.893023596869 at iteration 287\n",
            "loss:7707.659251915955 at iteration 288\n",
            "loss:7699.158174080684 at iteration 289\n",
            "loss:7688.234602780686 at iteration 290\n",
            "loss:7674.695435824459 at iteration 291\n",
            "loss:7658.11738206258 at iteration 292\n",
            "loss:7643.231469809603 at iteration 293\n",
            "loss:7637.310344527939 at iteration 294\n",
            "loss:7626.675770940007 at iteration 295\n",
            "loss:7620.966923055424 at iteration 296\n",
            "loss:7622.563571187474 at iteration 297\n",
            "loss:7621.151628066864 at iteration 298\n",
            "loss:7618.8647001139325 at iteration 299\n",
            "loss:7609.730664630269 at iteration 300\n",
            "loss:7594.871212990868 at iteration 301\n",
            "loss:7582.567209174531 at iteration 302\n",
            "loss:7568.2601378591435 at iteration 303\n",
            "loss:7556.168333360016 at iteration 304\n",
            "loss:7551.32986709495 at iteration 305\n",
            "loss:7541.611604473102 at iteration 306\n",
            "loss:7540.990078616452 at iteration 307\n",
            "loss:7542.001736242794 at iteration 308\n",
            "loss:7536.567472199471 at iteration 309\n",
            "loss:7534.944768874975 at iteration 310\n",
            "loss:7535.804214477539 at iteration 311\n",
            "loss:7529.8846638347395 at iteration 312\n",
            "loss:7515.378281514356 at iteration 313\n",
            "epoch:  3\n",
            "loss:9960.6708984375 at iteration 0\n",
            "loss:8348.147216796875 at iteration 1\n",
            "loss:8063.789876302083 at iteration 2\n",
            "loss:7851.1160888671875 at iteration 3\n",
            "loss:8245.15478515625 at iteration 4\n",
            "loss:9021.319417317709 at iteration 5\n",
            "loss:9698.100516183036 at iteration 6\n",
            "loss:9938.901672363281 at iteration 7\n",
            "loss:9986.201877170139 at iteration 8\n",
            "loss:9618.377099609375 at iteration 9\n",
            "loss:9317.30721768466 at iteration 10\n",
            "loss:9208.57470703125 at iteration 11\n",
            "loss:9523.459059495191 at iteration 12\n",
            "loss:9277.059884207589 at iteration 13\n",
            "loss:9336.049641927084 at iteration 14\n",
            "loss:9705.981475830078 at iteration 15\n",
            "loss:10019.00186695772 at iteration 16\n",
            "loss:9808.057318793402 at iteration 17\n",
            "loss:9896.755319695723 at iteration 18\n",
            "loss:10024.768774414062 at iteration 19\n",
            "loss:9847.597679501489 at iteration 20\n",
            "loss:9607.393399325285 at iteration 21\n",
            "loss:9408.238557235054 at iteration 22\n",
            "loss:9217.761739095053 at iteration 23\n",
            "loss:9142.37375 at iteration 24\n",
            "loss:9027.356370192309 at iteration 25\n",
            "loss:8943.845522280093 at iteration 26\n",
            "loss:8719.775721958706 at iteration 27\n",
            "loss:8746.677178744612 at iteration 28\n",
            "loss:8735.440999348959 at iteration 29\n",
            "loss:8746.014538180443 at iteration 30\n",
            "loss:8748.790237426758 at iteration 31\n",
            "loss:8723.618090080492 at iteration 32\n",
            "loss:8706.803079044117 at iteration 33\n",
            "loss:8695.674637276787 at iteration 34\n",
            "loss:8590.44036187066 at iteration 35\n",
            "loss:8628.37208350929 at iteration 36\n",
            "loss:8756.899272717928 at iteration 37\n",
            "loss:8633.817858573719 at iteration 38\n",
            "loss:8545.20264892578 at iteration 39\n",
            "loss:8521.485315834603 at iteration 40\n",
            "loss:8471.671293712798 at iteration 41\n",
            "loss:8397.054312772529 at iteration 42\n",
            "loss:8324.829612038353 at iteration 43\n",
            "loss:8277.03388671875 at iteration 44\n",
            "loss:8361.961181640625 at iteration 45\n",
            "loss:8466.416213015293 at iteration 46\n",
            "loss:8554.077748616537 at iteration 47\n",
            "loss:8656.212282764669 at iteration 48\n",
            "loss:8770.390498046874 at iteration 49\n",
            "loss:8935.55965647978 at iteration 50\n",
            "loss:9015.768301156852 at iteration 51\n",
            "loss:9089.596541494693 at iteration 52\n",
            "loss:9227.288764105902 at iteration 53\n",
            "loss:9357.081400923296 at iteration 54\n",
            "loss:9301.932686941964 at iteration 55\n",
            "loss:9194.707039816338 at iteration 56\n",
            "loss:9078.48911048626 at iteration 57\n",
            "loss:9193.217479641155 at iteration 58\n",
            "loss:9286.577770996093 at iteration 59\n",
            "loss:9202.317963146772 at iteration 60\n",
            "loss:9129.773268176663 at iteration 61\n",
            "loss:9081.494028242807 at iteration 62\n",
            "loss:9055.3063621521 at iteration 63\n",
            "loss:9013.800845102163 at iteration 64\n",
            "loss:8968.785796194366 at iteration 65\n",
            "loss:8940.996017228312 at iteration 66\n",
            "loss:8974.630266974955 at iteration 67\n",
            "loss:9014.848898182745 at iteration 68\n",
            "loss:9032.718258231027 at iteration 69\n",
            "loss:8967.994549818442 at iteration 70\n",
            "loss:8936.264366997613 at iteration 71\n",
            "loss:8916.18654015946 at iteration 72\n",
            "loss:8904.633897936023 at iteration 73\n",
            "loss:8813.677516276042 at iteration 74\n",
            "loss:8734.55045679996 at iteration 75\n",
            "loss:8720.402733740868 at iteration 76\n",
            "loss:8753.988453400441 at iteration 77\n",
            "loss:8740.788416608979 at iteration 78\n",
            "loss:8752.862106323242 at iteration 79\n",
            "loss:8761.526870539159 at iteration 80\n",
            "loss:8778.39742223228 at iteration 81\n",
            "loss:8807.00203254424 at iteration 82\n",
            "loss:8858.509390694755 at iteration 83\n",
            "loss:8815.625720932905 at iteration 84\n",
            "loss:8865.332459915517 at iteration 85\n",
            "loss:8918.62052970097 at iteration 86\n",
            "loss:8960.073638916016 at iteration 87\n",
            "loss:8962.872747871314 at iteration 88\n",
            "loss:8905.66171061198 at iteration 89\n",
            "loss:8882.602042732658 at iteration 90\n",
            "loss:8877.296437139097 at iteration 91\n",
            "loss:8845.829182942709 at iteration 92\n",
            "loss:8811.205839116523 at iteration 93\n",
            "loss:8801.67826120477 at iteration 94\n",
            "loss:8812.232668558756 at iteration 95\n",
            "loss:8848.47191376047 at iteration 96\n",
            "loss:8906.664792430644 at iteration 97\n",
            "loss:8921.329478870739 at iteration 98\n",
            "loss:8934.586799316407 at iteration 99\n",
            "loss:8886.811052076888 at iteration 100\n",
            "loss:8824.38731474035 at iteration 101\n",
            "loss:8782.825396787774 at iteration 102\n",
            "loss:8737.547971285307 at iteration 103\n",
            "loss:8726.789167131696 at iteration 104\n",
            "loss:8742.22264934036 at iteration 105\n",
            "loss:8760.725761627482 at iteration 106\n",
            "loss:8782.860733597367 at iteration 107\n",
            "loss:8807.819176910121 at iteration 108\n",
            "loss:8847.529467773438 at iteration 109\n",
            "loss:8935.592494105433 at iteration 110\n",
            "loss:8946.268112182617 at iteration 111\n",
            "loss:8955.549262392837 at iteration 112\n",
            "loss:8940.658659282484 at iteration 113\n",
            "loss:8924.07926503057 at iteration 114\n",
            "loss:8906.564392089844 at iteration 115\n",
            "loss:8902.14893621461 at iteration 116\n",
            "loss:8894.267410536944 at iteration 117\n",
            "loss:8885.267563763788 at iteration 118\n",
            "loss:8922.285288492838 at iteration 119\n",
            "loss:8920.385364879261 at iteration 120\n",
            "loss:8965.167650566726 at iteration 121\n",
            "loss:9034.093593194233 at iteration 122\n",
            "loss:9048.064069194179 at iteration 123\n",
            "loss:9070.542376953124 at iteration 124\n",
            "loss:9117.75680687314 at iteration 125\n",
            "loss:9109.067011795645 at iteration 126\n",
            "loss:9100.359926223755 at iteration 127\n",
            "loss:9087.577082197795 at iteration 128\n",
            "loss:9080.79711726262 at iteration 129\n",
            "loss:9087.470540985807 at iteration 130\n",
            "loss:9100.533708052202 at iteration 131\n",
            "loss:9111.265369845512 at iteration 132\n",
            "loss:9122.5871490934 at iteration 133\n",
            "loss:9086.654758029514 at iteration 134\n",
            "loss:9059.905149572036 at iteration 135\n",
            "loss:9035.243807381958 at iteration 136\n",
            "loss:9028.199549578238 at iteration 137\n",
            "loss:9002.783067881632 at iteration 138\n",
            "loss:8972.443545968192 at iteration 139\n",
            "loss:8986.562117339872 at iteration 140\n",
            "loss:8993.10262674681 at iteration 141\n",
            "loss:8991.495839365713 at iteration 142\n",
            "loss:8997.389185587564 at iteration 143\n",
            "loss:9028.874749124461 at iteration 144\n",
            "loss:9068.508459639876 at iteration 145\n",
            "loss:9070.219844480762 at iteration 146\n",
            "loss:9051.618855244404 at iteration 147\n",
            "loss:9028.527299509753 at iteration 148\n",
            "loss:8989.404881184895 at iteration 149\n",
            "loss:8953.19875795478 at iteration 150\n",
            "loss:8915.427258943257 at iteration 151\n",
            "loss:8893.779759625204 at iteration 152\n",
            "loss:8862.122341403714 at iteration 153\n",
            "loss:8833.58465064264 at iteration 154\n",
            "loss:8823.327716533955 at iteration 155\n",
            "loss:8801.898208909734 at iteration 156\n",
            "loss:8764.647256971915 at iteration 157\n",
            "loss:8728.688800547858 at iteration 158\n",
            "loss:8693.160057067871 at iteration 159\n",
            "loss:8658.345384680706 at iteration 160\n",
            "loss:8646.421220944252 at iteration 161\n",
            "loss:8618.750859734471 at iteration 162\n",
            "loss:8590.201680997523 at iteration 163\n",
            "loss:8595.718155184659 at iteration 164\n",
            "loss:8572.253156179406 at iteration 165\n",
            "loss:8569.05373140438 at iteration 166\n",
            "loss:8548.984334309896 at iteration 167\n",
            "loss:8515.82217172476 at iteration 168\n",
            "loss:8481.355948414523 at iteration 169\n",
            "loss:8476.981199744152 at iteration 170\n",
            "loss:8478.543002816134 at iteration 171\n",
            "loss:8445.1112716763 at iteration 172\n",
            "loss:8404.571060356053 at iteration 173\n",
            "loss:8412.669058314732 at iteration 174\n",
            "loss:8419.973545421253 at iteration 175\n",
            "loss:8426.9340199616 at iteration 176\n",
            "loss:8429.618280646506 at iteration 177\n",
            "loss:8413.927247457665 at iteration 178\n",
            "loss:8396.555440266928 at iteration 179\n",
            "loss:8378.455244032717 at iteration 180\n",
            "loss:8356.530797937414 at iteration 181\n",
            "loss:8335.168290268528 at iteration 182\n",
            "loss:8312.693687107252 at iteration 183\n",
            "loss:8309.615478515625 at iteration 184\n",
            "loss:8287.594559864332 at iteration 185\n",
            "loss:8268.345688763788 at iteration 186\n",
            "loss:8258.691719217504 at iteration 187\n",
            "loss:8278.066743396577 at iteration 188\n",
            "loss:8269.952136872944 at iteration 189\n",
            "loss:8254.11019434105 at iteration 190\n",
            "loss:8249.95739364624 at iteration 191\n",
            "loss:8239.612933381233 at iteration 192\n",
            "loss:8235.36571888088 at iteration 193\n",
            "loss:8227.670448968349 at iteration 194\n",
            "loss:8201.909202108578 at iteration 195\n",
            "loss:8175.0701935279185 at iteration 196\n",
            "loss:8171.556468000315 at iteration 197\n",
            "loss:8165.398062087783 at iteration 198\n",
            "loss:8154.481271972656 at iteration 199\n",
            "loss:8140.839714999222 at iteration 200\n",
            "loss:8127.2125957224625 at iteration 201\n",
            "loss:8131.003242379926 at iteration 202\n",
            "loss:8137.89453125 at iteration 203\n",
            "loss:8141.43241711128 at iteration 204\n",
            "loss:8138.0223234602545 at iteration 205\n",
            "loss:8123.154495018116 at iteration 206\n",
            "loss:8105.236022949219 at iteration 207\n",
            "loss:8092.061810799192 at iteration 208\n",
            "loss:8081.746628534226 at iteration 209\n",
            "loss:8063.013433519698 at iteration 210\n",
            "loss:8054.608958118367 at iteration 211\n",
            "loss:8052.117879804871 at iteration 212\n",
            "loss:8048.363304066881 at iteration 213\n",
            "loss:8034.565827125727 at iteration 214\n",
            "loss:8025.252059371383 at iteration 215\n",
            "loss:8019.861217867943 at iteration 216\n",
            "loss:8015.790478067661 at iteration 217\n",
            "loss:8006.884395512272 at iteration 218\n",
            "loss:7996.72694868608 at iteration 219\n",
            "loss:7997.67467654129 at iteration 220\n",
            "loss:7988.940561655405 at iteration 221\n",
            "loss:7997.201885685258 at iteration 222\n",
            "loss:7994.538909912109 at iteration 223\n",
            "loss:7981.0864474826385 at iteration 224\n",
            "loss:7974.1950532356195 at iteration 225\n",
            "loss:7973.053592631471 at iteration 226\n",
            "loss:7971.014365748355 at iteration 227\n",
            "loss:7970.254524597434 at iteration 228\n",
            "loss:7967.355802055027 at iteration 229\n",
            "loss:7966.4412646272995 at iteration 230\n",
            "loss:7964.51258166083 at iteration 231\n",
            "loss:7958.529565115344 at iteration 232\n",
            "loss:7954.922763922275 at iteration 233\n",
            "loss:7953.269510472074 at iteration 234\n",
            "loss:7939.120903403072 at iteration 235\n",
            "loss:7943.233942345728 at iteration 236\n",
            "loss:7937.764381729255 at iteration 237\n",
            "loss:7927.717011391867 at iteration 238\n",
            "loss:7915.496496582031 at iteration 239\n",
            "loss:7900.211859358791 at iteration 240\n",
            "loss:7885.428170196281 at iteration 241\n",
            "loss:7871.078042615098 at iteration 242\n",
            "loss:7859.041385838243 at iteration 243\n",
            "loss:7855.414526865434 at iteration 244\n",
            "loss:7865.6321396246185 at iteration 245\n",
            "loss:7875.051562104631 at iteration 246\n",
            "loss:7882.546652517011 at iteration 247\n",
            "loss:7879.485133894955 at iteration 248\n",
            "loss:7886.662533203125 at iteration 249\n",
            "loss:7894.219304422933 at iteration 250\n",
            "loss:7900.124017624628 at iteration 251\n",
            "loss:7904.144853554224 at iteration 252\n",
            "loss:7904.343761534203 at iteration 253\n",
            "loss:7901.802269071692 at iteration 254\n",
            "loss:7903.067834854126 at iteration 255\n",
            "loss:7901.089159776265 at iteration 256\n",
            "loss:7887.244893865068 at iteration 257\n",
            "loss:7878.2018882722 at iteration 258\n",
            "loss:7876.6650954026445 at iteration 259\n",
            "loss:7863.549939385776 at iteration 260\n",
            "loss:7859.02880859375 at iteration 261\n",
            "loss:7847.561248663261 at iteration 262\n",
            "loss:7829.844961455374 at iteration 263\n",
            "loss:7818.039945091392 at iteration 264\n",
            "loss:7818.878173828125 at iteration 265\n",
            "loss:7801.766859418891 at iteration 266\n",
            "loss:7797.55126588736 at iteration 267\n",
            "loss:7798.650002178207 at iteration 268\n",
            "loss:7801.84974681713 at iteration 269\n",
            "loss:7803.273936591905 at iteration 270\n",
            "loss:7814.184108958525 at iteration 271\n",
            "loss:7821.7247649811125 at iteration 272\n",
            "loss:7810.74879533531 at iteration 273\n",
            "loss:7811.159378551137 at iteration 274\n",
            "loss:7818.217808820199 at iteration 275\n",
            "loss:7826.677402978339 at iteration 276\n",
            "loss:7821.538475859937 at iteration 277\n",
            "loss:7819.561407930108 at iteration 278\n",
            "loss:7814.77095249721 at iteration 279\n",
            "loss:7810.70011885565 at iteration 280\n",
            "loss:7795.032584981715 at iteration 281\n",
            "loss:7781.50198331907 at iteration 282\n",
            "loss:7765.882071481624 at iteration 283\n",
            "loss:7758.425152480811 at iteration 284\n",
            "loss:7744.4311412464485 at iteration 285\n",
            "loss:7734.89575705711 at iteration 286\n",
            "loss:7721.13776228163 at iteration 287\n",
            "loss:7707.867647058823 at iteration 288\n",
            "loss:7699.283918709591 at iteration 289\n",
            "loss:7688.402407511812 at iteration 290\n",
            "loss:7674.871643902505 at iteration 291\n",
            "loss:7658.291875533277 at iteration 292\n",
            "loss:7643.433951656834 at iteration 293\n",
            "loss:7637.481950973252 at iteration 294\n",
            "loss:7626.8461443926835 at iteration 295\n",
            "loss:7621.158435757313 at iteration 296\n",
            "loss:7622.766636790845 at iteration 297\n",
            "loss:7621.1748218345 at iteration 298\n",
            "loss:7618.888454589844 at iteration 299\n",
            "loss:7609.7592846436355 at iteration 300\n",
            "loss:7594.894962133951 at iteration 301\n",
            "loss:7582.5430195119125 at iteration 302\n",
            "loss:7568.2561597322165 at iteration 303\n",
            "loss:7556.117168288934 at iteration 304\n",
            "loss:7551.283804700265 at iteration 305\n",
            "loss:7541.524795780741 at iteration 306\n",
            "loss:7540.9161773285305 at iteration 307\n",
            "loss:7542.068948788936 at iteration 308\n",
            "loss:7536.611399004536 at iteration 309\n",
            "loss:7535.013891680064 at iteration 310\n",
            "loss:7535.904519105569 at iteration 311\n",
            "loss:7529.970735885084 at iteration 312\n",
            "loss:7515.4873731090765 at iteration 313\n",
            "epoch:  4\n",
            "loss:9999.6083984375 at iteration 0\n",
            "loss:8375.134033203125 at iteration 1\n",
            "loss:8082.133463541667 at iteration 2\n",
            "loss:7866.6878662109375 at iteration 3\n",
            "loss:8258.73466796875 at iteration 4\n",
            "loss:9031.980712890625 at iteration 5\n",
            "loss:9706.416782924107 at iteration 6\n",
            "loss:9938.900817871094 at iteration 7\n",
            "loss:9987.317220052084 at iteration 8\n",
            "loss:9618.460498046876 at iteration 9\n",
            "loss:9319.9130859375 at iteration 10\n",
            "loss:9211.205810546875 at iteration 11\n",
            "loss:9527.059945913461 at iteration 12\n",
            "loss:9280.64564732143 at iteration 13\n",
            "loss:9338.423372395833 at iteration 14\n",
            "loss:9707.268676757812 at iteration 15\n",
            "loss:10019.557100183823 at iteration 16\n",
            "loss:9807.836832682291 at iteration 17\n",
            "loss:9897.352153577303 at iteration 18\n",
            "loss:10025.324096679688 at iteration 19\n",
            "loss:9848.113606770834 at iteration 20\n",
            "loss:9607.489080255682 at iteration 21\n",
            "loss:9408.021420686142 at iteration 22\n",
            "loss:9217.453247070312 at iteration 23\n",
            "loss:9143.137578125 at iteration 24\n",
            "loss:9027.64471905048 at iteration 25\n",
            "loss:8944.540201822916 at iteration 26\n",
            "loss:8720.152448381696 at iteration 27\n",
            "loss:8747.281654094828 at iteration 28\n",
            "loss:8735.360579427082 at iteration 29\n",
            "loss:8746.860540574597 at iteration 30\n",
            "loss:8749.26791381836 at iteration 31\n",
            "loss:8724.909016927084 at iteration 32\n",
            "loss:8708.43416819853 at iteration 33\n",
            "loss:8695.926981026785 at iteration 34\n",
            "loss:8590.875298394098 at iteration 35\n",
            "loss:8629.132654138513 at iteration 36\n",
            "loss:8757.127055921053 at iteration 37\n",
            "loss:8633.957794971955 at iteration 38\n",
            "loss:8545.36964111328 at iteration 39\n",
            "loss:8522.146841653963 at iteration 40\n",
            "loss:8472.629557291666 at iteration 41\n",
            "loss:8397.877021257267 at iteration 42\n",
            "loss:8325.586270419035 at iteration 43\n",
            "loss:8277.561284722222 at iteration 44\n",
            "loss:8362.681852921196 at iteration 45\n",
            "loss:8467.157704454787 at iteration 46\n",
            "loss:8554.383178710938 at iteration 47\n",
            "loss:8656.005879304847 at iteration 48\n",
            "loss:8770.2412890625 at iteration 49\n",
            "loss:8936.074908088236 at iteration 50\n",
            "loss:9016.519587590145 at iteration 51\n",
            "loss:9089.878132370282 at iteration 52\n",
            "loss:9228.077582465277 at iteration 53\n",
            "loss:9358.028480113637 at iteration 54\n",
            "loss:9302.792890276227 at iteration 55\n",
            "loss:9195.626811780428 at iteration 56\n",
            "loss:9079.264736833244 at iteration 57\n",
            "loss:9194.13829531912 at iteration 58\n",
            "loss:9287.005993652343 at iteration 59\n",
            "loss:9202.708267962345 at iteration 60\n",
            "loss:9129.818394814769 at iteration 61\n",
            "loss:9081.948238312252 at iteration 62\n",
            "loss:9055.450740814209 at iteration 63\n",
            "loss:9013.834836989183 at iteration 64\n",
            "loss:8968.617220791903 at iteration 65\n",
            "loss:8940.654191202193 at iteration 66\n",
            "loss:8974.44731947955 at iteration 67\n",
            "loss:9014.437312471693 at iteration 68\n",
            "loss:9032.85411202567 at iteration 69\n",
            "loss:8967.990959919674 at iteration 70\n",
            "loss:8936.356021457248 at iteration 71\n",
            "loss:8916.902915641052 at iteration 72\n",
            "loss:8905.230379671664 at iteration 73\n",
            "loss:8814.421643880209 at iteration 74\n",
            "loss:8735.058288574219 at iteration 75\n",
            "loss:8721.288533000203 at iteration 76\n",
            "loss:8755.242653871193 at iteration 77\n",
            "loss:8741.656642479233 at iteration 78\n",
            "loss:8753.686422729492 at iteration 79\n",
            "loss:8762.650628737461 at iteration 80\n",
            "loss:8779.398601252858 at iteration 81\n",
            "loss:8808.122702724962 at iteration 82\n",
            "loss:8859.478512718564 at iteration 83\n",
            "loss:8816.718718405331 at iteration 84\n",
            "loss:8866.914440066315 at iteration 85\n",
            "loss:8920.05525154903 at iteration 86\n",
            "loss:8961.41302212802 at iteration 87\n",
            "loss:8964.319634941186 at iteration 88\n",
            "loss:8906.798871527777 at iteration 89\n",
            "loss:8883.395400497939 at iteration 90\n",
            "loss:8878.069930366848 at iteration 91\n",
            "loss:8846.474483366936 at iteration 92\n",
            "loss:8811.57553295379 at iteration 93\n",
            "loss:8802.24741981908 at iteration 94\n",
            "loss:8812.831848144531 at iteration 95\n",
            "loss:8849.422831427191 at iteration 96\n",
            "loss:8907.70169005102 at iteration 97\n",
            "loss:8922.949208885731 at iteration 98\n",
            "loss:8936.375478515625 at iteration 99\n",
            "loss:8888.453453743812 at iteration 100\n",
            "loss:8825.97756060432 at iteration 101\n",
            "loss:8784.442280889714 at iteration 102\n",
            "loss:8738.963968130258 at iteration 103\n",
            "loss:8728.348607235863 at iteration 104\n",
            "loss:8743.292372217718 at iteration 105\n",
            "loss:8761.475186642085 at iteration 106\n",
            "loss:8783.875897442853 at iteration 107\n",
            "loss:8808.305755895211 at iteration 108\n",
            "loss:8847.8240345348 at iteration 109\n",
            "loss:8935.95790443764 at iteration 110\n",
            "loss:8946.851259504047 at iteration 111\n",
            "loss:8956.226763429895 at iteration 112\n",
            "loss:8941.245256390488 at iteration 113\n",
            "loss:8924.714366083559 at iteration 114\n",
            "loss:8907.395876128097 at iteration 115\n",
            "loss:8903.17410982572 at iteration 116\n",
            "loss:8895.298991575079 at iteration 117\n",
            "loss:8886.426915785845 at iteration 118\n",
            "loss:8923.41868693034 at iteration 119\n",
            "loss:8921.345818133394 at iteration 120\n",
            "loss:8966.199420866418 at iteration 121\n",
            "loss:9035.151194502667 at iteration 122\n",
            "loss:9049.04200203188 at iteration 123\n",
            "loss:9071.530423828124 at iteration 124\n",
            "loss:9118.712063259549 at iteration 125\n",
            "loss:9110.037699541708 at iteration 126\n",
            "loss:9101.453847885132 at iteration 127\n",
            "loss:9088.692823779675 at iteration 128\n",
            "loss:9081.846185772236 at iteration 129\n",
            "loss:9088.7868577797 at iteration 130\n",
            "loss:9101.925805294153 at iteration 131\n",
            "loss:9112.735382768444 at iteration 132\n",
            "loss:9123.89086731868 at iteration 133\n",
            "loss:9087.910461877893 at iteration 134\n",
            "loss:9061.133435417623 at iteration 135\n",
            "loss:9036.429247334056 at iteration 136\n",
            "loss:9029.244591754416 at iteration 137\n",
            "loss:9003.680627177946 at iteration 138\n",
            "loss:8973.432343401228 at iteration 139\n",
            "loss:8987.27822854333 at iteration 140\n",
            "loss:8993.619867888974 at iteration 141\n",
            "loss:8991.970988240275 at iteration 142\n",
            "loss:8998.05617099338 at iteration 143\n",
            "loss:9029.22515321929 at iteration 144\n",
            "loss:9068.796371668985 at iteration 145\n",
            "loss:9070.469339591305 at iteration 146\n",
            "loss:9051.96411957612 at iteration 147\n",
            "loss:9029.072221384753 at iteration 148\n",
            "loss:8989.789892578125 at iteration 149\n",
            "loss:8953.501366217404 at iteration 150\n",
            "loss:8915.673245078639 at iteration 151\n",
            "loss:8894.046062793606 at iteration 152\n",
            "loss:8862.370971679688 at iteration 153\n",
            "loss:8833.918471207156 at iteration 154\n",
            "loss:8823.609198154547 at iteration 155\n",
            "loss:8802.03150347084 at iteration 156\n",
            "loss:8764.82591382763 at iteration 157\n",
            "loss:8728.836038841391 at iteration 158\n",
            "loss:8693.577801513671 at iteration 159\n",
            "loss:8658.74960876844 at iteration 160\n",
            "loss:8646.831835334684 at iteration 161\n",
            "loss:8618.930031992906 at iteration 162\n",
            "loss:8590.360320300591 at iteration 163\n",
            "loss:8596.02051225142 at iteration 164\n",
            "loss:8572.442034250282 at iteration 165\n",
            "loss:8569.196579984562 at iteration 166\n",
            "loss:8549.12628900437 at iteration 167\n",
            "loss:8515.982666015625 at iteration 168\n",
            "loss:8481.535143324909 at iteration 169\n",
            "loss:8477.157904730902 at iteration 170\n",
            "loss:8478.783315259358 at iteration 171\n",
            "loss:8445.389292810694 at iteration 172\n",
            "loss:8404.783293625404 at iteration 173\n",
            "loss:8412.81807547433 at iteration 174\n",
            "loss:8420.054649353027 at iteration 175\n",
            "loss:8426.829764328433 at iteration 176\n",
            "loss:8429.396945910506 at iteration 177\n",
            "loss:8413.666979230316 at iteration 178\n",
            "loss:8396.279349093968 at iteration 179\n",
            "loss:8378.217606855362 at iteration 180\n",
            "loss:8356.291625305847 at iteration 181\n",
            "loss:8335.0595336247 at iteration 182\n",
            "loss:8312.510190880817 at iteration 183\n",
            "loss:8309.550766073691 at iteration 184\n",
            "loss:8287.580553936701 at iteration 185\n",
            "loss:8268.202300535804 at iteration 186\n",
            "loss:8258.485580768991 at iteration 187\n",
            "loss:8277.760985682251 at iteration 188\n",
            "loss:8269.459984709087 at iteration 189\n",
            "loss:8253.601568252005 at iteration 190\n",
            "loss:8249.259649276733 at iteration 191\n",
            "loss:8238.79641518074 at iteration 192\n",
            "loss:8234.57027285861 at iteration 193\n",
            "loss:8226.950496419271 at iteration 194\n",
            "loss:8201.154245181959 at iteration 195\n",
            "loss:8174.159632648913 at iteration 196\n",
            "loss:8170.501205290207 at iteration 197\n",
            "loss:8164.370629392078 at iteration 198\n",
            "loss:8153.507772827148 at iteration 199\n",
            "loss:8139.761101110657 at iteration 200\n",
            "loss:8126.297913806273 at iteration 201\n",
            "loss:8129.942478424223 at iteration 202\n",
            "loss:8136.862648758234 at iteration 203\n",
            "loss:8140.26235172923 at iteration 204\n",
            "loss:8136.901973391042 at iteration 205\n",
            "loss:8122.05772746358 at iteration 206\n",
            "loss:8104.155195382925 at iteration 207\n",
            "loss:8090.949659721703 at iteration 208\n",
            "loss:8080.669044712612 at iteration 209\n",
            "loss:8061.842009196349 at iteration 210\n",
            "loss:8053.3990035147035 at iteration 211\n",
            "loss:8050.95498127109 at iteration 212\n",
            "loss:8047.052423494999 at iteration 213\n",
            "loss:8033.331231263626 at iteration 214\n",
            "loss:8023.969988222475 at iteration 215\n",
            "loss:8018.510111584641 at iteration 216\n",
            "loss:8014.401312871811 at iteration 217\n",
            "loss:8005.366370910923 at iteration 218\n",
            "loss:7995.178193248402 at iteration 219\n",
            "loss:7996.002458529235 at iteration 220\n",
            "loss:7987.512683765308 at iteration 221\n",
            "loss:7995.8209228515625 at iteration 222\n",
            "loss:7993.284233638218 at iteration 223\n",
            "loss:7979.778651801215 at iteration 224\n",
            "loss:7972.899493462217 at iteration 225\n",
            "loss:7971.775465372901 at iteration 226\n",
            "loss:7969.645224587959 at iteration 227\n",
            "loss:7968.880560329388 at iteration 228\n",
            "loss:7966.033871327276 at iteration 229\n",
            "loss:7965.126358626725 at iteration 230\n",
            "loss:7963.2555936616045 at iteration 231\n",
            "loss:7957.24930582333 at iteration 232\n",
            "loss:7953.675895495292 at iteration 233\n",
            "loss:7951.864645840259 at iteration 234\n",
            "loss:7937.683810476529 at iteration 235\n",
            "loss:7941.877350239814 at iteration 236\n",
            "loss:7936.405539632845 at iteration 237\n",
            "loss:7926.375594007421 at iteration 238\n",
            "loss:7914.1117670694985 at iteration 239\n",
            "loss:7898.813295736352 at iteration 240\n",
            "loss:7883.955890750097 at iteration 241\n",
            "loss:7869.5872250152715 at iteration 242\n",
            "loss:7857.538855880987 at iteration 243\n",
            "loss:7854.050073242188 at iteration 244\n",
            "loss:7864.344124646691 at iteration 245\n",
            "loss:7873.919832422665 at iteration 246\n",
            "loss:7881.485108898532 at iteration 247\n",
            "loss:7878.486525692614 at iteration 248\n",
            "loss:7885.691536621094 at iteration 249\n",
            "loss:7893.256510578779 at iteration 250\n",
            "loss:7899.085462297712 at iteration 251\n",
            "loss:7903.0647672268715 at iteration 252\n",
            "loss:7903.379972683163 at iteration 253\n",
            "loss:7900.880531460632 at iteration 254\n",
            "loss:7902.076554775238 at iteration 255\n",
            "loss:7900.114316235256 at iteration 256\n",
            "loss:7886.195242002029 at iteration 257\n",
            "loss:7877.204979620385 at iteration 258\n",
            "loss:7875.71112107497 at iteration 259\n",
            "loss:7862.6320262923555 at iteration 260\n",
            "loss:7858.0678929918595 at iteration 261\n",
            "loss:7846.582769705768 at iteration 262\n",
            "loss:7828.851279981209 at iteration 263\n",
            "loss:7817.078005693544 at iteration 264\n",
            "loss:7817.872283706091 at iteration 265\n",
            "loss:7800.741349466731 at iteration 266\n",
            "loss:7796.627218673479 at iteration 267\n",
            "loss:7797.712746772624 at iteration 268\n",
            "loss:7800.964562988282 at iteration 269\n",
            "loss:7802.328652469874 at iteration 270\n",
            "loss:7813.230326484231 at iteration 271\n",
            "loss:7820.739441588685 at iteration 272\n",
            "loss:7809.803434274492 at iteration 273\n",
            "loss:7810.241254882812 at iteration 274\n",
            "loss:7817.226560288581 at iteration 275\n",
            "loss:7825.694576993315 at iteration 276\n",
            "loss:7820.564962044036 at iteration 277\n",
            "loss:7818.506711242019 at iteration 278\n",
            "loss:7813.659398978097 at iteration 279\n",
            "loss:7809.575667520435 at iteration 280\n",
            "loss:7793.899273378629 at iteration 281\n",
            "loss:7780.337273371936 at iteration 282\n",
            "loss:7764.794143461845 at iteration 283\n",
            "loss:7757.362120082922 at iteration 284\n",
            "loss:7743.411218603174 at iteration 285\n",
            "loss:7733.928195860328 at iteration 286\n",
            "loss:7720.052163441976 at iteration 287\n",
            "loss:7706.747451729428 at iteration 288\n",
            "loss:7698.172181017646 at iteration 289\n",
            "loss:7687.312749593938 at iteration 290\n",
            "loss:7673.710379404564 at iteration 291\n",
            "loss:7657.130829794822 at iteration 292\n",
            "loss:7642.283785657818 at iteration 293\n",
            "loss:7636.410584530588 at iteration 294\n",
            "loss:7625.76608482567 at iteration 295\n",
            "loss:7620.063300238715 at iteration 296\n",
            "loss:7621.686152720611 at iteration 297\n",
            "loss:7620.221413092469 at iteration 298\n",
            "loss:7617.939560139974 at iteration 299\n",
            "loss:7608.895264077424 at iteration 300\n",
            "loss:7594.05281481206 at iteration 301\n",
            "loss:7581.772715149933 at iteration 302\n",
            "loss:7567.411680924265 at iteration 303\n",
            "loss:7555.277575483478 at iteration 304\n",
            "loss:7550.531160242417 at iteration 305\n",
            "loss:7540.825998113675 at iteration 306\n",
            "loss:7540.2279952408435 at iteration 307\n",
            "loss:7541.386367550946 at iteration 308\n",
            "loss:7535.956977302797 at iteration 309\n",
            "loss:7534.333748477065 at iteration 310\n",
            "loss:7535.250401814778 at iteration 311\n",
            "loss:7529.290495753669 at iteration 312\n",
            "loss:7514.88428861776 at iteration 313\n",
            "epoch:  5\n",
            "loss:9980.7001953125 at iteration 0\n",
            "loss:8346.93603515625 at iteration 1\n",
            "loss:8062.903483072917 at iteration 2\n",
            "loss:7846.54150390625 at iteration 3\n",
            "loss:8243.251953125 at iteration 4\n",
            "loss:9020.68408203125 at iteration 5\n",
            "loss:9697.513113839286 at iteration 6\n",
            "loss:9934.15771484375 at iteration 7\n",
            "loss:9981.450520833334 at iteration 8\n",
            "loss:9615.487890625 at iteration 9\n",
            "loss:9315.52889737216 at iteration 10\n",
            "loss:9207.7763671875 at iteration 11\n",
            "loss:9524.386868990385 at iteration 12\n",
            "loss:9278.745535714286 at iteration 13\n",
            "loss:9337.653841145833 at iteration 14\n",
            "loss:9704.89697265625 at iteration 15\n",
            "loss:10016.296185661764 at iteration 16\n",
            "loss:9804.744845920139 at iteration 17\n",
            "loss:9893.429636101973 at iteration 18\n",
            "loss:10021.4494140625 at iteration 19\n",
            "loss:9844.44617280506 at iteration 20\n",
            "loss:9604.923140092329 at iteration 21\n",
            "loss:9405.533946161684 at iteration 22\n",
            "loss:9215.018208821615 at iteration 23\n",
            "loss:9141.1169921875 at iteration 24\n",
            "loss:9025.39071890024 at iteration 25\n",
            "loss:8942.54197410301 at iteration 26\n",
            "loss:8718.9130859375 at iteration 27\n",
            "loss:8746.528017241379 at iteration 28\n",
            "loss:8735.46123046875 at iteration 29\n",
            "loss:8746.376953125 at iteration 30\n",
            "loss:8748.690307617188 at iteration 31\n",
            "loss:8724.673029119318 at iteration 32\n",
            "loss:8708.296501608456 at iteration 33\n",
            "loss:8695.8013671875 at iteration 34\n",
            "loss:8590.277506510416 at iteration 35\n",
            "loss:8628.653689822635 at iteration 36\n",
            "loss:8757.345548930922 at iteration 37\n",
            "loss:8634.24385892428 at iteration 38\n",
            "loss:8545.703924560547 at iteration 39\n",
            "loss:8522.394203744283 at iteration 40\n",
            "loss:8472.493937174479 at iteration 41\n",
            "loss:8397.649624136991 at iteration 42\n",
            "loss:8325.070451216265 at iteration 43\n",
            "loss:8277.188492838543 at iteration 44\n",
            "loss:8362.620727539062 at iteration 45\n",
            "loss:8467.486717711105 at iteration 46\n",
            "loss:8554.736384073893 at iteration 47\n",
            "loss:8655.643689213966 at iteration 48\n",
            "loss:8770.123393554688 at iteration 49\n",
            "loss:8935.362816904106 at iteration 50\n",
            "loss:9015.377248910758 at iteration 51\n",
            "loss:9089.37591667895 at iteration 52\n",
            "loss:9228.06672724971 at iteration 53\n",
            "loss:9357.702481356535 at iteration 54\n",
            "loss:9302.483873639789 at iteration 55\n",
            "loss:9195.191397683662 at iteration 56\n",
            "loss:9078.901472420528 at iteration 57\n",
            "loss:9193.951440843484 at iteration 58\n",
            "loss:9286.933369954428 at iteration 59\n",
            "loss:9202.51285940702 at iteration 60\n",
            "loss:9130.049328219506 at iteration 61\n",
            "loss:9081.768760075645 at iteration 62\n",
            "loss:9055.047298431396 at iteration 63\n",
            "loss:9013.869557542068 at iteration 64\n",
            "loss:8968.604658647017 at iteration 65\n",
            "loss:8940.975152314599 at iteration 66\n",
            "loss:8974.715134564569 at iteration 67\n",
            "loss:9015.225773465807 at iteration 68\n",
            "loss:9033.680430385044 at iteration 69\n",
            "loss:8968.706236933318 at iteration 70\n",
            "loss:8936.610883924695 at iteration 71\n",
            "loss:8916.775962516052 at iteration 72\n",
            "loss:8905.282348632812 at iteration 73\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5d72891c49f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print(\"output\", output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(\"output shape after concat: \", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mconv_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(\"conv output\", conv_outputs[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9e38b6713688>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prints)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# if prints: print(\"checking reshaping: \", x[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flatten last two dims: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final output: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2150\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m     )\n\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLBL0hNWr8r-"
      },
      "source": [
        "# def evaluate(model, data_loader,  device='cuda'):\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "#     x_list = []\n",
        "#     y_list = []\n",
        "#     floor_list = []\n",
        "#     prexs_list = []\n",
        "#     preys_list = []\n",
        "#     prefloors_list = []\n",
        "#     for d in tqdm(data_loader):\n",
        "#         data_dict['BSSID_FEATS'] = d['BSSID_FEATS'].to(device).long()\n",
        "#         data_dict['RSSI_FEATS'] = d['RSSI_FEATS'].to(device).float()\n",
        "#         data_dict['site_id'] = d['site_id'].to(device).long()\n",
        "#         x = d['x'].to(device).float()\n",
        "#         y = d['y'].to(device).float()\n",
        "#         floor = d['floor'].to(device).long()\n",
        "#         x_list.append(x.cpu().detach().numpy())\n",
        "#         y_list.append(y.cpu().detach().numpy())\n",
        "#         floor_list.append(floor.cpu().detach().numpy())\n",
        "#         xy, floor = model(data_dict)\n",
        "#         prexs_list.append(xy[:, 0].cpu().detach().numpy())\n",
        "#         preys_list.append(xy[:, 1].cpu().detach().numpy())\n",
        "#         prefloors_list.append(floor.squeeze().cpu().detach().numpy())\n",
        "#     x = np.concatenate(x_list)\n",
        "#     y = np.concatenate(y_list)\n",
        "#     floor = np.concatenate(floor_list)\n",
        "#     prexs = np.concatenate(prexs_list)\n",
        "#     preys =np.concatenate(preys_list)\n",
        "#     prefloors = np.concatenate(prefloors_list)\n",
        "#     eval_score = comp_metric(x, y, floor, prexs, preys, prefloors)\n",
        "#     return eval_score\n",
        "\n",
        "# def get_result(model, data_loader, device='cuda'):\n",
        "#     model.eval()\n",
        "#     model.to(device)\n",
        "#     prexs_list = []\n",
        "#     preys_list = []\n",
        "#     prefloors_list = []\n",
        "#     data_dict = {}\n",
        "#     for d in tqdm(data_loader):\n",
        "#         data_dict['BSSID_FEATS'] = d['BSSID_FEATS'].to(device).long()\n",
        "#         data_dict['RSSI_FEATS'] = d['RSSI_FEATS'].to(device).float()\n",
        "#         data_dict['site_id'] = d['site_id'].to(device).long()\n",
        "#         xy, floor = model(data_dict)\n",
        "#         prexs_list.append(xy[:, 0].cpu().detach().numpy())\n",
        "#         preys_list.append(xy[:, 1].cpu().detach().numpy())\n",
        "#         prefloors_list.append(floor.squeeze(-1).cpu().detach().numpy())\n",
        "#     prexs = np.concatenate(prexs_list)\n",
        "#     preys =np.concatenate(preys_list)\n",
        "#     prefloors = np.concatenate(prefloors_list)\n",
        "#     return prexs, preys, prefloors"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}