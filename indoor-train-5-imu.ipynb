{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport json\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n\nfrom PIL import Image, ImageOps\nfrom skimage import io\nfrom skimage.color import rgba2rgb, rgb2xyz\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nfrom math import floor, ceil\nimport random\n\n# Train data generation\nimport collections\nimport csv\nfrom pathlib import Path\nfrom typing import List, Tuple, Any\n\nimport time\nimport re\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nimport multiprocessing\nfrom multiprocessing import Pool, Manager\n\nimport pickle\nimport math\nimport gc\nimport psutil\n\npd.set_option(\"display.max_columns\", 100)","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Settings and altering components for GCP\n\n# path settings\nroot_path = \"../input/indoor-location-navigation/\"\n# root_path = \"../jupyter/input/\"\ntrain_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\ntest_paths = glob.glob(root_path + \"test\" + \"/*\")\nmetafiles = glob.glob(root_path + \"metadata\" + \"/*\")\n\n# function imports using github repo in kaggle kernels\n# https://www.kaggle.com/getting-started/71642\n!cp -r ../input/indoorlocationcompetition20master/indoor-location-competition-20-master/* ./\nfrom io_f import read_data_file\nfrom compute_f import compute_step_positions, compute_steps, \\\ncompute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq\n\n# import for gcp settings\n# import compute_f\n# import io_f\n# import visualize_f\n# import main\n# from io_f import read_data_file\n# from compute_f import compute_step_positions, compute_steps, \\\n# compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions, split_ts_seq","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make directory for saving files\n!mkdir train\n!mkdir test","execution_count":107,"outputs":[{"output_type":"stream","text":"mkdir: cannot create directory ‘train’: File exists\nmkdir: cannot create directory ‘test’: File exists\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./test","execution_count":151,"outputs":[{"output_type":"stream","text":"5a0546857ecc773753327266_train.csv  5cd56c17e2acfd2d33b6c19b_train.csv\r\n5c3c44b80379370013e0fd2b_train.csv  5cd56c18e2acfd2d33b6c321_train.csv\r\n5cd56865eb294480de7167b6_train.csv  5cd56c1be2acfd2d33b6c766_train.csv\r\n5cd56b5ae2acfd2d33b58548_train.csv  5cd56c1ee2acfd2d33b6ceab_train.csv\r\n5cd56b5ae2acfd2d33b58549_train.csv  5cd56c28e2acfd2d33b6d7f3_train.csv\r\n5cd56b64e2acfd2d33b592b3_train.csv  5cd969ad39e2fc0b4afe67ed_train.csv\r\n5cd56b64e2acfd2d33b5932f_train.csv  5cd969ae39e2fc0b4afe68c3_train.csv\r\n5cd56b6ee2acfd2d33b5a247_train.csv  5cd969b639e2fc0b4afe6db2_train.csv\r\n5cd56b6fe2acfd2d33b5a386_train.csv  5cd969b839e2fc0b4afe6edc_train.csv\r\n5cd56b75e2acfd2d33b5af29_train.csv  5cd969bc39e2fc0b4afe71ad_train.csv\r\n5cd56b77e2acfd2d33b5b22b_train.csv  5cd969bd39e2fc0b4afe727d_train.csv\r\n5cd56b79e2acfd2d33b5b74e_train.csv  5cd969c739e2fc0b4afe7d60_train.csv\r\n5cd56b79e2acfd2d33b5b77c_train.csv  5cd969c839e2fc0b4afe7ff0_train.csv\r\n5cd56b7de2acfd2d33b5c14b_train.csv  5cd969d339e2fc0b4afe90f4_train.csv\r\n5cd56b86e2acfd2d33b5cf97_train.csv  5cd969db39e2fc0b4afe9bc2_train.csv\r\n5cd56b8be2acfd2d33b5db68_train.csv  5cd969ea39e2fc0b4afeaef3_train.csv\r\n5cd56b8de2acfd2d33b5dd26_train.csv  5cd969ef39e2fc0b4afeb42f_train.csv\r\n5cd56b91e2acfd2d33b5e4b1_train.csv  5cd969f139e2fc0b4afeb794_train.csv\r\n5cd56b96e2acfd2d33b5ef55_train.csv  5cd969f239e2fc0b4afeb9cd_train.csv\r\n5cd56b9be2acfd2d33b5fa12_train.csv  5cdac61de403deddaf467f30_train.csv\r\n5cd56ba0e2acfd2d33b600ed_train.csv  5cdac61fe403deddaf467f91_train.csv\r\n5cd56ba1e2acfd2d33b60372_train.csv  5cdac620e403deddaf467ff9_train.csv\r\n5cd56ba1e2acfd2d33b60565_train.csv  5cdac621e403deddaf468018_train.csv\r\n5cd56ba5e2acfd2d33b60e03_train.csv  5cdac622e403deddaf46803a_train.csv\r\n5cd56babe2acfd2d33b61827_train.csv  5cdac622e403deddaf46805a_train.csv\r\n5cd56bace2acfd2d33b618fe_train.csv  5cdac624e403deddaf4680c2_train.csv\r\n5cd56baee2acfd2d33b61a93_train.csv  5cdac625e403deddaf4680db_train.csv\r\n5cd56bb5e2acfd2d33b62b37_train.csv  5cdac625e403deddaf4680e6_train.csv\r\n5cd56bb7e2acfd2d33b62f0b_train.csv  5cdac626e403deddaf4680f4_train.csv\r\n5cd56bb9e2acfd2d33b633ea_train.csv  5d27099f03f801723c32511d_train.csv\r\n5cd56bc0e2acfd2d33b63f9b_train.csv  5d2709b303f801723c327472_train.csv\r\n5cd56bc2e2acfd2d33b640cc_train.csv  5d2709bb03f801723c32852c_train.csv\r\n5cd56bc2e2acfd2d33b64221_train.csv  5d2709c303f801723c3299ee_train.csv\r\n5cd56bc4e2acfd2d33b6455c_train.csv  5d2709d403f801723c32bd39_train.csv\r\n5cd56bcbe2acfd2d33b6526b_train.csv  5d2709dd03f801723c32cfb6_train.csv\r\n5cd56bd8e2acfd2d33b66008_train.csv  5d2709e003f801723c32d896_train.csv\r\n5cd56bdbe2acfd2d33b663c0_train.csv  5da138274db8ce0c98bbd3d2_train.csv\r\n5cd56be5e2acfd2d33b66e3f_train.csv  5da138314db8ce0c98bbf3a0_train.csv\r\n5cd56c01e2acfd2d33b698ba_train.csv  5da1383b4db8ce0c98bc11ab_train.csv\r\n5cd56c03e2acfd2d33b69c1f_train.csv  5da138764db8ce0c98bcaa46_train.csv\r\n5cd56c09e2acfd2d33b6a75b_train.csv  5da1389e4db8ce0c98bd0547_train.csv\r\n5cd56c0ce2acfd2d33b6ab27_train.csv  5da138b74db8ce0c98bd4774_train.csv\r\n5cd56c10e2acfd2d33b6b348_train.csv  5da958dd46f8266d0737457b_train.csv\r\n5cd56c11e2acfd2d33b6b3c8_train.csv  5dc8cea7659e181adb076a3f_train.csv\r\n5cd56c16e2acfd2d33b6bd44_train.csv\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter milisecond setting \nIMU_CUT = 250\nWPS_CUT = 5000\n\n# train number setting\n# TRAIN_NUM = len(train_paths)\n# TRAIN_NUM = round(len(train_paths) / 2)\nTRAIN_NUM = 10\n\n# floor translation\nFLOOR_MAP = {\"B3\":-3,\"B2\":-2,\"B1\":-1,\"F1\":0,\"1F\":0,\"F2\":1,\"2F\":1,\"F3\":2,\"3F\":2,\"F4\":3,\"4F\":3,\n             \"F5\":4,\"5F\":4,\"F6\":5,\"6F\":5,\"F7\":6,\"7F\":6,\"F8\":7,\"8F\": 7,\"F9\":8,\"9F\":8,\"F10\":9,\n             \"B\":0,\"BF\":1,\"BM\":2, \"G\":0, \"M\":0, \"P1\":0,\"P2\":1, \"LG2\":-2,\"LG1\":-1,\"LG\":0,\"LM\":0,\n             \"L1\":1,\"L2\":2,\"L3\":3,\"L4\":4,\"L5\":5,\"L6\":6,\"L7\":7,\"L8\":8,\"L9\":9,\"L10\":10,\"L11\":11}\n\n# Columns to shift to the beginning of df\nSHIFT_COLS = [\"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n              \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n              \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n\nSHIFT_COLS_TEST = [\"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n                   \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n                   \"y\", \"x\", \"wps_diff\", \"wifi_ts\", \"site_path_timestamp\"]\n\nINT_COLS = [\"wifi_ts\"]\nCAT_COLS = [\"file_id\", \"site_id\", \"floor\"]","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess\nprint(\"No. Files in Train: {:,}\".format(len(train_paths)), \"\\n\" +\n      \"No. Files in Test: {:,}\".format(len(test_paths)), \"\\n\" +\n      \"No. of metadata files: {:,}\".format(len(metafiles)))\n\n# Reading in 1 file\ndef pick_example(max_range, paths):\n    ex = random.randint(0, max_range)\n    example_path = paths[ex]\n    path = f\"{example_path}\"\n    paths = path.split(\"/\")\n    site = paths[4]\n    floorNo = paths[5]\n    floor_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_image.png\"\n    json_plan_filename = f\"{root_path}metadata/{site}/{floorNo}/floor_info.json\"\n    with open(json_plan_filename) as json_file:\n        json_data = json.load(json_file)\n    width_meter = json_data[\"map_info\"][\"width\"]\n    height_meter = json_data[\"map_info\"][\"height\"]\n    return path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter\n\npath, site, floorNo, floor_plan_filename, \\\njson_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\nprint(\"example path: \", path)\nprint(\"site: \", site)\nprint(\"floorNo: \", floorNo)\nprint(\"floor_plan_filename: \", floor_plan_filename)\nprint(\"json_plan_filename: \", json_plan_filename)\nprint(\"width: {}, height: {} \".format(width_meter, height_meter))\n\nwith open(path) as p:\n    lines = p.readlines()\nprint(\"No. Lines in 1 example: {:,}\". format(len(lines)))","execution_count":110,"outputs":[{"output_type":"stream","text":"No. Files in Train: 26,925 \nNo. Files in Test: 626 \nNo. of metadata files: 204\nexample path:  ../input/indoor-location-navigation/train/5cd56b5ae2acfd2d33b58549/5F/5d061c804a19c000086c4404.txt\nsite:  5cd56b5ae2acfd2d33b58549\nfloorNo:  5F\nfloor_plan_filename:  ../input/indoor-location-navigation/metadata/5cd56b5ae2acfd2d33b58549/5F/floor_image.png\njson_plan_filename:  ../input/indoor-location-navigation/metadata/5cd56b5ae2acfd2d33b58549/5F/floor_info.json\nwidth: 186.0957334716654, height: 181.78477159338266 \nNo. Lines in 1 example: 15,570\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for line in lines:\n#     print(line)","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Redefine the data extraction class\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass ReadData:\n    acce: np.ndarray\n    acce_uncali: np.ndarray\n    gyro: np.ndarray\n    gyro_uncali: np.ndarray\n    magn: np.ndarray\n    magn_uncali: np.ndarray\n    ahrs: np.ndarray\n    wifi: np.ndarray\n    ibeacon: np.ndarray\n    waypoint: np.ndarray\n\n\ndef read_data_file_ed(data_filename):\n    acce = []\n    acce_uncali = []\n    gyro = []\n    gyro_uncali = []\n    magn = []\n    magn_uncali = []\n    ahrs = []\n    wifi = []\n    ibeacon = []\n    waypoint = []\n\n    with open(data_filename, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    for line_data in lines:\n        line_data = line_data.strip()\n        if not line_data or line_data[0] == '#':\n            continue\n\n        line_data = line_data.split('\\t')\n\n        if line_data[1] == 'TYPE_ACCELEROMETER':\n            acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_GYROSCOPE':\n            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_ROTATION_VECTOR':\n            ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n            continue\n\n        if line_data[1] == 'TYPE_WIFI':\n            sys_ts = line_data[0]\n            ssid = line_data[2]\n            bssid = line_data[3]\n            rssi = line_data[4]\n            lastseen_ts = line_data[6]\n            wifi_data = [sys_ts, ssid, bssid, '_'.join([ssid, bssid]), rssi, lastseen_ts]\n            wifi.append(wifi_data)\n            continue\n\n        if line_data[1] == 'TYPE_BEACON':\n            ts = line_data[0]\n            uuid = line_data[2]\n            major = line_data[3]\n            minor = line_data[4]\n            txpower = line_data[5]\n            rssi = line_data[6]\n            distance = line_data[7]\n            mac_address = line_data[-2]\n            beacon_ts = line_data[-1]\n            ibeacon_data = [ts, '_'.join([uuid, major, minor]), txpower, rssi, distance, mac_address, beacon_ts]\n            ibeacon.append(ibeacon_data)\n            continue\n\n        if line_data[1] == 'TYPE_WAYPOINT':\n            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n\n    acce = np.array(acce)\n    acce_uncali = np.array(acce_uncali)\n    gyro = np.array(gyro)\n    gyro_uncali = np.array(gyro_uncali)\n    magn = np.array(magn)\n    magn_uncali = np.array(magn_uncali)\n    ahrs = np.array(ahrs)\n    wifi = np.array(wifi)\n    ibeacon = np.array(ibeacon)\n    waypoint = np.array(waypoint)\n\n    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find out how many wps datapoints and wifi datapoints one floor has\ntrain_path_floor = glob.glob(root_path + \"train\" + \"/*/*/\")\n# train_paths = glob.glob(root_path + \"train\" + \"/*/*/*\")\nex = random.randint(0, 6)\nprint(train_path_floor[ex])\nprint(\"no. of files of that floor: \", len(os.listdir(train_path_floor[ex])))\ncount = 0\nfor f in os.listdir(train_path_floor[ex]):\n    file_path = train_path_floor[ex] + f\n    data = read_data_file_ed(file_path)\n    count += len(data.waypoint)\n    \nprint(count)","execution_count":113,"outputs":[{"output_type":"stream","text":"../input/indoor-location-navigation/train/5cd56c0ce2acfd2d33b6ab27/F3/\nno. of files of that floor:  6\n32\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path, site, floorNo, floor_plan_filename, json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n# show_site_png(root_path, site=site)","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature candidate\n# You can't get the waypoint in test, so use acce and ahrs data to calculate relative positions\ndef calc_rel_positions(acce_datas, ahrs_datas):\n    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n    headings = compute_headings(ahrs_datas)\n    stride_lengths = compute_stride_length(step_acce_max_mins)\n    step_headings = compute_step_heading(step_timestamps, headings)\n    rel_positions = compute_rel_positions(stride_lengths, step_headings)\n    # only use del if we don't need timestamps\n    # rel_positions_del = np.delete(rel_positions, 0, 1)\n    return rel_positions\n\n# Feature candidate\n# Modify extract_magnetic_strength from github for one magnetic data point\ndef extract_one_magn_strength(magn_datas):\n    d = np.array(magn_datas[2:5])\n    return np.mean(np.sqrt(np.sum(d ** 2, axis=0)))","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path, site, floorNo, floor_plan_filename, \\\njson_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common methods\ndef extract_imu_rep(imu_data, wifi_ts):\n    imu_ts = imu_data[:, 0].astype(int)\n    diff_list = []\n    for ts in imu_ts:\n        diff = abs(int(wifi_ts) - ts)\n        diff_list.append(diff)\n    # diff_idx = np.argmin(diff_list)\n    # acce_diff_range = [(i,a) for i, a in enumerate(diff_list) if a < cut_line] # uncomment if we need to check acce_diff\n    acce_diff_range = [i for i, a in enumerate(diff_list) if a < IMU_CUT]\n    # print(len(acce_diff_range))\n    imu_filtered = imu_data[acce_diff_range]\n    # print(int(wifi_ts))\n    # print(imu_filtered[:, 0].astype(int)) # Check if it's taking 250ms acce correctly\n    # print(imu_filtered)\n    # print(imu_filtered[:, 1]) # Check if we're taking x values correctly\n    imu_avg_x = imu_filtered[:, 1].mean()\n    imu_avg_y = imu_filtered[:, 2].mean()\n    imu_avg_z = imu_filtered[:, 3].mean()\n    #print(imu_avg_x, imu_avg_y, imu_avg_z)\n    return imu_avg_x, imu_avg_y, imu_avg_z\n\ndef shift_columns(cols, df):\n    for col in cols:\n        df_cols = list(df.columns)\n        df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n        df = df[df_cols]\n    return df\n\n# convert data types of certain columns\ndef convert_dtypes(df, col_list, dtype):\n    for col in col_list:\n        df[col] = df[col].astype(dtype)","execution_count":117,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Train generator\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train specific methods\ndef extract_nearest_wps(wps_data, wifi_ts):\n    wps_ts = wps_data[:, 0].astype(int)\n    diff_list = []\n    for ts in wps_ts:\n        diff = abs(int(wifi_ts) - ts)\n        diff_list.append(diff)\n    diff_idx = np.argmin(diff_list)\n    return diff_list[diff_idx], wps_data[diff_idx]\n\ndef extract_train_path(path):\n    try:\n        ex_path = f\"{path}\"\n        ex_paths = ex_path.split(\"/\")\n        site_id = ex_paths[4]\n        floor = ex_paths[5]\n        f = FLOOR_MAP[floor]\n        file_id = ex_paths[6].split(\".\")[0]\n        return site_id, file_id, f, floor\n    except:\n        print(\"extract_path error\")\n\ndef make_wifi_df_train(path):\n    # First path\n    datas = read_data_file_ed(path)\n    acce_datas = datas.acce\n    magn_datas = datas.magn\n    ahrs_datas = datas.ahrs\n    wifi_datas = datas.wifi\n    ibeacon_datas = datas.ibeacon\n    wps = datas.waypoint # not to be used\n\n    # acce and ahrs data translation\n    # rel_positions = calc_rel_positions(acce_datas, ahrs_datas)\n\n    # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n\n    # Make wifi df with wifi_ts\n    dfs = []\n    df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n    for wifi_ts, g in df.groupby(0):\n        g = g.drop_duplicates(subset=1)\n        tmp = g.iloc[:,1:]\n        feat = tmp.set_index(1).T\n        feat[\"wifi_ts\"] = wifi_ts\n\n        # get closest wps\n        closest_wps = extract_nearest_wps(wps, wifi_ts)\n        feat[\"wps_diff\"] = closest_wps[0]\n        feat[\"x\"] = closest_wps[1][1]\n        feat[\"y\"] = closest_wps[1][2]\n        \n        # get average of acce within 250ms\n        acce_avgs = extract_imu_rep(acce_datas, wifi_ts)\n        feat[\"acce_x_avg\"] = acce_avgs[0]\n        feat[\"acce_y_avg\"] = acce_avgs[1]\n        feat[\"acce_z_avg\"] = acce_avgs[2]\n        \n        # get floor and other path data\n        site_id, file_id, f, floor = extract_train_path(path)\n        feat[\"site_id\"] = site_id\n        feat[\"file_id\"] = file_id\n        feat[\"floor_int\"] = f\n        feat[\"floor\"] = floor\n        \n        dfs.append(feat)\n    \n    return dfs\n\n\ndef make_train_df(paths_df, site_list):\n    for site in site_list:\n        df = paths_df[paths_df[\"site_id\"] == site]\n        paths = df[\"path\"].unique()\n        dfs_all = pool.map(make_wifi_df_train, tqdm(paths))\n        dfs_unpack = [row for df in dfs_all for row in df]\n        wifi_df = pd.concat(dfs_unpack)\n        wifi_df = shift_columns(SHIFT_COLS, wifi_df)\n        wifi_df = wifi_df.fillna(-999)\n        convert_dtypes(wifi_df, tqdm(INT_COLS), int)\n        convert_dtypes(wifi_df, tqdm(CAT_COLS), \"category\")\n        wifi_df.to_csv(f\"./train/{site}_train.csv\", index=False)\n        del wifi_df","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_path filtering\n\ndef extract_path_for_grouplist(path):\n    ex_path = f\"{path}\"\n    ex_paths = ex_path.split(\"/\")\n    site_id = ex_paths[4]\n    file_id = ex_paths[6].split(\".\")[0]\n    return [path, site_id, file_id]\n\n# create pathlist to be used by 2 types of paths list\npath_list = [extract_path_for_grouplist(item) for item in train_paths]\ndf_paths = pd.DataFrame(path_list, columns=[\"path\", \"site_id\", \"file_id\"])\nsite_id_path_list = df_paths[\"site_id\"].unique()\n\n# grouped_paths_list -> It takes 3 records from every site_id\ngrouped_paths_df = df_paths.groupby(\"site_id\").sample(n=3)\ngrouped_paths_list = list(grouped_paths_df[\"path\"].unique())\ndisplay(grouped_paths_df.head())\nprint(len(df_paths))","execution_count":150,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                    path  \\\n10656  ../input/indoor-location-navigation/train/5a05...   \n10588  ../input/indoor-location-navigation/train/5a05...   \n10891  ../input/indoor-location-navigation/train/5a05...   \n25371  ../input/indoor-location-navigation/train/5c3c...   \n25493  ../input/indoor-location-navigation/train/5c3c...   \n\n                        site_id                   file_id  \n10656  5a0546857ecc773753327266  5e15a27ef4c3420006d521c5  \n10588  5a0546857ecc773753327266  5e158f33f4c3420006d521ab  \n10891  5a0546857ecc773753327266  5d8f0955b6e29d0006fb8c0f  \n25371  5c3c44b80379370013e0fd2b  5d8c8058ba656a000636d4a8  \n25493  5c3c44b80379370013e0fd2b  5d0765b34cae4f000a2db652  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>site_id</th>\n      <th>file_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10656</th>\n      <td>../input/indoor-location-navigation/train/5a05...</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>5e15a27ef4c3420006d521c5</td>\n    </tr>\n    <tr>\n      <th>10588</th>\n      <td>../input/indoor-location-navigation/train/5a05...</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>5e158f33f4c3420006d521ab</td>\n    </tr>\n    <tr>\n      <th>10891</th>\n      <td>../input/indoor-location-navigation/train/5a05...</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>5d8f0955b6e29d0006fb8c0f</td>\n    </tr>\n    <tr>\n      <th>25371</th>\n      <td>../input/indoor-location-navigation/train/5c3c...</td>\n      <td>5c3c44b80379370013e0fd2b</td>\n      <td>5d8c8058ba656a000636d4a8</td>\n    </tr>\n    <tr>\n      <th>25493</th>\n      <td>../input/indoor-location-navigation/train/5c3c...</td>\n      <td>5c3c44b80379370013e0fd2b</td>\n      <td>5d0765b34cae4f000a2db652</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","text":"26925\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nnum_cores = multiprocessing.cpu_count()\npool = Pool(num_cores)\n\n# # Checking purposes\n# # 100 records:  288.2806177139282 sec\n# grouped_paths_df = grouped_paths_df.iloc[:9,:]\n# grouped_paths_df = grouped_paths_df.sample(n=100)\n# train_sites_list = grouped_paths_df[\"site_id\"].unique()\n# make_train_df(grouped_paths_df, train_sites_list)\n\n# REAL training\ntrain_sites_list = df_paths[\"site_id\"].unique()\nmake_train_df(df_paths, train_sites_list)\n\nprint(\"time to extract data: \", time.time() - start)\npool.close()","execution_count":120,"outputs":[{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00, 345.67it/s]\n100%|██████████| 1/1 [00:00<00:00, 940.64it/s]\n100%|██████████| 3/3 [00:00<00:00, 925.35it/s]\n100%|██████████| 1/1 [00:00<00:00, 3908.95it/s]\n100%|██████████| 1/1 [00:00<00:00, 729.95it/s]\n100%|██████████| 3/3 [00:00<00:00, 1053.49it/s]\n100%|██████████| 1/1 [00:00<00:00, 2267.19it/s]\n100%|██████████| 1/1 [00:00<00:00, 562.54it/s]\n100%|██████████| 3/3 [00:00<00:00, 1034.78it/s]\n100%|██████████| 2/2 [00:00<00:00, 7430.12it/s]\n100%|██████████| 1/1 [00:00<00:00, 424.78it/s]\n100%|██████████| 3/3 [00:00<00:00, 671.30it/s]\n100%|██████████| 1/1 [00:00<00:00, 2437.13it/s]\n100%|██████████| 1/1 [00:00<00:00, 394.65it/s]\n100%|██████████| 3/3 [00:00<00:00, 1026.34it/s]\n100%|██████████| 1/1 [00:00<00:00, 2853.27it/s]\n100%|██████████| 1/1 [00:00<00:00, 248.99it/s]\n100%|██████████| 3/3 [00:00<00:00, 582.43it/s]\n100%|██████████| 1/1 [00:00<00:00, 2993.79it/s]\n100%|██████████| 1/1 [00:00<00:00, 465.78it/s]\n100%|██████████| 3/3 [00:00<00:00, 948.58it/s]\n100%|██████████| 2/2 [00:00<00:00, 6892.86it/s]\n100%|██████████| 1/1 [00:00<00:00, 250.24it/s]\n100%|██████████| 3/3 [00:00<00:00, 708.34it/s]\n100%|██████████| 1/1 [00:00<00:00, 3248.88it/s]\n100%|██████████| 1/1 [00:00<00:00, 588.18it/s]\n100%|██████████| 3/3 [00:00<00:00, 941.13it/s]\n100%|██████████| 1/1 [00:00<00:00, 2087.76it/s]\n100%|██████████| 1/1 [00:00<00:00, 443.33it/s]\n100%|██████████| 3/3 [00:00<00:00, 849.05it/s]\n100%|██████████| 2/2 [00:00<00:00, 4373.62it/s]\n100%|██████████| 1/1 [00:00<00:00, 520.00it/s]\n100%|██████████| 3/3 [00:00<00:00, 865.64it/s]\n100%|██████████| 1/1 [00:00<00:00, 1814.93it/s]\n100%|██████████| 1/1 [00:00<00:00, 475.49it/s]\n100%|██████████| 3/3 [00:00<00:00, 742.05it/s]\n100%|██████████| 1/1 [00:00<00:00, 3041.55it/s]\n100%|██████████| 1/1 [00:00<00:00, 646.37it/s]\n100%|██████████| 3/3 [00:00<00:00, 954.05it/s]\n100%|██████████| 2/2 [00:00<00:00, 5457.78it/s]\n100%|██████████| 1/1 [00:00<00:00, 513.50it/s]\n100%|██████████| 3/3 [00:00<00:00, 1020.10it/s]\n100%|██████████| 3/3 [00:00<00:00, 7033.49it/s]\n100%|██████████| 1/1 [00:00<00:00, 430.27it/s]\n100%|██████████| 3/3 [00:00<00:00, 975.12it/s]\n100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]\n100%|██████████| 1/1 [00:00<00:00, 784.42it/s]\n100%|██████████| 3/3 [00:00<00:00, 920.01it/s]\n100%|██████████| 2/2 [00:00<00:00, 3288.36it/s]\n100%|██████████| 1/1 [00:00<00:00, 394.65it/s]\n100%|██████████| 3/3 [00:00<00:00, 745.74it/s]\n100%|██████████| 1/1 [00:00<00:00, 4084.04it/s]\n100%|██████████| 1/1 [00:00<00:00, 658.03it/s]\n100%|██████████| 3/3 [00:00<00:00, 1030.46it/s]\n100%|██████████| 1/1 [00:00<00:00, 4337.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 519.16it/s]\n100%|██████████| 3/3 [00:00<00:00, 973.23it/s]\n100%|██████████| 1/1 [00:00<00:00, 3480.75it/s]\n100%|██████████| 1/1 [00:00<00:00, 280.97it/s]\n100%|██████████| 3/3 [00:00<00:00, 719.68it/s]\n100%|██████████| 2/2 [00:00<00:00, 5363.56it/s]\n100%|██████████| 1/1 [00:00<00:00, 264.12it/s]\n100%|██████████| 3/3 [00:00<00:00, 793.37it/s]\n100%|██████████| 1/1 [00:00<00:00, 2053.01it/s]\n100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n100%|██████████| 3/3 [00:00<00:00, 916.19it/s]\n100%|██████████| 1/1 [00:00<00:00, 3070.50it/s]\n100%|██████████| 1/1 [00:00<00:00, 663.24it/s]\n100%|██████████| 3/3 [00:00<00:00, 882.95it/s]\n100%|██████████| 1/1 [00:00<00:00, 2943.37it/s]\n100%|██████████| 1/1 [00:00<00:00, 620.55it/s]\n100%|██████████| 3/3 [00:00<00:00, 808.67it/s]\n100%|██████████| 1/1 [00:00<00:00, 3554.49it/s]\n100%|██████████| 1/1 [00:00<00:00, 558.12it/s]\n100%|██████████| 3/3 [00:00<00:00, 936.23it/s]\n100%|██████████| 2/2 [00:00<00:00, 7598.38it/s]\n100%|██████████| 1/1 [00:00<00:00, 567.10it/s]\n100%|██████████| 3/3 [00:00<00:00, 925.08it/s]\n100%|██████████| 1/1 [00:00<00:00, 4324.02it/s]\n100%|██████████| 1/1 [00:00<00:00, 547.20it/s]\n100%|██████████| 3/3 [00:00<00:00, 839.31it/s]\n100%|██████████| 1/1 [00:00<00:00, 2322.43it/s]\n100%|██████████| 1/1 [00:00<00:00, 663.24it/s]\n100%|██████████| 3/3 [00:00<00:00, 855.63it/s]\n100%|██████████| 1/1 [00:00<00:00, 3276.80it/s]\n100%|██████████| 1/1 [00:00<00:00, 579.64it/s]\n100%|██████████| 3/3 [00:00<00:00, 1203.42it/s]\n100%|██████████| 1/1 [00:00<00:00, 3160.74it/s]\n100%|██████████| 1/1 [00:00<00:00, 426.94it/s]\n100%|██████████| 3/3 [00:00<00:00, 800.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 3093.14it/s]\n100%|██████████| 1/1 [00:00<00:00, 816.97it/s]\n100%|██████████| 3/3 [00:00<00:00, 832.97it/s]\n100%|██████████| 1/1 [00:00<00:00, 3258.98it/s]\n100%|██████████| 1/1 [00:00<00:00, 497.31it/s]\n100%|██████████| 3/3 [00:00<00:00, 617.45it/s]\n100%|██████████| 1/1 [00:00<00:00, 3068.25it/s]\n100%|██████████| 1/1 [00:00<00:00, 290.04it/s]\n100%|██████████| 3/3 [00:00<00:00, 968.36it/s]\n100%|██████████| 1/1 [00:00<00:00, 3352.76it/s]\n100%|██████████| 1/1 [00:00<00:00, 386.36it/s]\n100%|██████████| 3/3 [00:00<00:00, 931.17it/s]\n100%|██████████| 1/1 [00:00<00:00, 3004.52it/s]\n100%|██████████| 1/1 [00:00<00:00, 899.10it/s]\n100%|██████████| 3/3 [00:00<00:00, 1181.38it/s]\n100%|██████████| 1/1 [00:00<00:00, 3446.43it/s]\n100%|██████████| 1/1 [00:00<00:00, 801.82it/s]\n100%|██████████| 3/3 [00:00<00:00, 1158.11it/s]\n100%|██████████| 1/1 [00:00<00:00, 3477.86it/s]\n100%|██████████| 1/1 [00:00<00:00, 414.83it/s]\n100%|██████████| 3/3 [00:00<00:00, 1049.89it/s]\n100%|██████████| 1/1 [00:00<00:00, 3028.38it/s]\n100%|██████████| 1/1 [00:00<00:00, 824.84it/s]\n100%|██████████| 3/3 [00:00<00:00, 1094.36it/s]\n100%|██████████| 1/1 [00:00<00:00, 3916.25it/s]\n100%|██████████| 1/1 [00:00<00:00, 616.08it/s]\n100%|██████████| 3/3 [00:00<00:00, 902.58it/s]\n100%|██████████| 1/1 [00:00<00:00, 3320.91it/s]\n100%|██████████| 1/1 [00:00<00:00, 770.30it/s]\n100%|██████████| 3/3 [00:00<00:00, 1063.91it/s]\n100%|██████████| 1/1 [00:00<00:00, 2970.47it/s]\n100%|██████████| 1/1 [00:00<00:00, 471.80it/s]\n100%|██████████| 3/3 [00:00<00:00, 826.68it/s]\n100%|██████████| 1/1 [00:00<00:00, 3887.21it/s]\n100%|██████████| 1/1 [00:00<00:00, 505.58it/s]\n100%|██████████| 3/3 [00:00<00:00, 853.19it/s]\n100%|██████████| 1/1 [00:00<00:00, 3788.89it/s]\n100%|██████████| 1/1 [00:00<00:00, 1066.71it/s]\n100%|██████████| 3/3 [00:00<00:00, 756.78it/s]\n100%|██████████| 1/1 [00:00<00:00, 3685.68it/s]\n100%|██████████| 1/1 [00:00<00:00, 386.96it/s]\n100%|██████████| 3/3 [00:00<00:00, 839.87it/s]\n100%|██████████| 2/2 [00:00<00:00, 5515.19it/s]\n100%|██████████| 1/1 [00:00<00:00, 339.95it/s]\n100%|██████████| 3/3 [00:00<00:00, 762.51it/s]\n100%|██████████| 2/2 [00:00<00:00, 6569.00it/s]\n100%|██████████| 1/1 [00:00<00:00, 538.49it/s]\n100%|██████████| 3/3 [00:00<00:00, 975.95it/s]\n100%|██████████| 1/1 [00:00<00:00, 4604.07it/s]\n100%|██████████| 1/1 [00:00<00:00, 635.21it/s]\n100%|██████████| 3/3 [00:00<00:00, 1078.50it/s]\n100%|██████████| 1/1 [00:00<00:00, 3057.07it/s]\n100%|██████████| 1/1 [00:00<00:00, 320.57it/s]\n100%|██████████| 3/3 [00:00<00:00, 650.25it/s]\n100%|██████████| 1/1 [00:00<00:00, 1607.01it/s]\n100%|██████████| 1/1 [00:00<00:00, 348.16it/s]\n100%|██████████| 3/3 [00:00<00:00, 948.65it/s]\n100%|██████████| 1/1 [00:00<00:00, 3398.95it/s]\n100%|██████████| 1/1 [00:00<00:00, 1326.05it/s]\n100%|██████████| 3/3 [00:00<00:00, 1380.77it/s]\n100%|██████████| 1/1 [00:00<00:00, 3960.63it/s]\n100%|██████████| 1/1 [00:00<00:00, 875.09it/s]\n100%|██████████| 3/3 [00:00<00:00, 1207.57it/s]\n100%|██████████| 1/1 [00:00<00:00, 4156.89it/s]\n100%|██████████| 1/1 [00:00<00:00, 436.09it/s]\n100%|██████████| 3/3 [00:00<00:00, 820.21it/s]\n100%|██████████| 1/1 [00:00<00:00, 4006.02it/s]\n100%|██████████| 1/1 [00:00<00:00, 863.91it/s]\n100%|██████████| 3/3 [00:00<00:00, 1076.66it/s]\n100%|██████████| 1/1 [00:00<00:00, 3443.60it/s]\n100%|██████████| 1/1 [00:00<00:00, 326.81it/s]\n100%|██████████| 3/3 [00:00<00:00, 702.92it/s]\n100%|██████████| 1/1 [00:00<00:00, 3201.76it/s]\n100%|██████████| 1/1 [00:00<00:00, 586.94it/s]\n100%|██████████| 3/3 [00:00<00:00, 874.30it/s]\n100%|██████████| 1/1 [00:00<00:00, 3876.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 559.09it/s]\n100%|██████████| 3/3 [00:00<00:00, 1017.46it/s]\n100%|██████████| 1/1 [00:00<00:00, 2945.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 327.02it/s]\n100%|██████████| 3/3 [00:00<00:00, 776.58it/s]\n100%|██████████| 1/1 [00:00<00:00, 3998.38it/s]\n","name":"stderr"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00, 679.90it/s]\n100%|██████████| 3/3 [00:00<00:00, 1128.11it/s]\n100%|██████████| 1/1 [00:00<00:00, 1277.58it/s]\n100%|██████████| 1/1 [00:00<00:00, 774.43it/s]\n100%|██████████| 3/3 [00:00<00:00, 1018.03it/s]\n100%|██████████| 1/1 [00:00<00:00, 2669.83it/s]\n100%|██████████| 1/1 [00:00<00:00, 682.67it/s]\n100%|██████████| 3/3 [00:00<00:00, 987.75it/s]\n100%|██████████| 1/1 [00:00<00:00, 3557.51it/s]\n100%|██████████| 1/1 [00:00<00:00, 330.26it/s]\n100%|██████████| 3/3 [00:00<00:00, 645.84it/s]\n100%|██████████| 1/1 [00:00<00:00, 2413.29it/s]\n100%|██████████| 1/1 [00:00<00:00, 638.79it/s]\n100%|██████████| 3/3 [00:00<00:00, 1042.93it/s]\n100%|██████████| 1/1 [00:00<00:00, 3949.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 600.22it/s]\n100%|██████████| 3/3 [00:00<00:00, 883.01it/s]\n100%|██████████| 1/1 [00:00<00:00, 3339.41it/s]\n100%|██████████| 1/1 [00:00<00:00, 751.53it/s]\n100%|██████████| 3/3 [00:00<00:00, 1037.68it/s]\n100%|██████████| 2/2 [00:00<00:00, 4571.45it/s]\n100%|██████████| 1/1 [00:00<00:00, 284.77it/s]\n100%|██████████| 3/3 [00:00<00:00, 923.72it/s]\n100%|██████████| 1/1 [00:00<00:00, 2557.50it/s]\n100%|██████████| 1/1 [00:00<00:00, 705.40it/s]\n100%|██████████| 3/3 [00:00<00:00, 842.23it/s]\n100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n100%|██████████| 1/1 [00:00<00:00, 237.19it/s]\n100%|██████████| 3/3 [00:00<00:00, 504.12it/s]\n100%|██████████| 2/2 [00:00<00:00, 4332.96it/s]\n100%|██████████| 1/1 [00:00<00:00, 709.22it/s]\n100%|██████████| 3/3 [00:00<00:00, 468.24it/s]\n100%|██████████| 1/1 [00:00<00:00, 3536.51it/s]\n100%|██████████| 1/1 [00:00<00:00, 425.21it/s]\n100%|██████████| 3/3 [00:00<00:00, 855.05it/s]\n100%|██████████| 1/1 [00:00<00:00, 3281.93it/s]\n100%|██████████| 1/1 [00:00<00:00, 812.38it/s]\n100%|██████████| 3/3 [00:00<00:00, 879.12it/s]\n100%|██████████| 1/1 [00:00<00:00, 3563.55it/s]\n100%|██████████| 1/1 [00:00<00:00, 727.29it/s]\n100%|██████████| 3/3 [00:00<00:00, 919.74it/s]\n100%|██████████| 1/1 [00:00<00:00, 3625.15it/s]\n100%|██████████| 1/1 [00:00<00:00, 539.39it/s]\n100%|██████████| 3/3 [00:00<00:00, 774.33it/s]\n100%|██████████| 1/1 [00:00<00:00, 1596.61it/s]\n100%|██████████| 1/1 [00:00<00:00, 377.70it/s]\n100%|██████████| 3/3 [00:00<00:00, 646.77it/s]\n100%|██████████| 1/1 [00:00<00:00, 2477.44it/s]\n100%|██████████| 1/1 [00:00<00:00, 732.89it/s]\n100%|██████████| 3/3 [00:00<00:00, 1097.99it/s]\n100%|██████████| 1/1 [00:00<00:00, 3708.49it/s]\n100%|██████████| 1/1 [00:00<00:00, 780.92it/s]\n100%|██████████| 3/3 [00:00<00:00, 1122.07it/s]\n100%|██████████| 1/1 [00:00<00:00, 3182.32it/s]\n100%|██████████| 1/1 [00:00<00:00, 737.65it/s]\n100%|██████████| 3/3 [00:00<00:00, 969.78it/s]\n100%|██████████| 1/1 [00:00<00:00, 2692.11it/s]\n100%|██████████| 1/1 [00:00<00:00, 273.74it/s]\n100%|██████████| 3/3 [00:00<00:00, 1041.63it/s]\n100%|██████████| 1/1 [00:00<00:00, 1541.46it/s]\n100%|██████████| 1/1 [00:00<00:00, 845.11it/s]\n100%|██████████| 3/3 [00:00<00:00, 1171.38it/s]\n100%|██████████| 1/1 [00:00<00:00, 2886.65it/s]\n100%|██████████| 1/1 [00:00<00:00, 562.54it/s]\n100%|██████████| 3/3 [00:00<00:00, 881.28it/s]\n100%|██████████| 2/2 [00:00<00:00, 1912.15it/s]\n100%|██████████| 1/1 [00:00<00:00, 410.40it/s]\n100%|██████████| 3/3 [00:00<00:00, 1040.34it/s]\n100%|██████████| 1/1 [00:00<00:00, 3945.72it/s]\n100%|██████████| 1/1 [00:00<00:00, 434.55it/s]\n100%|██████████| 3/3 [00:00<00:00, 770.02it/s]\n100%|██████████| 1/1 [00:00<00:00, 3429.52it/s]\n100%|██████████| 1/1 [00:00<00:00, 275.00it/s]\n100%|██████████| 3/3 [00:00<00:00, 849.39it/s]\n100%|██████████| 1/1 [00:00<00:00, 2030.16it/s]\n100%|██████████| 1/1 [00:00<00:00, 856.33it/s]\n100%|██████████| 3/3 [00:00<00:00, 1139.34it/s]\n100%|██████████| 1/1 [00:00<00:00, 2700.78it/s]\n100%|██████████| 1/1 [00:00<00:00, 686.69it/s]\n100%|██████████| 3/3 [00:00<00:00, 903.10it/s]\n100%|██████████| 1/1 [00:00<00:00, 3137.10it/s]\n100%|██████████| 1/1 [00:00<00:00, 411.93it/s]\n100%|██████████| 3/3 [00:00<00:00, 900.71it/s]\n100%|██████████| 1/1 [00:00<00:00, 3554.49it/s]\n100%|██████████| 1/1 [00:00<00:00, 260.66it/s]\n100%|██████████| 3/3 [00:00<00:00, 623.56it/s]","name":"stderr"},{"output_type":"stream","text":"time to extract data:  288.2806177139282\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"---\n## Test generator\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test specific methods\ndef extract_nearest_wifi(wifi_datas, timestamp):\n    diff_list = []\n    wifi_ts = wifi_datas[:, 0]\n    for ts in wifi_ts:\n        diff = abs(int(timestamp) - int(ts))\n        diff_list.append(diff)\n    min_value = min(diff_list)\n    diff_indices = [i for i, x in enumerate(diff_list) if x == min_value]\n    wifi_datas = wifi_datas[diff_indices]\n    return wifi_datas\n\ndef make_wifi_df_test(zipped_paths):\n    site_id, file_id, timestamp, site_path_timestamp = zipped_paths\n    file_path = \"../input/indoor-location-navigation/test/\" + file_id + \".txt\"\n    datas = read_data_file_ed(file_path)\n    acce_datas = datas.acce\n    magn_datas = datas.magn\n    ahrs_datas = datas.ahrs\n    wifi_datas = datas.wifi\n    ibeacon_datas = datas.ibeacon\n    # wps = datas.waypoint # not to be used\n\n    # acce and ahrs data translation\n    # rel_positions = calc_rel_positions(acce_datas, ahrs_datas)\n    # print(\"wifi unique ts len: \", len(set(wifi_datas[:, 0])))\n\n    # Make wifi df with wifi_ts\n    wifi_datas = extract_nearest_wifi(wifi_datas, timestamp)\n    \n    dfs = []\n    df = pd.DataFrame(wifi_datas[:,[0,2,4]])\n    for wifi_ts, g in df.groupby(0):\n        g = g.drop_duplicates(subset=1)\n        tmp = g.iloc[:,1:]\n        feat = tmp.set_index(1).T\n        feat[\"wifi_ts\"] = wifi_ts\n\n        # get closest wps\n        feat[\"wps_diff\"] = abs(int(wifi_ts) - int(timestamp))\n        feat[\"x\"] = np.nan\n        feat[\"y\"] = np.nan\n\n        # get average of acce within 250ms\n        acce_avgs = extract_imu_rep(acce_datas, wifi_ts)\n        feat[\"acce_x_avg\"] = acce_avgs[0]\n        feat[\"acce_y_avg\"] = acce_avgs[1]\n        feat[\"acce_z_avg\"] = acce_avgs[2]\n        \n        # get floor and other path data\n        feat[\"site_path_timestamp\"] = site_path_timestamp\n        feat[\"site_id\"] = site_id\n        feat[\"file_id\"] = file_id\n        feat[\"floor_int\"] = np.nan\n        feat[\"floor\"] = np.nan\n        \n        dfs.append(feat)\n    \n    return dfs\n\ndef make_test_df(zipped_path, site):\n    dfs_all = pool.map(make_wifi_df_test, tqdm(zipped_path))\n    dfs_unpack = [row for df in dfs_all for row in df]\n    wifi_df = pd.concat(dfs_unpack)\n    wifi_df = shift_columns(SHIFT_COLS_TEST, wifi_df)\n    wifi_df = wifi_df.fillna(-999)\n    convert_dtypes(wifi_df, tqdm(INT_COLS), int)\n    convert_dtypes(wifi_df, tqdm(CAT_COLS), \"category\")\n    # display(wifi_df.head())\n    # print(wifi_df.iloc[:, :30].info())\n    wifi_df.to_csv(f\"./test/{site}_test.csv\", index=False)\n    del wifi_df","execution_count":121,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get submission file\nsub_df = pd.read_csv(\"/kaggle/input/indoor-location-navigation/sample_submission.csv\")\nsub_df[[\"site_id\", \"file_id\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\nsub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n# sub_df_site_list = sub_df[\"site_id\"].unique()","execution_count":122,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"start = time.time()\nnum_cores = multiprocessing.cpu_count()\npool = Pool(num_cores)\n\n# 100 records:  33.47870922088623 sec\n# comment out to run all\n# sub_df = sub_df.sample(n=100)\n# sub_df = sub_df.iloc[:9, :]\ntest_sites = sub_df[\"site_id\"].unique()\n\n# Run generator for each building\nfor site in test_sites:\n    sub_df_filtered = sub_df[sub_df[\"site_id\"] == site]\n    site_file_zip = list(zip(sub_df_filtered[\"site_id\"], \\\n                             sub_df_filtered[\"file_id\"], \\\n                             sub_df_filtered[\"timestamp\"], \\\n                             sub_df_filtered[\"site_path_timestamp\"]))\n    make_test_df(site_file_zip, site)\n\n# display(wifi_df.head())\n\nprint(\"time to extract data: \", time.time() - start)\npool.close()","execution_count":123,"outputs":[{"output_type":"stream","text":"100%|██████████| 8/8 [00:00<00:00, 10091.56it/s]\n100%|██████████| 1/1 [00:00<00:00, 122.95it/s]\n100%|██████████| 3/3 [00:00<00:00, 532.54it/s]\n100%|██████████| 2/2 [00:00<00:00, 4629.47it/s]\n100%|██████████| 1/1 [00:00<00:00, 465.00it/s]\n100%|██████████| 3/3 [00:00<00:00, 1048.66it/s]\n100%|██████████| 10/10 [00:00<00:00, 14503.13it/s]\n100%|██████████| 1/1 [00:00<00:00, 227.69it/s]\n100%|██████████| 3/3 [00:00<00:00, 637.69it/s]\n100%|██████████| 9/9 [00:00<00:00, 12663.11it/s]\n100%|██████████| 1/1 [00:00<00:00, 136.64it/s]\n100%|██████████| 3/3 [00:00<00:00, 623.60it/s]\n100%|██████████| 6/6 [00:00<00:00, 11507.01it/s]\n100%|██████████| 1/1 [00:00<00:00, 227.25it/s]\n100%|██████████| 3/3 [00:00<00:00, 979.60it/s]\n100%|██████████| 6/6 [00:00<00:00, 4933.51it/s]\n100%|██████████| 1/1 [00:00<00:00, 170.22it/s]\n100%|██████████| 3/3 [00:00<00:00, 892.15it/s]\n100%|██████████| 6/6 [00:00<00:00, 15496.20it/s]\n100%|██████████| 1/1 [00:00<00:00, 323.16it/s]\n100%|██████████| 3/3 [00:00<00:00, 716.53it/s]\n100%|██████████| 6/6 [00:00<00:00, 16699.29it/s]\n100%|██████████| 1/1 [00:00<00:00, 178.44it/s]\n100%|██████████| 3/3 [00:00<00:00, 731.31it/s]\n100%|██████████| 6/6 [00:00<00:00, 12282.00it/s]\n100%|██████████| 1/1 [00:00<00:00, 304.42it/s]\n100%|██████████| 3/3 [00:00<00:00, 655.84it/s]\n100%|██████████| 7/7 [00:00<00:00, 15371.79it/s]\n100%|██████████| 1/1 [00:00<00:00, 201.99it/s]\n100%|██████████| 3/3 [00:00<00:00, 835.02it/s]\n100%|██████████| 5/5 [00:00<00:00, 8771.02it/s]\n100%|██████████| 1/1 [00:00<00:00, 337.60it/s]\n100%|██████████| 3/3 [00:00<00:00, 806.34it/s]\n100%|██████████| 4/4 [00:00<00:00, 6410.86it/s]\n100%|██████████| 1/1 [00:00<00:00, 307.68it/s]\n100%|██████████| 3/3 [00:00<00:00, 980.59it/s]\n100%|██████████| 3/3 [00:00<00:00, 6757.74it/s]\n100%|██████████| 1/1 [00:00<00:00, 331.43it/s]\n100%|██████████| 3/3 [00:00<00:00, 714.94it/s]\n100%|██████████| 2/2 [00:00<00:00, 2654.62it/s]\n100%|██████████| 1/1 [00:00<00:00, 230.66it/s]\n100%|██████████| 3/3 [00:00<00:00, 631.64it/s]\n100%|██████████| 4/4 [00:00<00:00, 12282.00it/s]\n100%|██████████| 1/1 [00:00<00:00, 253.72it/s]\n100%|██████████| 3/3 [00:00<00:00, 781.45it/s]\n100%|██████████| 2/2 [00:00<00:00, 4286.46it/s]\n100%|██████████| 1/1 [00:00<00:00, 248.70it/s]\n100%|██████████| 3/3 [00:00<00:00, 832.97it/s]\n100%|██████████| 2/2 [00:00<00:00, 4156.89it/s]\n100%|██████████| 1/1 [00:00<00:00, 435.14it/s]\n100%|██████████| 3/3 [00:00<00:00, 877.04it/s]\n100%|██████████| 7/7 [00:00<00:00, 4791.14it/s]\n100%|██████████| 1/1 [00:00<00:00, 241.11it/s]\n100%|██████████| 3/3 [00:00<00:00, 986.43it/s]\n100%|██████████| 1/1 [00:00<00:00, 3008.83it/s]\n100%|██████████| 1/1 [00:00<00:00, 443.98it/s]\n100%|██████████| 3/3 [00:00<00:00, 1136.26it/s]\n100%|██████████| 1/1 [00:00<00:00, 2576.35it/s]\n100%|██████████| 1/1 [00:00<00:00, 499.56it/s]\n100%|██████████| 3/3 [00:00<00:00, 776.82it/s]\n100%|██████████| 1/1 [00:00<00:00, 2357.68it/s]\n100%|██████████| 1/1 [00:00<00:00, 312.22it/s]\n100%|██████████| 3/3 [00:00<00:00, 1087.64it/s]\n100%|██████████| 1/1 [00:00<00:00, 4025.24it/s]\n100%|██████████| 1/1 [00:00<00:00, 391.15it/s]\n100%|██████████| 3/3 [00:00<00:00, 1033.16it/s]\n100%|██████████| 1/1 [00:00<00:00, 3851.52it/s]\n100%|██████████| 1/1 [00:00<00:00, 589.42it/s]\n100%|██████████| 3/3 [00:00<00:00, 1078.13it/s]","name":"stderr"},{"output_type":"stream","text":"time to extract data:  33.47870922088623\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n\n# num_cores = multiprocessing.cpu_count()\n# print(f\"num_cores={num_cores}\")\n# pool = Pool(num_cores)\n\n# # 10 paths:  6.070369720458984\n# # 100 paths:  87.05400061607361\n# # dfs_all = pool.map(make_wifi_df, tqdm(train_paths[:TRAIN_NUM]))\n# dfs_all = pool.map(make_wifi_df, tqdm(grouped_paths_list[:10]))\n\n# # time to process:  11.514546155929565\n# # dfs_all = []\n# # for path in train_paths[:TRAIN_NUM]:\n# #     dfs_all.append(make_wifi_df(path))\n\n# print(len(dfs_all))\n# print(\"time to extract data: \", time.time() - start)\n# pool.close()","execution_count":124,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n\n# num_cores = multiprocessing.cpu_count()\n# pool = Pool(num_cores)\n\n# # Do this for each building\n\n# # 10 paths:  8.992910146713257\n# # 100 paths:  2454.589078426361\n# dfs_unpack = [row for df in dfs_all for row in df]\n# wifi_df = pd.concat(dfs_unpack)\n\n# print(\"time for df conversion: \", time.time() - start)\n# print(len(wifi_df.columns))\n# print(len(wifi_df))\n# display(wifi_df.head())\n# pool.close()","execution_count":125,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wifi_df.iloc[:,:50].info()","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n\n# # move columns\n# cols = [\"acce_z_avg\", \"acce_y_avg\", \"acce_x_avg\", \\\n#         \"site_id\", \"file_id\", \"floor_int\", \"floor\", \\\n#         \"y\", \"x\", \"wps_diff\", \"wifi_ts\"]\n\n# for col in cols:\n#     df_cols = list(wifi_df.columns)\n#     df_cols.insert(0, df_cols.pop(df_cols.index(col)))\n#     wifi_df = wifi_df[df_cols]\n  \n# # Fillna\n# wifi_df = wifi_df.fillna(-999)\n\n# display(wifi_df.head())\n# print(len(wifi_df))\n\n# print(\"time to shift columns: \", time.time() - start)\n# print(wifi_df.iloc[:,:50].info())","execution_count":127,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"available RAM:\", psutil.virtual_memory())\n\n# train_file_name = \"indoor_train_5.pkl\"\n\n# with open(train_file_name, \"wb\") as file:\n#     pickle.dump(wifi_df, file)\n\n# del wifi_df\n# del dfs_unpack\n# del dfs_all\n# gc.collect()\n\n# print(\"available RAM after cleanup:\", psutil.virtual_memory())","execution_count":128,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Load data it back in\n# train_file_name = \"indoor_train_5.pkl\"\n\n# with open(train_file_name, \"rb\") as file:\n#     df_train = pickle.load(file)","execution_count":129,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"df len: \", len(df_train), \"\\n\")\n# print(\"site_id nunique: \", df_train[\"site_id\"].nunique(), \"\\n\")\n# print(\"site_id value_counts: \", df_train[\"site_id\"].value_counts(), \"\\n\")\n# print(\"file_id nunique: \", df_train[\"file_id\"].nunique(), \"\\n\")\n# print(\"x value_counts: \", df_train[\"x\"].value_counts(), \"\\n\")\n# print(\"y value_counts: \", df_train[\"y\"].value_counts(), \"\\n\")\n# print(\"wifi_ts nunique: \", df_train[\"wifi_ts\"].nunique(), \"\\n\")\n# print(\"wps_diff nunique: \", df_train[\"wps_diff\"].nunique(), \"\\n\")\n# display(df_train.head())","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_pp = df_train.loc[:, [\"site_id\", \"x\", \"y\", \"acce_x_avg\", \"acce_y_avg\", \"acce_z_avg\"]]\n# display(df_train_pp.head())\n# sns.pairplot(df_train_pp, hue=\"site_id\")","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Check the wps_diff distribution\n# # Need to filter out those wps that are above 5000ms difference from wifi_ts\n# f, ax = plt.subplots(figsize=(8, 8))\n# f.patch.set_facecolor(\"white\")\n# sns.distplot(df_train[\"wps_diff\"])\n# plt.show()","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_slim = df_train[df_train[\"wps_diff\"] < WPS_CUT]\n# perc = round(len(df_train_slim)/len(df_train)*100, 2)\n\n# print(\"no of records: \", len(df_train))\n# print(f\"Filter df_train with {WPS_CUT}, it retains {perc} % of data\")","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Visualizing timestamp distribution\n\n# # LabelEncode site_id, file_id, floor_converted, ssid, bssid\n# # def col_encode(df, cols):\n# #     for col in cols:\n# #         le = preprocessing.LabelEncoder()\n# #         df[\"%s_le\"%col] = le.fit_transform(df[col])\n\n# # col_enc = [\"site_id\", \"file_id\", \"wifi_ssid\", \"wifi_bssid\", \"beacon_ssid\"]\n# # col_encode(df_train, tqdm(col_enc))\n\n# # convert data types of certain columns\n# def convert_dtypes(df, col_list, dtype):\n#     for col in col_list:\n#         df[col] = df[col].astype(dtype)\n\n# convert_dtypes(df_train, tqdm([\"wifi_ts\"]), int)\n# convert_dtypes(df_train, tqdm([\"file_id\", \"site_id\", \"floor\"]), \"category\")\n\n# # Check\n# display(df_train.head())","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Methods for preprocessing train data: Timestamp handling\n# def find_diff_ts(ts, data):\n#     data_ts = data[0]\n#     diff_ts = int(data_ts) - int(ts)\n#     return diff_ts\n\n# def find_start_ts(path):\n#     with open(path, 'r', encoding='utf-8') as file:\n#         lines = file.readlines()\n\n#     for line_data in lines:\n#         line_data = line_data.strip()\n#         m = re.search(r\"(?<=startTime.)(.*)\", line_data)\n#         start_ts = m.groups(0)\n#         if m:\n#             return (start_ts[0])\n\n# def find_smallest_diff(t, data):\n#     if data.size == 0:\n#         return np.array([])\n#     else:\n#         data_ts = data[:, [0]]\n#         diff = []\n#         for ts in data_ts:\n#             diff.append(abs(int(t) - int(ts)))\n#         closest_index = np.argmin(diff) # if multiple records have the same value..?\n#         return data[closest_index]","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Method for preprocessing train data: splitting acce/ahrs/gyro/magn\n# def split_axis(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no axis data\")\n#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         x_axis = data[1]\n#         y_axis = data[2]\n#         z_axis = data[3]\n#         try:\n#             accuracy = data[4]\n#         except IndexError:\n#             accuracy = np.nan\n#         return [data_ts, diff_ts, x_axis, y_axis, z_axis, accuracy]\n\n# # Method for preprocessing train data: splitting wifi\n# def split_wifi(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no wifi data\")\n#         return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         ssid = data[1]\n#         bssid = data[2]\n#         rssi = data[3]\n#         if len(data) > 5:\n#             freq = data[4]\n#             last_seen_ts = data[5]\n#         else:\n#             freq = np.nan\n#             last_seen_ts = data[-1]\n#         return [data_ts, diff_ts, ssid, bssid, rssi, freq, last_seen_ts]\n\n# # Method for preprocessing train data: splitting ibeacon\n# def split_beacon(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no beacon data\")\n#         return [np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         ssid = data[1]\n#         rssi = data[2]\n#         return [data_ts, diff_ts, ssid, rssi]\n\n# # Method for preprocessing train data: calc rel pos\n# def split_rel_pos(data, start_ts):\n#     if data.size == 0:\n#         # print(\"no rel_pos data\")\n#         return [np.nan, np.nan, np.nan, np.nan]\n#     else:\n#         data_ts = data[0]\n#         diff_ts = int(data[0]) - int(start_ts)\n#         x_axis = data[1]\n#         y_axis = data[2]\n#         return [data_ts, diff_ts, x_axis, y_axis]","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Extract path and other data\n# def extract_path(path, floor_map):\n#     # split path\n#     try:\n#         ex_path = f\"{path}\"\n#         ex_paths = ex_path.split(\"/\")\n#         site_id = ex_paths[4]\n#         floor = ex_paths[5]\n#         f = floor_map[floor]\n#         file_id = ex_paths[6].split(\".\")[0]\n#         return [site_id, file_id, f, floor]\n#     except:\n#         print(\"extract_path error\")\n\n# # Definitely needs to be refactored\n# def extract_data(path):\n#     start_ts = find_start_ts(path)\n#     path_datas = read_data_file(path)\n#     acce = path_datas.acce\n#     ahrs = path_datas.ahrs\n#     magn = path_datas.magn\n#     gyro = path_datas.gyro\n#     acce_uncali = path_datas.acce_uncali\n#     magn_uncali = path_datas.magn_uncali\n#     gyro_uncali = path_datas.gyro_uncali\n#     wifi = path_datas.wifi\n#     wps = path_datas.waypoint\n#     ibeacon = path_datas.ibeacon\n#     rel_positions = calc_rel_positions(acce, ahrs)\n\n#     # Changed from: just extracting wps time stamps -> take all acce uncalib timestamps\n#     # ts = np.unique(wps[:, [0]])\n#     if acce_uncali.any():\n#         # print(\"acce_uncali\")\n#         ts = np.unique(acce_uncali[:, [0]]) # take uncalibrated access, as sometimes access has less data\n#     elif acce.any():\n#         # print(\"acce\")\n#         ts = np.unique(acce[:, [0]])\n#     else:\n#         print(\"no acce or acce_uncali\")\n\n#     # extract data for each timestamp of waypoints\n#     res = []\n#     for t in ts:\n#         try:\n#             wp_closest = find_smallest_diff(t, wps)\n#             closest_wp_ts = wp_closest[0]\n#             diff_ts_wp_ts = abs(int(t) - int(closest_wp_ts))\n#             # time_stamp_cut = 2000, only the records within 2 sec of waypoint are kept\n#             if diff_ts_wp_ts < time_stamp_cut:\n#                 # flag to indicate how close the data point is to the wps\n#                 # print(\"diff_ts_wp_ts\", diff_ts_wp_ts)\n#                 within_100ms = True if abs(diff_ts_wp_ts) <= 100 else False\n#                 within_200ms = True if abs(diff_ts_wp_ts) <= 200 else False\n#                 x = wp_closest[1]\n#                 y = wp_closest[2]\n#                 # print(\"x, y: \", x, y)\n#                 diff_start_ts = int(t) - int(start_ts)\n#                 diff_start_wp_ts = int(closest_wp_ts) - int(start_ts)\n#                 # print(\"diff_start_ts, diff_start_wp_ts: \", diff_start_ts, diff_start_wp_ts)\n#                 acce_closest = split_axis(find_smallest_diff(t, acce), start_ts)\n#                 ahrs_closest = split_axis(find_smallest_diff(t, ahrs), start_ts)\n#                 magn_closest = split_axis(find_smallest_diff(t, magn), start_ts)\n#                 magn_closest.append(extract_one_magn_strength(magn_closest)) # append magnetic strength only for the magn data\n#                 gyro_closest = split_axis(find_smallest_diff(t, gyro), start_ts)\n#                 # print(\"acce: \", acce_closest)\n#                 # print(\"ahrs: \", ahrs_closest)\n#                 # print(\"magn: \", magn_closest)\n#                 # print(\"gyro: \", gyro_closest)\n#                 acce_u_closest = split_axis(find_smallest_diff(t, acce_uncali), start_ts)\n#                 magn_u_closest = split_axis(find_smallest_diff(t, magn_uncali), start_ts)\n#                 gyro_u_closest = split_axis(find_smallest_diff(t, gyro_uncali), start_ts)\n#                 # print(\"acce_u_closest: \", acce_u_closest)\n#                 # print(\"magn_u_closest: \", magn_u_closest)\n#                 # print(\"gyro_u_closest: \", gyro_u_closest)\n#                 wifi_closest = split_wifi(find_smallest_diff(t, wifi), start_ts)\n#                 if len(ibeacon) > 0:\n#                     beacon_closest = split_beacon(find_smallest_diff(t, ibeacon), start_ts)\n#                 else:\n#                     beacon_closest = [np.nan, np.nan, np.nan, np.nan]\n#                 rel_pos = split_rel_pos(find_smallest_diff(t, rel_positions), start_ts)\n#                 # print([t, x, y, int(closest_wp_ts), acce_closest, acce_u_closest])\n#                 res.append([int(t), start_ts, diff_start_ts, x, y, int(closest_wp_ts), diff_start_wp_ts, diff_ts_wp_ts, within_100ms, within_200ms] + \\\n#                            acce_closest + ahrs_closest + magn_closest + gyro_closest + \\\n#                            acce_u_closest + magn_u_closest + gyro_u_closest + \\\n#                            wifi_closest + beacon_closest + rel_pos\n#                           )\n#             else:\n#                 # print(\"no wp made it through timestamp cut\")\n#                 continue\n#         except Exception as exc:\n#             pass\n#             # print(\"Error message: \", exc)\n#             # print(\"extract_test_data error\")\n#     return res","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # %%timeit\n\n# # 5.55 ms ± 1.76 ms per loop\n# path, site, floorNo, floor_plan_filename, \\\n# json_plan_filename, width_meter, height_meter = pick_example(len(train_paths), train_paths)\n\n# def one_trace_to_rows(path, floor_map):\n#     try:\n#         path_info = extract_path(path, floor_map)\n#         data = extract_data(path)\n#         # rows = list(itertools.chain(path_info, *data))\n#         rows = []\n#         for d in data:\n#             row = path_info + d\n#             rows.append(row)\n#             # print(\"row: \", row)\n#         return rows\n#     except:\n#         print(\"one_trace_to_rows error at: \", path)\n\n# # path -> train/5cd56bdbe2acfd2d33b663c0/L3/5dfc8108241c3600064049b9.txt\n# # time w/ for loop with 1 train_path -> 11.6\n# # time w/ itertools.chain for 1 train_path -> 11.8\n# start = time.time()\n# path_info = extract_path(path, floor_map)\n# print(\"path: \", path_info)\n# rows = one_trace_to_rows(path, floor_map)\n# print(\"time to process one train_path\", time.time() - start)\n# #print(\"col count: \", len(rows[0]))\n# print(\"rows: \", rows)","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Run row making function for all training paths\n# # print(train_paths[:10])\n# import time\n# start = time.time()\n\n# all_rows = []\n# for train_path in train_paths[:10]:\n#     rows = one_trace_to_rows(train_path, floor_map)\n#     all_rows.extend(rows)\n\n# one_trace_df = pd.DataFrame(all_rows)\n# display(len(one_trace_df))\n\n# # Data below are the time it took to create the old version of training data (only waypoints)\n# # without Pool\n# # 10 -> 1.64 sec\n# # 100 -> 28.12 sec\n# # 1000 -> 286.67 sec\n# # to process training (~26,000 files) -> ~7500 sec (~2hours)\n# print(time.time() - start)\n\n# with Pool\n# no need for wrapper with pool.starmap -> https://qiita.com/okiyuki99/items/a54797cb44eb4ae571f6\n\n# Memo about Pool\n# with Pool\n# 10 -> 1.09 sec\n# 100 -> 12.35 sec\n# 1000 -> 113.87 sec\n# to process training (~26,000 files) -> ~3000 sec (~50min)","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Check if we can make df\n\n# # column names\n# col_names = [\"site_id\", \"file_id\", \"floor_converted\", \"floor\", \\\n#              \"ts\", \"start_ts\", \"diff_start_ts\", \"x\", \"y\", \\\n#              \"closest_wp_ts\", \"diff_start_wp_ts\", \"diff_ts_wp_ts\", \"within_100ms\", \"within_200ms\", \\\n#              \"acce_ts\", \"diff_acce_ts\", \"acce_x\", \"acce_y\", \"acce_z\", \"acce_acc\", \\\n#              \"ahrs_ts\", \"diff_ahrs_ts\", \"ahrs_x\", \"ahrs_y\", \"ahrs_z\", \"ahrs_acc\", \\\n#              \"magn_ts\", \"diff_magn_ts\", \"magn_x\", \"magn_y\", \"magn_z\", \"magn_acc\", \"magn_strength\",\\\n#              \"gyro_ts\", \"diff_gyro_ts\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_acc\", \\\n#              \"acce_u_ts\", \"diff_acce_u_ts\", \"acce_u_x\", \"acce_u_y\", \"acce_u_z\", \"acce_u_acc\", \\\n#              \"magn_u_ts\", \"diff_magn_u_ts\", \"magn_u_x\", \"magn_u_y\", \"magn_u_z\", \"magn_u_acc\", \\\n#              \"gyro_u_ts\", \"diff_gyro_u_ts\", \"gyro_u_x\", \"gyro_u_y\", \"gyro_u_z\", \"gyro_u_acc\", \\\n#              \"wifi_ts\", \"diff_wifi_ts\", \"wifi_ssid\", \"wifi_bssid\", \"wifi_rssi\", \"wifi_freq\", \"wifi_last_seen_ts\", \\\n#              \"beacon_ts\", \"diff_beacon_ts\", \"beacon_ssid\", \"beacon_rssi\", \\\n#              \"rel_ts\", \"diff_rel_ts\", \"rel_x\", \"rel_y\"\n#             ]\n\n# print(len(col_names))\n\n# df = pd.DataFrame(rows, columns=col_names)\n# print(\"df len: \", len(df))\n# print(\"site_id nunique: \", df[\"site_id\"].nunique())\n# print(\"file_id nunique: \", df[\"file_id\"].nunique())\n# print(\"x value_counts: \", df[\"x\"].value_counts())\n# print(\"y value_counts: \", df[\"y\"].value_counts())\n# print(\"event ts nunique: \", df[\"ts\"].nunique())\n# print(\"start ts nunique: \", df[\"start_ts\"].nunique()) # should be one\n# print(\"diff_ts_wp_ts value_counts: \", df[\"diff_ts_wp_ts\"].value_counts())\n# print(\"diff_ts_wp_ts nunique: \", df[\"diff_ts_wp_ts\"].nunique())\n# print(\"within_100ms value_counts: \", df[\"within_100ms\"].value_counts())\n# print(\"within_100ms nunique: \", df[\"within_100ms\"].nunique())\n# print(\"within_100ms count: \", df[\"within_100ms\"].count())\n# print(\"within_200ms value_counts: \", df[\"within_200ms\"].value_counts())\n# print(\"within_200ms nunique: \", df[\"within_200ms\"].nunique())\n# print(\"within_200ms count: \", df[\"within_200ms\"].count())\n# display(df.head())","execution_count":140,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Set pool\n# num_cores = multiprocessing.cpu_count()\n# print(f\"num_cores={num_cores}\")\n# # args = [(p, floor_map) for p in train_paths[:train_num]]\n# args = [(p, floor_map) for p in grouped_paths_list]\n# pool = Pool(num_cores)\n\n# start = time.time()\n# # w/ 250ms settings, 3 random samples from each site_id\n# # 2 paths -> 18.7 sec\n# # 10 paths -> 315 sec (df len is 1994)\n# # 100 paths -> 708 sec (df len is 7183)\n# # all ~ 600 paths -> \n\n# # errors\n# # grouped_paths_list -> 100 paths -> site_id: 8 errors, 27 correct\n# # grouped_paths_list -> 100 paths -> file_id: 23 errors, 77 correct\n\n# # all in one go -> xxx sec\n# # array_split -> 5891.8 sec\n\n# # all in one go\n# # res = pool.starmap(one_trace_to_rows, args)\n\n# # split the args\n# res = []\n# for arg in tqdm(np.array_split(args, 50)):\n#     res.extend(pool.starmap(one_trace_to_rows, arg))","execution_count":141,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################## KEEP THIS CELL FOR LATER REF ##############################\n\n# Error in ~20% of the train paths -> caused by not having acces_uncali to create the event timestamps\n\n# error files\n# /5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\n# /5cd56b5ae2acfd2d33b58548/2F/5cf214bbc852a70008c01607.txt\n# /5cd56b5ae2acfd2d33b58548/2F/5cf214bda50dc300099d34cc.txt\n# /5cd56b61e2acfd2d33b58d20/F2/5d085df529994a0008202661.txt\n# /5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\n# /5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\n# /5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\n# /5cd56b5ae2acfd2d33b5854a/B1/5d073ba64a19c000086c559b.txt\n# /5cd56b5ae2acfd2d33b5854a/F1/5d07603e4cae4f000a2db525.txt\n# /5cd56b63e2acfd2d33b591c2/F2/5d0b0668912a980009fe91f2.txt\n# /5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\n# /5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\n# /5cd56b64e2acfd2d33b592b3/F2/5d0c9321c99c56000836df18.txt\n# /5cd56b64e2acfd2d33b592b3/F3/5d0c9952ea565d0008e34e8b.txt\n# /5cd56b64e2acfd2d33b592b3/F4/5d0c9d65ea565d0008e34ea2.txt\n# /5cd56b5ae2acfd2d33b58549/5F/5d0613514a19c000086c432a.txt\n# /5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\n# /5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\n# /5cd56b79e2acfd2d33b5b74e/F3/5d0b015e2f8a26000805ba44.txt\n# /5cd56b79e2acfd2d33b5b74e/F1/5d0af3452f8a26000805b830.txt\n# /5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\n# /5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\n# /5cd56b6be2acfd2d33b59d1f/F3/5d0896415125450008037c76.txt\n\n# base_path = \"../input/indoor-location-navigation/train\"\n# error_files = [\n#     \"/5cd56b5ae2acfd2d33b58548/1F/5cf20b29718b08000848aa0a.txt\",\n#     \"/5cd56b61e2acfd2d33b58d20/F2/5d085dea4a2bd40008d47468.txt\",\n#     \"/5cd56b61e2acfd2d33b58d20/F4/5d086c44d85da00008644fce.txt\",\n#     \"/5cd56b5ae2acfd2d33b5854a/F3/5d078bab0e86b60008036348.txt\",\n#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbfb2f8a26000805b9cb.txt\",\n#     \"/5cd56b63e2acfd2d33b591c2/F1/5d0afbf92f8a26000805b9c9.txt\",\n#     \"/5cd56b5ae2acfd2d33b58549/2F/5d11a6089c50c70008fe89bc.txt\",\n#     \"/5cd56b79e2acfd2d33b5b74e/F3/5d0b01522f8a26000805ba3e.txt\",\n#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a1545125450008037d87.txt\",\n#     \"/5cd56b6be2acfd2d33b59d1f/F1/5d08a14e3f461f0008dac56c.txt\"\n# ]\n\n# working_path = \"../input/indoor-location-navigation/train/5d2709c303f801723c3299ee/1F/5dad7d6daa1d300006faa80c.txt\"\n# error_paths = [base_path + e for e in error_files]\n# rows = one_trace_to_rows(error_paths[1], floor_map)\n# print(rows)","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n\n# df_train = pd.DataFrame(res[0], columns=col_names)\n# for r in res[1:]:\n#     df = pd.DataFrame(r, columns=col_names)\n#     df_train = df_train.append(df, ignore_index=True)\n\n# print(\"time to process\", time.time() - start)\n# print(\"length of df made\", len(df_train))\n# display(df_train.head(10))","execution_count":143,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def list_to_df(row_list):\n#     df_train = pd.DataFrame(row_list[0], columns=col_names)\n#     for r in row_list[1:]:\n#         df = pd.DataFrame(r, columns=col_names)\n#         df_train = df_train.append(df)\n#     return df_train\n\n# start = time.time()\n# pool = Pool(num_cores)\n\n# df_train = pool.map(list_to_df, tqdm(res))\n\n# # print(\"train_path count\", len(train_paths[:train_num]))\n# print(\"time to process\", time.time() - start)\n# print(\"length of df made\", len(df_train))\n# display(df_train.head(10))\n# pool.close()","execution_count":144,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate moving averages\n# Differencing respect to time (as each timestep is unevenly spaced)","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Save the file in parquet\n# # https://www.kaggle.com/pedrocouto39/fast-reading-w-pickle-feather-parquet-jay\n# # https://www.kaggle.com/prmohanty/python-how-to-save-and-load-ml-models\n\n# # Saving train data\n# train_file_name = \"indoor_train_4.pkl\"\n\n# with open(train_file_name, \"wb\") as file:\n#     pickle.dump(df_train, file)\n\n# # Save them to output\n# # df_train.to_csv('df_train_2.csv',index=False)\n# # df_test.to_csv('df_test.csv',index=False)","execution_count":146,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Load data it back in\n# with open(train_file_name, \"rb\") as file:\n#     df_train = pickle.load(file)","execution_count":147,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"df len: \", len(df_train), \"\\n\")\n# print(\"file_id unique: \", (df_train[\"file_id\"].nunique()), \"\\n\")\n# print(\"site_id unique: \", (df_train[\"site_id\"].nunique()), \"\\n\")\n# print(\"site_id value_counts: \", (df_train[\"site_id\"].value_counts()))\n# display(df_train.head())","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Get submission file\n# sub_df = pd.read_csv(\"/kaggle/input/indoor-location-navigation/sample_submission.csv\")\n# sub_df[[\"site\", \"file\", \"timestamp\"]] = sub_df[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n# sub_df = sub_df.drop(columns=[\"floor\", \"x\", \"y\"])\n# # grouped_df = sub_df.groupby(\"file\").sample(n=2)\n# # all_file_id = grouped_df[\"file\"].unique()\n# # print(len(grouped_df))\n# # print(len(all_file_id))\n# # display(grouped_df.head())\n# display(sub_df.head())\n\n# test_site_id = sub_df[\"site\"].unique()\n# train_site_id = df_train[\"site_id\"].unique()\n# print(test_site_id, \"\\n\")\n# print(train_site_id, \"\\n\")\n# a = list(set(test_site_id) & set(train_site_id))\n# print(a)","execution_count":149,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}